---
documentclass: exam
title: Draft
author: Scott Tang
mainfont: FreeSerif
CJKmainfont: Noto Serif CJK SC
mathfont: XITS Math
fontsize: 12pt
numbersections: true
---
# I

## Intro 

This paper is probably not a typical metaphysics paper. Although the main upshot is on metaphysics, the kind of move I make applies beyond metaphysics. 

In the background, this paper deals with a central tension. On one hand, it seems that, ultimately all possible philosophical conclusions must be derived from intuitive presuppositions. If so, then we cannot establish anything if we throw away all of our intuitions (or dispositions, sentiments, etc.). On the other hand, many of our intuitions are unreliable, and even contradict one another. The problem, then, is to figure out which part of our intuition cannot (or at least, should not) be thrown away and can safely rely on.

In the foreground, this paper focuses mainly on metaphysics, but the argument I made for metaphysics also extends to other areas, like ethics. To be specific, I first argue that the background tension is somehow ignored in discussions about fundamentality. As a result, the idea that facts about fundamentality are "objectively objective", though well-motivated, seems to rely on magic or wishful thinking. 

Then I argue for a principle that basically says, we cannot throw away our rational agency. The rest of the paper explores the consequences of this principle. For example, it can be used to establish a direct, non-magical account of fundamentality. But given that the background tension is topic-neutral, this principle can achieve more. For instance, it also solves a problem in ethics: why we should sometimes trust a friend's word, even when it goes against the evidence.

## The Question of Reality

The starting point of this paper comes from the following question: *How much reality should we read off our linguistic features?*

### clarification

First, it seems to suggest linguistic idealism. It doesn't. Reality certainly is not metaphysically constructed upon our language in any way.

Without language, we might still have ways to understand reality, perhaps through perception or mysterious experiences. But, this is not how metaphysics works. In metaphysics, we inevitably rely on sophisticated argument that relies on intuitive premises. But this has generated a long-standing worry in philosophy. Since medieval times, philosophers (e.g. nominalists) have been worrying that our intuition about the world comes from *words* instead of reality. Hence my question: when we try to talk about reality, which parts genuinely reflect how things are, and which parts are mere "shadows of language"?

This question appears central to almost all metaphysical problems. For example, 

- Almost all natural languages have a subject-predicate structure. Should we then say, the world itself is constituted of objects and properties? 
- Grammatically we have predicates like 'good' or 'bad'. Then should we read them off our language and say there are such properties in the world? 
- The proposition $P$ may seem syntactically more primitive than $\neg \neg P$. Should we read this structure off our language and say the *fact* that $P$ is different, and in some sense more fundamental than the *fact* that $\neg \neg P$? 
- Quantum physics describes the world in terms of wave functions. Should we then interpret that reality as just a bunch of waves (or maybe, a huge, holistic wave)?

It is obvious that not every linguistic feature reveals reality. Take the proper names "Superman" and "Clark Kent." We don't think there are really two distinct entities out there connected by a relation named "identity". This is a non-trivial claim. Formally, one could construct a toy model $M = \langle D, I \rangle$ with $D = \{s, c\}$, $I(\text{'Superman'}) = s$, $I(\text{'Clark Kent'}) = c$ and $I(\text{'is'})= \{ \langle s,c \rangle, \langle c,s \rangle \}$. Such model would satisfy 'Superman is Clark Kent'.[^1] But intuitively we do not take such a model to *capture reality* because we think 'Superman' and 'Clark Kent' should refer to the same entity in the domain. 

This question can be taken as a better way to formulate realism. The traditional approach formulates realism in terms of *truth*. For example, scientific realism is often described as the view that, scientiﬁc theories are true(==citation from BVF==). And anti-realists like van Fraassen argue that they aren't really true, but merely _empirically adequate_.

However, this way of framing realism relies on a problematic conception of truth. The problem is that *truth is cheap*. Consider a game of chess: no one would think we are saying something false when we state, "There is a bishop on the board." Yet, the truth of that statement is partly established by the rules of chess, not by the world itself. This shows that truth can arise from linguistic rules, not just from reality. 

This way of framing realism creates a loophole for deflationists. It allows them to call themselves "lightweight realists" simply by agreeing that our metaphysical claims are true, while maintaining that this truth is superficial. But we do have a strong intuition to say, there is something deeper than that. For example, the toy model $M$ about Superman and Clark Kent constructed earlier would make our utterance about their identity come out *true*. But we do feel there is something wrong about it.

If we frame the question of realism in my fashion, however, then the deflationists must say more. For example, according to easy ontologists, adopting a language is easy, and thus ontology is easy. (==citation from Amie Thomasson==) But ontology can't be that easy. Even if "there is $x$" can be made trivially true, it is not trivial to freely read $x$ off the sentence and take $x$ to be really *out there*. And Sider's project about Ontologese is best understood as, we should only read the (most fundamental) reality off the perfectly natural language, not unnatural languages that include kinds like "incars" and "outcars".

Another important distinction needs to be clarified. There is a difference between merely *utilizing* a language and *adopting* a language. To start with, what I mean by language is broad, and includes inferential rules and laws. In other words, that "theory" and "language" are interchangeable in this paper. 

In fact, a full account of this distinction has to be discussed at length elsewhere. But the basic idea is simple. To *utilize* a language is to merely follow syntactic rules and form well-formed sentences. One does not need to take a stance on how to interpret the language. For example, mathematical formalists, like Hilbert, are merely utilizing mathematical languages without reading anything off it. To *adopt* a language, however, one has to also *think* in its semantics and *accept* the picture of reality it implies. A calculator surely doesn't *adopt* arithmetic. In addition, we do not always adopt *literal* English because we don't really think "bank" is a unified category.

The way I've framed the question, i.e. how much reality we should read off our linguistic features, is different from the standard story told in metametaphysics. I think the standard narrative, which I'll lay out in a moment, tends to miss something important. I take the familiar story told by ==Schaffer (2009 on what grounds what)==, which is the most-cited paper in this field, as the orthodox version (though I'll be telling a bit differently).

## The standard story

It goes something like this: contemporary metaphysics began with the Quine-Carnap debate. After Quine arguably won the debate, his approach to metaphysics prevailed. For Quine, the main task of metaphysics is to figure out what indispensably *exists*. Whether numbers or electrons exist depends on whether we must quantify over them in our best theories. This methodology was extended to other topics. Lewis, in some sense, can be seen as following the Quinean methodology: Since our best account of *modality* requires quantifying over possible worlds, possible worlds are real. From there, other concepts can be analyzed. *Causation* is explained by counterfactuals, which are analyzed in terms of real possible worlds. Modality *de re* can be analyzed by real possible worlds, plus some pragmatic, context-sensitive similarities. The takeaway is that most problems in metaphysics can be solved by first figuring out what exists and then maybe add a dash of pragmatics.

But then, the neo-Carnapians entered the scene. I believe the most important the idea from the neo-Carnapians is that, the meaning of "exists" can vary.[^2] According to this view, when different people debate about what exists, they are speaking different languages. If their languages are truth-functionally equivalent, then this is a mere verbal dispute.

Imagine one side speaks A-language and the other speaks B-language. For any sentence uttered by A-speakers, B-speakers will have their respective interpretation the sentence, vice versa. If the interpretation from each side always agrees on the truth value, then there's no disagreement other than disagreement on language: the other side is just expressing the fact differently. If that's the case, then many debates about what exists are merely verbal.

The implication here is serious. If metaphysics is founded on questions of existence, but quantifier variance shows that there are many equally good meanings of "existence" and the debates are merely verbal, then metaphysics risks becoming a largely trivial field of study.

Meanwhile the Neo-Aristotelians also come into play.[^3] They also attack on the neo-Quineans. They argued that even if we had a complete list of what exists, we'd still be missing something important: what is most fundamental, and what grounds what. Schaffer, for instance, argues that questions about existence are trivial. He says that of fictional characters and abstract objects trivially exist. The real, interesting question is about what grounds their existence. This shift in focus from existence to fundamentality has been the dominant trend in metaphysics for the past twenty years or so.

### The problem with the standard story

Here's the problem. From the neo-Aristotelian perspective, this makes them look like the next logical step. In this view, the neo-Carnapians performed a useful, but purely *negative*, task by showing the flaws in the neo-Quinean focus on existence: the meaning of "exist" could vary. With that groundwork laid, the neo-Aristotelians can then step in to offer a *positive* new project, and say the various meaning of "exist" are "exist-more-fundamentally" and "exist-less-fundamentally". Thus, they turning the page on the neo-Quine-Carnap debate and building something new around fundamentality. (==citation==)

As a result of this framing, the standard story tends to sideline the neo-Carnapians. This isn't to say they're completely ignored, of course; people like Sider, Schaffer, and Bennett certainly take them seriously. Rather, the problem is that the standard story implies the neo-Carnapian critique is *only* about existence. And if that's the case, then the neo-Aristotelians, with their new focus on fundamentality, can safely disregard it. 

But this is surely *not* the case. The neo-Carnapian argument, on a high level, is about *semantic variance*. Therefore, it applies to any theory as long as the theory allows semantic variance. 

In order to see this more clearly, let's return to the question I framed at the start: *How much reality should we read off our linguistic features?*

**For the neo-Quineans**, the answer is *sometimes-ism about existence*. We can only sometimes read entities off our ordinary language. We need to paraphrase our ordinary language and scientific theories. While the exact rules for this paraphrasing are messy, but the basic idea is to create a language that maximizes pragmatic virtues like simplicity, elegance, and explanatory power. It is from this idealized, paraphrased language that we are meant to read real entities off the values being quantified over.

**For the neo-Carnapians**, the most plausible answer is *a general form of always-ism*: we can always freely read reality off any languages we think in, as long as they are truth-functionally equivalent.[^4] When another party, say John, uses a different yet equivalent language, there's nothing wrong with John, nor is there anything wrong with us. First, we can interpret both sides as speaking the truth, so there's no disagreement about facts. Second, from our point of view, there's nothing wrong with "John's way of thinking" because his resulting beliefs are also true. Here's why: if a sentence $S$ is true in John's language​, then it is true for John to believe that $S$ *under the guise of* John's language.[^5] By the same token, from John's point of view, there's nothing wrong with us either, whether in terms of truth or in terms of beliefs. I leave some complications of this account in a footnote.[^6]

**For the neo-Aristotelians**, it seems they are holding *sometimes-ism about explanation or definition*, and *always-ism about existence*.[^7] Everything we can talk about trivially exists, but it might be derivative or idea-dependent. And the way we trace this kind of dependency is through the structure of definition or explanation (==citation to Fine's essence and modality, guide to ground, and Schaffer's work==).

The essential feature of definitions (and explanations) is that they are inherently *asymmetric*: nothing can define itself, and if $A$ is defined by $B$, then $B$ cannot be defined by $A$. However, this only applies to "real definitions," not "nominal definitions." For example, we may nominally define "A is on the left side of B" by "B is on the right side of A," and define "B is on the right side of A" by "A is on the left side of B." In fact, this kind of circularity probably appears occasionally when terms are defined in the dictionary.

Given this distinction, we cannot say that any definition in our language trace fundamentality. Only the "real" ones trace fundamentality. The same principle applies to explanation. Although grounding relations are explanatory, they are not explanation relations per se. Explanation is (in)famously context-sensitive (==citation to van Fraassen, Kitcher==). It depends on things such as what we're emphasizing, and what interests our audience. Grounding relations, by contrast, are intended to be worldly, "objectively objective", and context-independent. Since they are separate, only "real" explanation can trace fundamentality. This leads neo-Aristotelians to embrace *sometimes-ism* about explanation or definition.

## Objective Metaphysical Privilege

Now we can clearly see the issue. Neo-Carnapians are arguing for a general form of *always-ism*, which would naturally includes the *always-ism about explanation or definition*. The pressure that the neo-Carnapian argument would put on the neo-Aristotelians' *sometimes-ism* is that, what makes some definition or explanation privileged so that they get to trace fundamentality, but the other doesn't? 

I will later defend sometimes-ism against always-ism. In fact, the *general form of always-ism* is already under pressure. In the previous paragraph, I constructed an exotic toy model that satisfies "Superman is Clark Kent". According to always-ism, there is no problem with this model. Something already seems wrong. But for now I will ignore these problems, put on the hat of neo-Carnapians and explain why the neo-Aristotelians did not give any satisfying answer. 

In the following three sections, I will leave aside the Quinean-flavored view. This view establish fundamentality by pragmatic epistemology, i.e. the fact is determined by the best system that has the most pragmatic virtues like simplicity, elegance and explanatory power. I argue against this view in the second part of this paper. But standard neo-Aristotelians vehemently reject that metaphysics is just Quinean pragmatics. Therefore, in the next three sections, I attack the non-pragmatic neo-Aristotlian view that, we can objectively establish an absolute account of fundamentality that does not lead to semantic variance.

### Naturalness

Let me start with a way to establish a privileged definition using a familiar concept: *naturalness*. This notion was famously introduced by Lewis (==Citation a new theory of universals==), but his account was generally restricted to properties. It was Sider who attempted to use this concept to establish fundamentality (==Citation writing the book of the world==). Basically, Sider extends the notion of naturalness such that linguistic features beyond predicates (e.g., quantifiers, sentential connectives, sentential operators, predicate operators) can be evaluated by whether they carve the world at its joints or not.[^8] The linguistic features of the best language would then perfectly match the joints of reality. Given this extended understanding of naturalness, Sider contends that the most fundamental layer of the world is revealed by the structure of this best language. In this spirit, a "real" explanation/definition is the explanation/definition that only uses perfectly natural expressions.

But surely if the meaning of "naturalness" could vary between languages, then different languages could be considered "the best language" under different semantic interpretations of "naturalness." This is something Sider wants to avoid, because he argues that "naturalness" is supposed to be absolute and "objectively objective." The problem, then, is what makes the meaning of "naturalness" invariant across languages?

One way to defend this invariance is to appeal to reference magnetism. Sider stipulates that naturalness itself is perfectly natural (or, in his terminology, "structure itself is structural" ==p. 137==). And the meaning of the term "naturalness" is automatically attracted to the most natural candidate meaning (==Sider p.23==). Therefore, there is a unique answer to what the "reference magnet" is.

But this doesn't solve the problem. There is no contradiction in having two different languages, both of which agree on the sentence "naturalness itself is perfectly natural" while disagreeing on the extension of "naturalness." For example, in a mereological essentialist's language (ME-language), the ME-quantifier only ranges over simples, while in an ordinary language (OD-language), the OD-quantifier also ranges over composite objects. Assuming they both agree that naturalness itself is perfectly natural, there seems to be no contradiction in each side claiming its own quantifier to be perfectly natural, as long as they interpret "perfectly natural" differently. 

But the greater issue for this solution is that, it is more plausible to take there to be no reference magnets. As Hirsch (==citations==) and Warren (==2023==) pointed out, in the case where language use can determine the reference, magnetism cannot override it. Assume there are Martians seems as intelligent as we are, but they use '+' in a way that we need to interpret it as quus to make their utterance come out true. In this case, we should say the Martians are using '+' to mean quus. And in the cases of vagueness, like bald, where use cannot determine reference, we think it is just indeterminate. In other words, when language use determines the reference, magnetism does not exist. When the use does not determine the reference, magnetism does not exist. Therefore, magnetism does not exist.[^9]

The initial motivation to posit reference magnetism is to explain why our '+' means plus, not quus. It seems our *actual use* does not determine plus, while there is a strong impulse to say the meaning of '+' is plus, not quus. But "actual use" might be a too narrow view about language use. Language use should include our disposition of use, and the disposition is enough to say '+' means plus. More will be said later, but for now I believe this should suffice to dismiss reference magnetism.

Let's return to the original problem: if reference magnetism fails, then what makes the meaning of "naturalness" invariant across languages? More generally, when is any term's meaning fixed in this way?

In a trivial sense, of course, any symbol can vary in meaning; "naturalness" could be used to mean "hello," for instance. But this isn't the kind of meaningful variation we're concerned with here. There might be several ways to explain "invariance".[^10] But the relevant idea might help establish the invariance of "naturalness" here is that, the world can somehow teach us whether we are getting the right concept. If we are using a wrong concept, the world will correct us.

What do I mean by "the world can teach us the right concept"? Special relativity seems to be a nice example. We naturally think that changing inertial reference frames is just a sheer transformation, and the speed of light would depend on the observer's motion. But empirical facts tell us that, surprisingly, our intuition is wrong. No matter how we change our reference frames, the speed of light remains invariant. Therefore there are some privileged transformations, like Lorentz transformations, that objectively does better job at describing the world. Therefore, the world teaches us that, our seemingly natural intuition is in fact incorrect.

The parallel argument used to support an "objectively objective naturalness" is Goodman's riddle of induction. The basic idea is that, a concept like "grue" must be flawed because it leads to a wrong induction. Therefore the world teaches us that some concepts are objectively better than others. 

This is perhaps the strongest case for the objectivity of naturalness, but I believe it ultimately fails. To recap the riddle: Since all emeralds observed before 2050 are green, these emeralds are also by definition "grue" (where grue means green and observed before 2050, or blue otherwise). If we use grue for induction, the grue-induction would tell us that the next emerald we see after 2050 will be grue. However, we intuitively know the next emerald will be green, meaning it will not be grue. Since the green-induction and the grue-induction are formally the same, the problem does not lie in the structure of induction. Then the concept "grue" itself must be the problem.

But I think the world cannot teach grue-speakers that grue-induction is problematic. And therefore, for grue-speakers, there is nothing wrong with grue. Let's consider two cases.

**Case 1**: In the first case, *we* assume that "being green" is an essential property of the kind "emerald." But why would *grue-speakers* accept this? By the same token, they might take "being grue" as the essence of an "emerald." If we showed them a green emerald in 2051, they would simply say, "*This isn't an emerald, because it's not grue.*" In that case, their induction is perfectly reliable: everything they call an "emerald" is, in fact, grue. Of course, this scenario is overly simple because it assumes the kind "emerald" is defined solely by its color. If so, the disagreement becomes a matter of a priori definition, not a failure of induction.

**Case 2**: So, let's consider a more realistic and relevant case. Assume both we and the grue-speakers agree that emeralds are defined by their chemical structure ZYX, not their color. Through induction, our science posit a law of nature that "*ZYX necessitates greenness.*" In grue-science, however, induction would posit a law that "*ZYX necessitates grue-ness.*" Assume that when 2050 arrives, the next emerald turns out green. Therefore, grue-law will be falsified. Then it follows that, the laws of nature is simpler when formulated in terms of green. Thus, the world teaches us that, green is objectively better than grue. 

But this argument doesn't seem right for two reasons. First, Inductive failure is normal. A predicate can be natural, but simply fail to apply to the induction. For example, we might inductively conclude that all swans are white. The discovery of a black swan doesn't make the property "white" is objectively worse than "white or black". It just means we hit on a false hypothesis.

Second, the whole argument relies on the presupposition that, in grue-science, the grue-speakers start with the hypothesis *ZYX necessitates grue-ness.* But given they take the property "green if observed before 2050, or blue otherwise" to be simpler than "green", why wouldn't they instead adopt the hypothesis "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*" as simpler than "*ZYX necessitates grue-ness*"?

To prevent further complications, assume that ZYX is invariant in both our law and the grue-law. Let us even grant that the grue-speakers initially start with the hypothesis *ZYX necessitates grue-ness*, and that this hypothesis is falsified by the first emerald after 2050. After further work, grue scientists discover that the corrected grue-law, like "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*", is the right one.

Now, the previous argument runs, our law "*ZYX necessitates green*" is objectively simpler than this corrected grue-law. Therefore, green exhibits objective similarity, not grue. Why? Because the properties in the simplest laws of nature 'carve nature at its joints'.

But why would the grue-speakers accept this argument? My stipulation is that grue-speakers have different intuitions about simplicity and similarity from ours. They take properties, that are disjunctive in our terms, to be genuinely simple similar; likewise they treat disjunctive propositions as simpler. Given this stipulation, they would say their corrected law is actually simpler than ours. Therefore, grue and bleen are more privileged than green.

In other words, if the meaning of "the best system" could vary, then the best system account of laws of nature cannot be objectively established. If natural properties are specified by the laws of nature, while the laws of nature can vary in different frameworks, then "naturalness" can vary in different frameworks. Therefore, even if the grue-speakers agree with us that, natural properties are metaphysically privileged, the world cannot force the grue-speakers to give up a different extension of "natural properties".

Surely for us, there is no problem, in accordance with Bayesian principles, to adopt a higher credence for green-laws based on prior distribution. This may be enough to say we are justified to believe green-law, and this is enough for our confirmation. But there is no theoretical reason to take greenness as the metaphysically privileged concept across different frameworks. Nothing can stop "naturalness" having semantic variance.

### Essence

Maybe "naturalness" is already too niche. Indeed, the most orthodoxical neo-Aristotelian way to establish fundamentality is through concepts like grounding or essence. For example, the most fundamental layer is ungrounded (i.e. independent), and everything is either the most fundamental, or is grounded by the most fundamentals (i.e. compleness). In other words, the *sometimes-ism* about definition/explanation is explained by grounding or essence.

The concept of grounding, however, notoriously lacks consensus. Before getting further into this, I wish to go into a slightly earlier concept, i.e. essence.

The approach to establishing fundamentality through essence is largely due to Kit Fine's series of works (=="Essence and Modality," "Senses of Essence," "Ontological Dependence," and "The Logic of Essence"==). Of course, Fine prefers to treat ontological dependence as primitive. But Fine acknowledges that this is merely a technical choice, and we can instead define ontological dependence in terms of essence: $x$ depends on $y$ when there is some property $\phi$ (that does not mention $y$) such that in virtue of $x$'s nature it's true that $y$ has $\phi$, but in virtue of $x$'s nature it's not true that everything has $\phi$. Formally, according to Fine we can choose to define $x \ge y =_{df} \Box_x(y=y)$ along with other axioms in order to capture the informal definition. 

Here, I will not go into the details of Fine's axiomatic system. And I acknowledge that, essence is indeed a concept upon which *de re* modality should be built, not the other way around. Instead, my question is, why should we take essence to be a feature of reality, not a feature of language? To show this, we need to show that, if there is an alternative language that attribute essence differently, then that attribution is *objectively* wrong.

Imagine a set-theoretic creature, maybe a robot called Robo, who perceives every object as a singleton set. For example, Robo directly perceives a table as $\{table\}$ and Socrates as $\{Socrates\}$. Only by theoretical reconstruction can Robo realize that what she perceives is necessarily related to the individual Socrates. For Robo, then, the individual we call Socrates is a theoretical construction from the entity she directly perceives. Therefore, from her perspective, Socrates depends on $\{Socrates\}$.[^11]

Is there anything objectively wrong with this creature Robo? Of course, there is a sense in which Robo is wrong because her attribution of essence violates our intuition. But this reason is *symmetric*: Robo can by the same token accuse us of being counterintuitive. Yet, if Robo's attribution of essence is *objectively* flawed, the reason cannot be symmetric. So, appealing to our own intuition does not count as a good reason to say Robo's attribution of essence is flawed.

I think the only hope to establish the invariance of essence attribution is to let the world teach us whether the concept we are using is wrong. For example, the wrong attribution of essence will make some induction fail, etc. But I can't see any way to do this.

### Grounding(Building)

Now let's get into grounding. Here what I mean by "grounding" here is Wilson's talk of "small-'g' grounding relations" (==Wilson 2014==), or what Bennett calls "building". In fact, let me switch to the terminology of "building" in the rest of the discussion, in order not to conflate with the specific grounding that is usually thought of as a relation between facts. I use building here because fundamentality may not just be established by the specific version of grounding.

Building relation is a set of relations that are directed, necessitating and generative (==Bennett citation==). Some examples are composition, constitution, set-formation, (property) realization, grounding and causation. To say that $x$ builds $y$ is to say $x$ bears one of these building relations with $y$. According to Bennett, fundamentality can be elegantly established by building relationships. In a rough way to put it, the most fundamental layer is unbuilt, and if $A$ builds $B$, then $A$ is more fundamental than $B$.

Immediately, this account of fundamentality is already *relative*, because there are different building relations. For example, according Shaffer's priority monism, the entire cosmos $builds_{grounds}$ its parts, but its parts $build_{compose}$ the entire cosmos. According to this view, there is no "objectively objective" fact of something being absoutely fundamental; Rather, something can only be fundamental *relative to a building relation*.

Even if so, we may still say, *the set of relations* that count as building relation is absolute and objectively objective. If so, then fundamentality, though being relative to a specific building relation, is still invariant across different language. The problem, then again, is whether this can be established.

In the discussion of essence, I have talked about the directness given by set-formation is problematic: the members of a set forms a set does not follow that the members are more fundamental than the set. For Robo, this direction might be reversed: instead of set-formation being a building relation, they take set-membership as a building relation. 

Recall Bennett's account about building: a building relation has to be *directed*, *necessitating* and *generative*. The problem here is the generative requirement. According to Bennett, the generative requirement is that,

> (G) For all building relations B, and all x and y, x's B-ing y makes true certain explanatory and generative claims. For example, if a builds b, then b exists (obtains, is instantiated . . . ) because a does, b exists (obtains, is instantiated) in virtue of a, a makes b exist (obtain, be instantiated), and so forth.

Surely, Robo agree with us that $\text{Socrates} \in \{Socrates\}$. But what they disagree with us that set-formation is not generative. They think set-membership is generative instead.

The problem can be made more general. If $Rxy$ makes $\square (x \leftrightarrow y)$ true, then let's call the relation $R$ *mutually necessitating*.[^12] Given any directed and mutually necessitating relation, we can always find a reverse relation that is also directed and necessitating. And reversal is not the only formal maneuver we can do. Say a binary asymmetric relation $R\subseteq D\times D$ is mutually necessitating. Then we can define inverse $R^{-1}=\{(x,y)\in D\times D \mid R(y,x)\}$. Given that $R^{-1}(x,y) \leftrightarrow R(y,x)$, it is easy to see $R^{-1}$ is also asymmetric and necessitating. Moreover, let $x\equiv_{\Box}y$ abbreviate $\Box(x\leftrightarrow y)$. $R$ lies inside the union of $\equiv_{\Box}$-classes, i.e. $R\subseteq\bigcup_{C\in D/\!\equiv_{\Box}}(C\times C)$. Hence within each $\equiv_{\Box}$-class $C$ of size $n$ one may independently pick for every unordered pair $\{u,v\}\subset C$ either $u\to v$, $v\to u$, or neither, producing $3^{\binom{n}{2}}$ asymmetric, necessitating relations on $C$. Whenever some $\equiv_{\Box}$-class has $n\ge 2$ there are therefore combinatorially many relations $R^{*}$ that (i) predicate over the same domain, (ii) are necessitating and directed, yet (iii) are extensionally different from a given directed, necessitating $R$.

Therefore, if there is any mutually necessitating relation $R$ in the set of building relations, and we want to rule out $R^{*}$, then the problem is how. Bennett suggest the generative constraint, but this constraint cannot establish the invariance of the set of building relations. For Robo, $\{Socrates\}$ exists explains Socrates exists. But we cannot say this is not the "real" explanation, because Socrates is more fundamental than $\{Socrates\}$. This just begs the question. And I don't see other reason to say there is any problem with Robo's understanding of fundamentality.

Indeed, I believe this is the most problematic part of Bennett's account of building relation. Surprisingly, even Bennett concede that what counts generative may be arbitrary, conventional, framework-dependent and "nothing further to be said" (==p. 59==) If this is the case, then how can we save the idea that the set of building relations is invariant and "objectively objective"? I can only see the following two ways out.

The first way is to simply not rule out the alternative relations $R^{*}$. This suggestion, however, makes the generative constraint redundant. It would mean that for any group of mutually necessitating entities, we could order them however we want and claim that our arbitrary ordering reveals a sense of fundamentality. But if a collection of things can be ordered in any arbitrary way, it seems the best way to describe the situation is to say that the things themselves have no intrinsic order. They just merely necessitate each other. Therefore, even if this solution formally works, this move will depart from the intended meaning of fundamentality.

A second way out is to deny that there can be mutually necessitating building relations altogether. If so, then relations like set-formation, successor operator, and logical connectives would be removed from the list of building relations. And it seems that, some other relations, like composition, would be preserved. For example, if we do not adopt mereological essentialism, then material composition is not mutually necessitating, because the parts necessitate the whole, but the whole does not necessitate those specific parts.

But this move undermines the fundamentality project. The fundamentality project is advertised as "post-modal metaphysics". This is because building relations are advertised to be hyperintensional, and "tighter than necessity". But if we rule out mutually necessitating relations, then the remaining relations can be simply characterized by modality.

In addition, this move still does not make the set of building relations invariant. For example, the meaning of "composition" can vary. Mereological nihilists, organicists, common sense metaphysicians, and four-dimensionalists all have different interpretations of "composition". If we allow any of these interpretations to count as a building relation, then what is considered fundamental becomes arbitrary. This, again, seems to depart from the intended meaning of fundamentality.

In addition, it is not only the generative constraint that may lead to semantic variance. The necessitating constraint may also lead to variance, given that there are different senses of necessitation. This has been discussed by Sidelle (==The Grounding Mystique==): Given that modality is at least partly conventional, the deflationary challenge for metaphysical necessity extends equally to grounding.

## Upshot

The gist of the tedious discussion above is simple. Imagine an alien whose language contains "fundamentality" that plays the same role in their metaphysics as our "fundamentality" does in our metaphysics. However, their "fundamentality" has a different extension. If we do not rely on the Quinean epistemology, and claim that fundamentality is a *worldly, objective feature*, then we must be able to show why the alien's concept is *objectively* wrong. Crucially, the reason we provide must not be symmetric; it can't be a reason the aliens could also use against us (e.g., "it violates *our* intuitions"). As the previous sections have shown, it seems we cannot find such a non-question-begging reason in the current way of establishing fundamentality.

It seems the only hope for finding this kind of non-symmetric reason would be the following: the world itself could somehow teach us which "fundamentality" is the correct use. But the challenges from imagining strange aliens essentially press the following question: if we were to throw away some of our intuitions, would the world still be able to teach us anything? The upshot of the discussion is that *it would not*. In a slogan: ***If we are willing to throw away our intuitions, the world cannot teach us anything.***

I believe this slogan is true. However, in the next part of this paper, I show that some of our intuitions cannot be thrown away, and it would lead to substantial consequences.

# II

The arguments discussed in the previous section share a common pattern: we imagine a strange creature who thinks and speaks a radically different language. The challenge is that we seem unable to prove their language is metaphysically worse than ours.

A natural, initial response is to dismiss these creatures as simply _insane_. This is, in fact, the kind of response I want to defend. My strategy, however, isn't to argue that there is something wrong with these alien languages _per se_. Instead, I want to focus on what's wrong with the relationship between _us_ and these languages.

To put it simply, *certain constraints prevents us from coherently adopting and committing to just any theory*. If these constraints have substantial metaphysical consequences, then any theory we can coherently adopt must be consistent with them. Moreover, these metaphysical consequences will be *invariant* across all theories available to us.

Let me clarify further. One might object that, physics already presents a picture of reality that is far beyond our understanding. For example, it's very difficult to truly make sense of concepts like fields, curved spacetime, or probabilistic states. Yet, these are highly successful theories, and we don't reject them. In fact, we make use of them all the time.

To reply, I want to emphasize again the difference between *adopting* a theory and merely *utilizing* it. This distinction is clear in quantum mechanics. Physicists apply the Schrödinger equation to compute predictions and do experiments. Yet they may remain silent about how to interpret the equation. They just shut up and calculate. If so, then they are merely utilizing these equations without adopting them into their worldview. What I am arguing for is the constraint on adopting a theory. There is no problem with using a theory that violates these constraints to do experiments or make predictions. However, as long as we want to truly understand the reality a theory claims to reveal, these constraints apply.

## The principle of Sanity

The constraint I suggest is the following:

**The principle of sanity**
: For someone who is capable of philosophical reflection, it is impossible to genuinely believe oneself to not be a rational agent.

This principle also applies to acceptance (i.e. It is impossible for someone to accept oneself to not be a rational agent), but I will focus on belief in the rest of the paper unless specified otherwise. Anyway, the gist is, it is impossible for us to really adopt a theory that eliminates rational agency.

Here, "philosophical reflection" roughly refers to activities such as examining and revising one's beliefs, analyzing arguments, and choosing between competing theories. The condition "for a person capable of philosophical reflection" is to rule out certain exotic counterexamples. For instance, someone who has been so thoroughly brainwashed that their only possible belief is "I am not a rational agent" would not qualify as a counterexample. I assume all readers who have already made it here are automatically capable of philosophical reflection.

What I mean by "rational" here is a minimal sense of means-end coherence. For example, someone who wills their hand to rise but randomly cause their leg going up would lack this coherence. Thus, even if this randomness could somehow count as agency, it wouldn't be rational agency in the relevant sense.

The precise meaning of "rational agency", of course, is highly controversial and open to multiple acceptable interpretations. Consequently, this principle will have different variants depending on how one chooses to define the term. However, I contend that the *principle of sanity* is plausible under most, if not all, commonsensical interpretations.

Throughout this paper, I will discuss three different commonsensical interpretation of "rational agency". But for current purpose, it will suffice by adopting a minimal account of agency as *epistemic rational agency*. In the following discussion I will just use "epistemic agency" as a shorthand for"epistemic rational agency", unless I want to stress the means-end coherence aspect of my intended sense of agency.

What I mean by "epistemic agency" here is not the kind of agency resisted by doxastic involuntarists. According to doxastic involuntarists, *direct* voluntary control over belief is not possible for us. But here, I use "epistemic rational agency" to refer to any sort of non-random control over our beliefs, no matter how indirect. For example, we have the ability to choose to re-examine our beliefs and investigate new evidence. Doxastic involuntarists, like ==Hieronymi 2008==, also acknowledge that we possess this kind of indirect control. Therefore, it is uncontroversial that we have epistemic rational agency in the intended sense.

*The principle of sanity* is more or less a stipulative axiom in this paper that I take to be self-evident. But I will still try to give some defense. 

First, implicitly, this is already a widely accepted principle. Aristotle appears to adopt this principle in Chapter 9 of _De Interpretatione_. There, he rejects the principle of bivalence due to our deliberation and action. He argues that, deliberation and action requires that, propositions about the future must be open (19a10). Then they cannot have a truth value now (19a26). Therefore the principle of bivalence is false. However, Aristotle's argument seems to epistemic instead of metaphysical. When he writes that "what will be has an origin both in deliberation and in action" (19a6-9), the plausible interpretation is that our *beliefs* about the future originate from these rational activities, not the future state of affairs itself. If so, his argument is best understood as follows: it is impossible for us to genuinely *believe* that the future is predetermined while simultaneously viewing ourselves as rational agents who deliberate. Therefore, by *the principle of sanity*, we cannot adopt the principle of bivalence.

There are also more contemporary cases. People seems to reject epiphenomenalism largely due to the reason that, if there's no mental causation, then it seems we wouldn't have any agency, which is insane. Therefore, in fact people have been implicitly using this principle.

Second, I will try to sketch a direct defense. If the principle were false, then it would be possible for a creature to engage in philosophical reflection without genuinely believing itself to have epistemic agency. I claim that this is impossible. Philosophical reflection requires utilizing abstract concepts in order to make inference. But forming abstract concepts requires intentionality. And if someone can intentionality form concepts, one has the ability to bring about epistemic entities. And to bring about epistemic entities is to have epistemic agency. Moreover, one must be able to be aware of there intention to some extent, they must believe that they have such agency to some extent.

I don't expect this argument to be invincible, given the complex nature of agency and intentionality. But I hope it gives some further weight to this principle's plausibility. After all, I believe it is just insane to reject this principle.[^13]

Together with the following innocent principle that:

- **Ought Implies Can**: If it is impossible for us to believe that P, then it is not the case that we ought to believe that P.

It follows that *the Principle of Sanity* is incompatible with the traditional account of rationality: A belief is rational iff the reason to believe is truth-tracking. This is because *the principle of sanity* does not allow us to believe that we are not rational agents, regardless of the evidence. Assume our epistemic norm is to hold only true beliefs, and scientists tell us that rational agency is just an illusion, then we would be obligated to believe that we are not rational agents. But according to *Ought Implies Can*, there is no such obligation.

Then immediately, one might object that if epistemic norms must be truth-tracking, then we should rationally reject *the Principle of Sanity* itself. However, what does it mean to "rationally reject" a principle? If doing so is an act of philosophical reflection, then, as argued earlier, the very act requires us to presuppose our own epistemic agency. Therefore, any attempt to "rationally reject" *the Principle of Sanity* is self-defeating, as the act of rejection presupposes the very rational agency the principle commands us to accept.

There is also an auxiliary argument that targets at the Quinean methodology. In the first part of this paper, I put aside the Quinean epistemology for establishing fundamentality. According to this view, the best theory of fundamentality that has the most pragmatic virtues like simplicity, elegance and explanatory power. But what how could these pragmatic virtues more valuable than our own rational agency?

## Metaphysical Consequences

Given the Principle of Sanity, some direct corollaries follow. The basic idea is, given that we must think we are epistemic agents, we cannot coherently adopt a theory that says we aren't.

### Negative Corollaries

**One straightforward corollary is on causation.** To be an agent means to bring something about. 'Bringing about' is a causal notion. Therefore it is analytically true that, there is agency only if there is causation. It directly follows that, we cannot the following kind of theory: There is no such thing as causation. All there is is just a description of Humean regularity, possible worlds, etc.

To clarify, I am not against non-eliminative analysis of causation. It is meaningful to show how causation connects to other ideas. All I am saying is that, if we eliminate causation yet still believe we are epistemic agents, we would be inconsistent.

**Another corollary is on personal identity.** Let me state the conclusion first and then explain why. Given *the Principle of Sanity*, we cannot adopt the following kind of theory: "I" is an illusion and/or does not exist. All there is is just psychologically continuous or connected personal states.[^14]

This kind of impersonal view is advocated by Hume and Anscombe (==some citation here==). But a more important advocator of this view is ==Parfit 1984==. Surely, Parfit did not went that far to say that "I" does not exist. However, according to Parfit, there is nothing deep about personal identity. It would be better if we describe our lives in an impersonal way, without using "I" or other concepts that require person identity. He argues that doing so reveals a deep and liberating truth about our nature.

However, according to the Principle of Sanity, we cannot adopt such an impersonal language. This is because rational agency requires first-person pronoun. To prove this, I will go back to the more general *rational agency* instead of the specific *epistemic agency*. But nothing important hangs on this. The proof also applies to *epistemic agency* with some modification of the example.

The proof is as follows: Rational agency requires de se attitude. De se attitudes requires the *adoption* of first-person pronouns 'I' (or other syntactic equivalence). Therefore, rational agency requires us to read reality off the first-person pronoun and adopt the existence of "I". 

What is attitude de se? This terminology is introduced by Lewis (==attitudes de dicto and de se==), but the motivation comes from Perry (==the essential indexical==). In a nutshell, there are a type of propositional attitude that are essentially different from other kinds of attitudes: believing that I..., intending that I..., regretting that I.... Therefore, de se attitudes by definition requires the *adoption* of first-person pronouns.

This uniqueness of de se attitude is illustrated in the Perry's famous example:

> I once followed a trail of sugar on a supermarket floor, pushing my cart down the aisle on one side of a tall counter and back the aisle on the other, seeking the shopper with the torn sack to tell him he was making a mess. With each trip around the counter, the trail became thicker. But I seemed unable to catch up. Finally it dawned on me. ***I was the shopper I was trying to catch***.  
> I believed at the outset that the shopper with a torn sack was making a mess, and I was right. But I did not believe that I was making a mess—at least not initially. That belief came later. And when it did, I stopped circling the counter and rearranged the torn sack in my cart. (Perry 1979)

Only after Perry acquires the *de se* belief and thereby changes his behaviour, can his action be described as rational. A merely *de re* belief does not suffice. Suppose Perry sees a person, and forms the de re belief "this (person) is making a mess". Unknown to Perry, he is actually looking in a mirror, and "this" refers to Perry himself. Now Perry believes "*this is making a mess*" without believing "*I am making a mess*". Therefore, these two beliefs are distinct. 

But notice that, the sentences "*this is making a mess*" and "*I am making a mess*" have the same propositional content (in Kaplan's terminology). If the propositional content are the same but the two beliefs are distinct, then they must be believed in different ways:  "this is making a mess" is a belief *de re*, while "I am making a mess" is a belief *de se*. 

More importantly, rational action can only be produced by *de se* beliefs. One cannot rationally act on *de re* belief. It would be purely random if Perry stop chasing after the following: He look in the mirror, believing that *this person* is making a mess without believing that *he himself* is making a mess.

Some, myself included, have been tempted to resist this *de se* essentialism for action by arguing that one can act rationally just on *de re* beliefs. I think that resistance can't work.

For instance, @cappelenInessentialIndexicalPhilosophical2015 argues that there is nothing special about *de se* attitudes. For example, "Superman can fly" and "Clark Kent can fly" are both *de re* beliefs, and one can believe the former without believing the latter. Hence, we do not need to posit a *de se* attitude to explain the difference between the belief that "*This person is making a mess*" and the belief that "*I am making a mess*": 'I' works like a proper name and just yields *de re* attitudes.

The challenge for this account is to explain: why it appears that, only I-beliefs and I-intentions can produce rational action. For example, I can intend Perry to stop, but this intention cannot therefore directly make Perry to stop. But if I intend myself to stop, and this intention would directly make myself to stop.

If one insists that I-beliefs are nothing more than ordinary *de re* beliefs, one cannot also claim the following: I-beliefs are a special subtype of *de re* beliefs that uniquely produces action. This move merely renames *de se* attitudes as "*special de re* attitudes". It makes no substantive difference. To deny that the *de se* attitude is essential for action, one must maintain that all *de re* attitudes can in principle produce action.

This is indeed the direction Cappelen and Dever take. To make sense of this counterintuitive claim, they propose the so-called Action Inventory Model. According to this model, one keeps an inventory of actions they can perform. When Perry sees another person (say Kripke) making a mess and wants that person to stop, he forms an intention that *Kripke should stop*. But Kripke's stopping is not a possible action in Perry's inventory of action. So nothing happens. By contrast, when Perry forms the intention that *I should stop*, this intention picks out an action that is in Perry's inventory of action, i.e. Perry's stopping. Perry therefore stops.

But this account obviously doesn't work. In order to get rid of the complications that are brought into by the mirror, let's imagine the following scenario. 

Perry woke up in an empty room with a television. He suffers amnesia and forgets his name. Then he watches what he thinks is a live television broadcast. On the screen a person tagged "Perry" is being chased by a lion. Feeling sympathy, he forms the intention, "*Perry should run away now*."

Unknown to him, the footage is old and the person on screen is actually himself. This hidden fact does not alter his belief state: he still thinks the broadcast is live, and he still thinks "Perry" names some stranger. But his intention is indeed "Perry should run away now." According to the Action Inventory Model account, this is a *de re* intention, and Perry would run. But obviously it is not a rational action for someone to flee simply because they think a stranger on television should flee.

But let's even grant it that, in this case, Perry produces a rational action. Here, a bigger problem is the following. We can construct a similar scenario in which the person on the screen is Perry's identical twin brother, Berry (assuming there is such a person). In that scenario, Perry's qualitative experiences are identical to the original case; therefore the intentions he forms are the same. So Perry should produce the same action in both cases. But according to the Action Inventory Model, Berry's running is not available to Perry. Hence, if Berry is the person on television, Perry would not run. 

In other words, the problem with any denial of *de se* essentialism is this: It not only yields strange results, but it leads to different results in qualitatively identical scenarios. In any situation where Perry produces a rational action, we can always construct a case where everything is qualitatively identical for Perry. But in this case, the content of the *de re* belief is, accidentally, about someone else. If action depends on *de re* content, then the same intention would accidentally produce different unexpected results. But I have clarified earlier that this is not rational agency because it lacks minimal means-end coherence.

### Positive Corollaries

So far, I've illustrated two direct corollaries of the Principle of Sanity on causation and personal identity. These are negative meta-level conclusions, telling us which theories we *cannot* adopt. However, the principle can also yield positive, objective-level results.

#### Are Positive Corollaries Possible?

Immediately a question arises here. *The Principle of Sanity* is a meta-theoretical constraint. Why should an object-level theory has to adopt the content of the meta-level theory?

Here's the basic idea: If *the Principle of Sanity* entails that, it is impossible for us to adopt that $\neg P$, then $P$ has to be an invariable part for any complete metaphysical framework we can coherently adopt. Therefore, we can claim that $P$. Moreover, any attempt to vary $P$ semantically would violate *the Principle of Sanity* and thus be impossible for us to adopt. Hence, we can claim that get the "objectively objective" metaphysics that we want.

To clarify, as I said, *the Principle of Sanity* itself can vary depending on the interpretation of "rational agency." This, however, doesn't weaken my argument. Once an interpretation of "rational agency" is precisified, so too is the set of $P$s that the principle requires us to accept.

Hard-core metaphysicians will probably vehemently resist this and argue the this: The following is a meta-metaphysical claim: *$P$ is invariable in any metaphysical theory we can coherently adopt.* But it does not entail the metaphysical claim: *$P$ is metaphysically true*. The former is a claim about theories, the latter is about the world. We cannot know anything about the world just by think about theories. Therefore, it is impossible for us to make any move from the meta-level to the object-level. 

There are two ways to answer this. The first way is to acknowledge this objection. For any object-level claim being discussed in the subsequent sections, one should translate it into a meta-level constraint. For example, I will argue for an agency-based interpretation for intervention. One should translate this as the following: we cannot coherently adopt a theory that claims semantic invariance yet does not give an agency-based interpretation of intervention.

The second way is to maintain that the entailment holds, and the hard-core metaphysicians have a wrong conception of metaphysics. According to the hard-core metaphysicians, it is possible that $P$, meanwhile it is impossible for us to coherently adopt that $P$. In other words, it is possible that, we cannot coherently adopt the "true metaphysics". 

But in practice, we reject this possiblity all the time. Assume $A$ and $B$ are two *inconsistent* sentences about the world. Anyone would agree that we must deny at least one of them, because they are inconsistent. But this justification rely on the following presupposition: *If a theory is inconsistent, then it is not a metaphysically true theory.*  But what justifies this? The only reason that I can conveive of is that, *we cannot coherently adopt such an inconsistent theory*. But this justification is not available for the hard-core metaphysicians, because "inconsistent theories are not adoptable" doesn't entail that "the world is consistent".[^15] Therefore, according to the hard-core metaphysicians, we do not have to deny $A$ or $B$. Maybe the "true metaphysics" really is somehow inconsistent. But this sounds insane. The referent of "the world" in metaphysics never entertains the possiblity that, it might be inconsistent. Indeed, if we allow this possibility, then I don't know if we can say anything meaningful at all about the world.

I admit this is a non-trivial move. So let me defend this further. What is the problem for the hard-core conception of metaphysics? According to this conception, we cannot derive anything about the world from how we would *think* about the world. But winding back to the beginning of this paper: *If we throw away all of our intuitions, then we cannot establish anything*. And the intuitive belief here is: we are rational agents. If we are not allowed to derive anything about the world from our intuition, then this kind of metaphysics can only belong to God, not to human beings. For human beings, then, we should pursue human knowledge and understand reality in a human way, not in God's way. Therefore, if metaphysics is to be a subject of human inquiry at all, it must accept our human limitations.

I personally stand with the second reply and allow the move from the meta-level to the object-level. However, for people who are not persuaded by this, they can adopt the first reply and translate my subsequent claims into meta-level claims.

#### Causation

I want to argue that *the Principle of Sanity* gives an agency-based interpretation to the interventionist approach to causation. After my defense of the interventionist account, I will then extend it to the analysis of fundamentality.

Let me give a quick introduction to this interventionist approach. This approach uses Structural Equation Models (SEM) to analyze causation. Nowadays among statisticians and scientists, this is the probably most popular way to make causal inference. The most basic interventionist causal analysis is this: $A$ causes $B$ if intervention on $A$'s state will change $B$'s state. The latter part can be formalized and evaluated by causal models. Let's quickly go through the basic formalization to get the gist. A causal model is $M=\{U,V,F\}$ where $U$ is a set of exogenous variables, $V$ is a set of endogenous variables, and $F$ is a set of structural equations that determines the value of endogenous variables by exogenous variables. A structural equation takes the form of $X=f(Y)$, where "$=$" does not mean equality, but means value assignment (like in programming). $X=f(Y)$ means that the value of $X$ is set to $f(Y)$. Given an assignment of exogenous variables $u$, we will have all the values in $M$ fixed. Assume $A$ is $a$ and $B$ is $b$ in the actual world. To analyze whether $A$ causes $B$, we surgically set the value of $A$ to $a' \neq a$  and therefore produce a new counterfactual model $M_{A=a'}$. Given the same assignment $u$, if value of $B$ in $M_{A=a'}$ is not $b$, then we say $A$ causes $B$.

Surely, this basic counterfactual account does not work in the cases of preemption and overdetermination. ==Halpern & Pearl 2005== developed a more complicated account of actual cause that solves these problems. But here I will not go into these details. Metaphysicians are more or less skeptical about this account *not* because of its detailed formalization. The worry of this approach is that, the analysis irreducibly rely on a "surgical intervention" to set the value of a variable. But this very concept of "intervention" is a causal concept. Therefore, this account is using causal notion to analyze causal notion, which is circular. 

And this circularity leads to *a problem of interpretation*. All languages need interpretation. The formal language of SEM also needs an interpretation. But if the semantic interpretation of SEM in turn requires another SEM, then it will lead to infinite regression. The standard reply is to admit that causation as primitive, and the notion of intervention depends on causation. But how come that we automatically have a primitive interpretation of causation? I believe more can be said.

##### The Strong Principle of Sanity

Now, I want to derive a positive account of causation from *the Principle of Sanity*. But the current interpretation of "rational agency" is *epistemic agency*. To the extent that it necessitates causation, it only necessitates the causation between *mental states*. Therefore, if we want to establish causation among physical objects, we have to adopt a stronger reading of "rational agency" in the _Principle of Sanity_. 

Now, "rational agency" not only include *epistemic agency*, but also *the capacity to control our bodies*. Call the strengthened claim _the Strong Principle of Sanity_. Note that my earlier defense appealed to the analytic entailment of epistemic agency from the capacity for philosophical reflection; That defense does not extend to *the Strong Principle of Sanity*: a smart ghost has no control over any body, but it might be capable of philosophical reflection. However, there are ways to make *the Strong Principle of Sanity* a priori again. For example, one could formulate it as 

- For someone who is capable of moving one's eyes intentionally in order to read this paper, it is impossible to genuinely believe oneself to not be a rational agent.

I will not further discuss the details of this maneuver. The gist is to make the constraint entail rational agency, and it is uncontroversial that we satisfy thiBut I think even without this constraint, it just seems insane for any human being to reject this.

Now given *the Strong Principle of Sanity*, we get the indispensability of the agency of bodily control. But this kind of agency just is mental-physical causation. I will use it to establish physical causation.

##### Solving the Interpretation Problem

Previously I discussed how to interpret "surgical intervention." Why must the intervention be "surgical"? Because we must ensure that the intervention affects only the target variable. Other variables may change as a result of intervention. But their changes should only be determined by the structural equations. The standard worry is that empirical interventions are never truly surgical. Assume we want to intervene on the current in a coil. We cannot directly create current to flow in a coil; we have to create a moving magnetic field. This in turn requires moving a magnet, and so on. And presumably this would lead to tons of variable to change. Therefore, no worldly intervention has the purity the SEM formalism presupposes, and the operation lacks an interpretation.

Given the _Strong Principle of Sanity_, the answer is straightforward. To surgically intervene on a physical event is to posit direct mental causation of that event. By mentally causing the relevant physical variable to take a new value, we obtain a clean, surgical intervention. And because interventionist models can analyze physical causation, we can, via SEMs, develop a full account of causation.

In other words, on my interpretation there is no problematic circularity in the interventionist account. It analyzes non-agential causation by appeal to a primitive notion of agential causation, and the *Strong Principle of Sanity* licenses that primitive.

In fact, agency-based interpretations were actually the predecessors of contemporary interventionism. ==von Wright (1971); Peter Menzies and Huw Price (1993)== developed agent-based accounts that aimed to reduce causation to agency. However, that kind of reductionism does not work, because it is just impossible to separate the notion of causation from agency. My interpretation differs. I do not try to reduce causation to non-causal notions; I am just analyzing non-agential causation in terms of agential causation, while maintaining that the interpretation of agential causation is guaranteed by my principle.

Another problem is more salient: an agent-based analysis of causation seems anthropocentric. A standard objection runs as follows: in the early universe there were arguably no agents. If physical causation is analyzed by agency, then there was no causation in the universe until agents existed. That consequence is surely wrong. So, the objector concludes, physical causation cannot be analyzed by agency.

My reply is that actual causation is analyzed by *possible* intervention. Such intervention depends not on actual agency but on *possible* agency. Therefore, even when there are no actual agents in the universe, there is still causation, because agency is still *possible*. Again, the notion of agency here is not anthropocentric: it specifies no particular human body parts. It must be so. We cannot know a priori which parts of the world comprise our bodies.

Let me explain where my argument lead us. The direct positive conclusion from the *Strong Principle of Sanity* require us to take mental and mental-physical causation to be *really out there*. And through *utilizing* interventionist's formal language, we can talk about physical causation. But it is not required for us to *adopt* this language and say physical causation is really out there. Indeed, what I am claiming is compatible with a deflationary view about physical causation, but not mental-physical or mental causation.

#### Fundamentality

Finally, it's time to go back to where I started: fundamentality. The reason why I spent so much time on the interventionist account is that, it has a very important feature that I need to establish fundamentality: It ensures the *asymmetry*. 

In the most basic counterfactual analysis of causation, A actually causes B if (1) A and B both occur in the actual world; and (2) if A had not occurred, then B would not have occurred. On Lewis-style possible-world semantics, (2) is true if, in the closest worlds to the actual world where A does not occur, B also does not occur. However, this semantics does not rule out symmetry. If, in the closest worlds where B does not occur, A also does not occur, we would conclude that B actually causes A. Setting aside the possibility of cyclic causation, causation should be asymmetric. Yet there is no straightforward way to modify Lewis-style possible-world semantics to guarantee this asymmetry.

By contrast, in interventionist semantics (setting aside cyclic models), the asymmetry is ensured. Let us illustrate with the toy model $M = \{\{A\}, \{B\}, \{B = A\}\}$ and the actual assignment $u(A) = 1$. The counterfactual "if A had not occurred, then B would not have occurred" is true because, given $u$, the value of $B$ in $M_{A=0}$ is 0. But the counterfactual "if $B$ had not occurred, then $A$ would not have occurred" is false because, given $u$, the value of $A$ in $M_{B=0}$ is 1. In other words, so long as the model is acyclic, the asymmetry is ensured by design.

Is this circular? It seems that the structural equation $B = A$ in $M$ already presupposes this asymmetry. If the reason we treat $M$ as an appropriate causal model is that, we presuppose that $A$ causes $B$, then it is circular. However, structural equations can be discovered empirically via intervention tests. For example, to test whether $B = A$, we can intervene on $A$ and observe how the value of $B$ changes. Therefore, the world can teach us whether is an appropriate model, without circularity. And given that *the Strong Principle of Sanity* ensures the semantic invariance of intervention, we do not need to worry about a strange creature who interpret it differently.

Now, recall my problem for Bennett's account of fundamentality: for any mutually necessitating relations, we were unable to privilege a direction for asymmetry. Bennett used the generative constraint to ensure the direction, but this constraint is framework dependent. Therefore, it does not ensure the invariance of the privilege.

However, given the invariance of SEM, the direction provided by SEM is privileged. But let me slow down: SEM is used for causation. How can it be used for fundamentality?

In fact, using SEM to analyze fundamentality is no new idea. ==Schaffer grounding in the image of causation== and ==Wilson metaphysical causation== have all written in detail on how to use structural equations to analyze fundamentality, and even see horizontal building as metaphysical causation. The hard question, however, is how to choose the *appropriate* model for analysis. In the case of causation, we can do intervention tests to empirically discover the appropriate equation. But it's not clear how to do intervention tests on singleton Socrates.

Let me use a toy example to illustrate this problem. Assume we choose a model where the structural equation is $\{Socreates\}=Socrates$. When we intervene on $Socrates$, the value of $\{Socreates\}$ would change. When we intervene on $\{Socreates\}$, the value of $Socrates$ would not. Therefore, we can easily get the result that Socrates builds singleton Socrates.

But the problem is, why do we choose $\{Socreates\}=Socrates$ as the *appropriate* equation? Recall the previous challenge. Robo would choose $Socrates=\{Socreates\}$. Why is Robo's choice problematic? I have not seen a satisfying reply in the current literature. For example, Schaffer seems to adopt a neutral position about model choice, and thus did not provide reason against Robo's choice.

Recall Bennett's formulation: "building", "making" and "generative". All of these are agential concepts. I believe that, Bennett's intuition aligns with my analysis. There must be something related to agency that leads to the directness that we want. Informally speaking, we can in some sense build singleton Socrates out of Socrates, but we cannot build Socrates out of singleton Socrates.

Let me explicate this intuition into an argument. According to my previous discuss, intervention depends on agency. But agency depends on intention formation. If an intervention is inconceivable, then there is no way to form an intention. If there cannot be such an intention, then the agential intervention is impossible. 

Now I want to argue that, there is no way to conceive of a surgical intervention on singleton Socrates. In other words, we cannot conceive of a way to directly modify singleton Socrates. We can intervene on Socrates, or maybe on set theory. But these are all indirect intervention, and thus not surgical. Therefore, a surgical intervention on singleton Socrates is impossible.

If an intervention is impossible for a variable, then the interventionist counterfactual "If we intervene on singleton Socrates, then the value of Socrates would change" is ill-posed. Therefore, it is impossible to establish $Socreates=\{Socrates\}$, because there is no way to intervene on $\{Socreates\}$.

Is this a good reason to show that, Robo's metaphysics is flawed? It doesn't seem so. This reason is still symmetric. Robo could say, they cannot conceive of an intervention on Socrates. But they can conceive of an intervention on $\{Socrates\}$. Does this mean that my attempt failed? No. What Robo mean by "agency" would be fundamentally different from ours. In our term, Robo is not a rational agent. Maybe Robo is a rational schmagent. But given *the Strong Principle of Sanity*, we cannot give up that we are rational agents and claim we are rational schmagents. Therefore, the principle tells us that, Robo's view about "intervention" is not adoptable for us. Therefore, their view on fundamentality is also not adoptable for us. If you accept the previous move from meta-level to object-level, then we should say the following: It is objectively true that, the appropriate equation is $\{Socrates\}=Socrates$. Therefore, $Socrates$ is more fundamental than $\{Socrates\}$.

Let's now check again where my argument leads us to. First, What I argued so far easily extends to building relations other than set-formation, but to save space, I leave this to the readers. Second, my argument established an account of how to establish a privileged direction for building relations. Like the previous discussion of physical causation, this is compatible with a deflationary view about the building relations established by SEM. However, there is one thing that cannot be deflated: the mental-physical causation required by rational agency. In fact, maybe it's better not to call this causation, because I am inclined to say this is atemporal. Anyway, the fact that, our intention *builds* our action must not be deflated. In this sense, fundamentality cannot be not altogether deflated into interventionist formalism, because this mental-physical building relation is required by our sanity.

## Beyond metaphysics

The next thing I want to show is that, *the Principle of Sanity* should be taken seriously because it is not just a metaphysical principle. It is supposed to be a constraint on any philosophical investigation. In order to show this, I want to talk about a consequence it implies in ethics.

Consider the follow dilemma from ==Baker 1987 trust and rationality==: Imagine your closest friend is Smith. One day, the police show up at your door with compelling evidence against him: surveillance footage shows someone who looks just like Smith robbing a bank at gunpoint, and they've found his fingerprints at the scene. The police don't know where Smith is and ask you to contact them if you see him. A few days later, Smith appears at your door. He desperately claims he's been framed and had nothing to do with the robbery. The question is: should you trust his word?

==Hieronymi 2005 The reasons of trust== argues that in this case we cannot rationally trust our friend. She argues that once we think about this belief, we'll find it lacks proper foundation. This is because only beliefs formed through a truth-directed process can withstand rational reflection. If one is rational, then one will lose the belief formed by practical considerations alone.

In my defense for *the Principle of Sanity*, I have already argued against Hieronymi's truth-directed view of rationality. But to make this clearer, let me now defend the connection between sanity and trust.

Before we proceed, I need to reinterpret rational agency again. What I mean by "rational agency" now is *moral agency*. And by moral agency, I mean the ability to have what ==Williams 1985 ethics and the limits of philosophy== called "thick ethical concepts", such as courage, treachery, brutality, and gratitude. Again, I can make some maneuver to make this principle a priori again:

**The principle of moral sanity**
: For someone who is capable of living an ethical life, it is impossible to genuinely believe oneself to not be a rational moral agent.

This interpretation of rational agency is quite different from the previous control-based interpretation. However, I believe this sense of rational agency is also a part of our commonsensical interpretation. And again I am inclined to say that, it is just insane to believe that one is not a moral agent.

Let me lay out my argument first and then defend them step by step. First, I'll argue that for human beings, being a moral agent is impossible without social interaction. Second, social interaction is impossible without assuming that others are being truthful. Given our friends are central to our social interaction, trusting friends is necessary for sustaining moral agency. If unfortunately, your evidence constantly conflicts with a friend's testimony, your sanity requires you to trust your friend against the evidence.

Let's start with the first step: *Being a moral agent requires social interaction*. Bernard Williams argues extensively. Williams distinguishing between "thick" and "thin" ethical concepts. Thin concepts, like good, right, and ought, are abstract and useful for theoretical reflection, but they are detached from any specific context. Thick concepts, e.g. courage, treachery, brutality, and gratitude, are rooted in a specific social world and guide our actions within it. We live and act in specific communities, not in an abstract vacuum. Our action is always conducted in a specific community. Therefore, the reasons for our action must irreducibly reply on thick concepts. And we can only acquire these community-dependent concepts through social interaction, not by pure reflection alone. Therefore, social interaction is essential for being a moral agent.

It is important to clarify that Williams' argument is not an *a priori* claim; it is an empirical claim about human beings. Williams draws an analogy with language: for humans, acquiring a language is impossible without social interaction. While plausible, this isn't a necessary. We can certainly conceive of a creature born with the innate ability to speak English. Similarly, we can imagine a creature born with full-blown ethical concepts, requiring no social learning. Williams' point is simply about how we, as human beings, *actually* acquire the concepts essential to our moral agency, a point I find plausible.

Now the second step: *social interaction is impossible without assuming that others are being truthful*. What is social interaction? Consider the difference between interacting with your phone and interacting with people. When we interact with our phone, we don't try to understand what it's thinking. We assume it has no thoughts or attitudes and simply use it based on its functions. In contrast, when interacting with people, we must figure out their intentions and attitudes, then react accordingly. In other words, social interaction requires reaction to other's attitude. And reaction to other's attitude requires understanding other people's thought. Therefore, social interaction requires understanding other people's thought.

The problem is, we can't read minds. Our only way to understand others is through their behavior (including linguistic behaviors). But given someone's behavior, how can we know what they're thinking? For example, imagine a person who acts relaxed when they feel pain, but says "ouch" and winces when they feel calm. It seems we could only interpret their behavior correctly if we already knew what they were thinking.

Now we are in a circle. To know what other people are thinking, we must appeal to their behavior. But to know what their behavior means, we must first know what they are thinking! This, in fact, is a familiar circle coming from the discussion of radical translation. 

Here is the solution: If there is any hope at all to understand another person's thoughts, we must start with a basic assumption: that their behavior is the kind of behavior we would exhibit if we had the relevant thought. For instance, suppose we see Quine pointing to various tables and saying "teburu." Our natural starting point is to interpret "teburu" as "table."

However, this is only a starting point, as misalignments can occur. We might then notice that whenever a table is made of wood, Quine calls it "tsukue" and insists it is not a "teburu." Should we conclude that Quine has made a mistake, since our hypothesis is that "teburu" means "table," and a wooden table is still a table? If we do this, Quine's insistence becomes incomprehensible. Instead, the best way to make sense of his linguistic behavior is to revise our interpretation: "teburu" means non-wooden tables, and "tsukue" means wooden tables.

This kind of solution was proposed by ==Davidson citation== as *the Principle of Charity.* This principle has nowadays become a widely accepted cornerstone of metasemantics. But what is the motivation for this principle? In essence, it asks us to try our best to understand others by the following procedure: Interpret their thoughts in a way that makes their behavior appear as *reasonable* as possible. If their behavior cannot be made reasonable, then we simply cannot understand what they are doing.

When it comes to linguistic behavior, what makes an assertion reasonable is that it comes out as *true*. In the previous example, What makes Quine's insistence that a wooden table is not a "teburu" *reasonable*? It's the *truth* of the statement "This is not a teburu", according to Quine's meaning of "teburu". By adjusting our interpretation, we make Quine's behavior understandable. And this surely extends to non-linguistic behavior. If Quine is staring at you with a frown on his face, then the most reasonable interpretation is this: His attitude towards you is not very friendly.

Most importantly, this entire process of understanding collapses if we don't assume that others are being *truthful*. To be truthful simply means to behave corresponding to what they actually think. Without assuming this correspondence between behavior and thought, no reasonable interpretation is possible. For instance, if we were to assume Quine is deliberately messing with us, there is no way we can figure out what on earth does "teburu" mean, or what his attitude is when he is staring at you with a frown on his face. Therefore, to gain understanding at all, we must assume that others are being, in general, truthful.

Previously I have shown that, social interaction requires understanding other people's thought. And now I have shown that, understanding other people's thought requires the assumption that others are being truthful. Therefore, we get the conclusion: social interaction requires the assumption that others are being truthful.

Now the third part: *Given friends are central to our social interaction, trusting friends is necessary for sustaining moral agency*. This is not really a logical step. I tend to use the word "friend" as a terminology here. I stipulate that, those who are central to our social interaction are called "friends". And to assume one's truthfulness just means to trust someone. In the first step, we proved that moral agency is acquired by social interaction. In the second step, we proved that social interaction is impossible without trusting those who you socially interact with. And given the stipulation that friends are those who you mainly interact with, we get the conclusion: to acquire (and sustain) moral agency, we are required to trust our friends most of the time.

To be clear, this doesn't mean we can never distrust our friends. It's certainly possible to maintain enough social interaction while occasionally believing a friend is lying. The problem arises in the case where you consistently have evidence that contradicts what most of your friends say. If you always stick to the evidence in such a situation, your friends' behavior will become incomprehensible. This would become an exclusion from normal social interaction. And we would lose grasp of our ethical life after being excluded from the social world. In this unfortunate scenario, *the Principle of Moral Sanity* suggests that to remain morally sane, one must trust friends' words against the evidence.

According to how Hieronymi defined rationality, it is rational to be insane. Therefore, it is rational to reject *the Principle of Moral Sanity*. Could this be a plausible position? In the discussion of *the Principle of Sanity*, I argued that rationally rejecting it would be self-defeating, because rational rejection requires epistemic agency. Rejecting _the Principle of Moral Sanity_, however, is not self-defeating in this way. I would choose moral sanity over rationality, as I believe a good life essentially constitutes an ethical life. But if one insists that rationality trumps moral sanity, then maybe this is where rational argument comes to an end. The choice between moral sanity and rationality might ultimately be a choice about one's form of life.

## Conclusion

Now let's go back to the question that I raised in the beginning of the paper: *How much reality should we read off our linguistic features?* My reply is, we must at least read the things that sustain our rational agency off our linguistic features. For example, the first-person pronoun, certain sort of causation and fundamentality. Further than that, I remain neutral and tend to adopt a general form of *always-ism*.

So far I have suggested three senses of rational agency: epistemic agency, bodily agency, and moral agency. They correspond to *the Principle of Sanity*, *the Strong Principle of Sanity* and *The Principle of Moral Agency*. Although one can adopt some and reject some, I believe it is more plausible to accept them all. I believe they all derive from the commonsensical notion of rational agency.

What would be the takeaway point of this whole paper, then? I think the point I want to make is the following. Throughout the history of philosophy, there has always been a strong impulse to secure an absolute truth that is objective and eternal. But this is impossible. Again, only deductive proofs can secure truth in this fashion. But any deductive proof relies on premises. Ultimately, some premises will have to depend on our intuitions. If so, then we should not abandon the kind of intuition that we cherish in our ordinary life: our own rational agency. To adopt a worldview that eliminates this that is, for us, simply insane.


[^1]: Needless to say, in a complete model for English language, many other vocabularies need different interpretations.
[^2]: I'm focusing on quantifier variance because I believe it's the most plausible and basic form of the neo-Carnapian position. ==Thomasson (2015 p. 70)== herself thinks her easy ontology doesn't depend on quantifier variance, but this is surely mistaken. Her project relies on the triviality of changing the extension of the predicate "exists". But this surely changes the meaning of the quantifier.
[^3]: Surely this paper is not intended to become the zoology of metametaphysics, so these labels may not be precise. What I mean by neo-Aristotelians is basically the fans of fundamentality. For example, Sider counts as a neo-Aristotelian in my sense. Though Sider is usually considered as a neo-Lewisian, which is closer to neo-Quinean. But the label is not important anyway.
[^4]: A less plausible answer is that they cannot make sense of my question. My reply is, I cannot make sense of this answer. As long as one can make sense of the distinction between merely utilizing a language and truly adopting and think in a language, my question should make sense. For example, even Hirsch accept this distinction in ==Dividing Reality, p.14==: "*one does not abandon a position merely by adopting a language*". There are some terminology issue here. What Hirsch means by "adopting a language" is what I mean by "utilizing a language", and what Hirsch means by "taking a position" is what I mean by "adopt a language".
[^5]: This "under the guise" talk may be more rigorously put in 2D-semantics, but gist is simple. It is motivated by the following kind of phenomenon. "Superman can fly" and "Clark Kent can fly" are the same fact, but they are different objects of doxastic attitudes: one might believe the former but not the latter. One way to explain this is to say that someone believes of $x$ *under the guise* of 'Superman', that $x$ can fly, while not believing $x$ under the guise of 'Clark Kent' that $x$ can fly. The basic idea of the "guise" account is that, even if an expression directly refers without relating to a description, the belief content may still involve something linguistic beyond the referent. Kripke's puzzle about beliefs shows that, this also extends to sentences (==citation to Kripke==). A French sentence $S_{F}$ and an English sentence $S_{E}$ may co-refer to the same proposition $P$, yet one may believe that $S_{F}$ but disbelieve that $S_{E}$. The solution is to say one believe that $P$ *under the guise of* $S_{F}$, but disbelieve that $P$ *under the guise of* $S_{E}$. If there are equivalent languages can serve as equally good guises, there are many different ways to form beliefs about the reality. Each of these guises constitutes genuine, true knowledge. This move allows neo-Carnapians to embrace a form of lightweight realism, distancing themselves from an anti-realist never-ism. 
[^6]: However, this approach immediately faces a challenge. If I treat other people's languages as mere guises of reality, why shouldn't I treat my own language as a mere guise as well? Moreover, if my language is also a guise, then it seems we would collapse back into the anti-realist's never-ism, because it presupposes a reality that is unspeakable without guise.<br>In fact, the answer to the challenge is worth a paper-length discussion, so I cannot fully address it here in a footnote. To sketch the answer, consider the following case. First, let's establish that the set of all well-formed English sentences, taken as finite strings, is countable. We can have a bijective function $f$ that maps every unique English sentence to a unique natural number. From this, we can construct a One-Word-Sentence language (OWS-language for short). In this language, each natural number (and thus each corresponding English sentence) is represented by a single, unique symbol (e.g., '∇', '§', 'Д'). Crucially, these symbols carry no internal syntactic information that mirrors the structure of the original English sentences. Now, imagine a creature that genuinely thinks in this OWS-language, not one that is secretly translating from English. Suppose that when we apply the inverse function, $f^{−1}$, to decode their one-word utterances, we find they agree with us on the truth value of every proposition. They even affirm the OWS-symbol that decodes to "There are objects and properties." Here is the problem. This creature's way of thinking is inconceivable to us. Given we stipulated that they think in OWS, it seems they cannot grasp the concepts of objects and properties. Yet, their statements, once translated, shows they think there are objects and properties. It is in cases like this that we must appeal to the "guised reality" view. We are forced to say that they believe there are objects and properties under the guise of their OWS-language, because there is no other way to understand their take on reality. Or in other words, we are *unable* to read reality off this OWS language. In contrast, our own understanding is transparent. We do not take of our language as a "guise" because it is normally a transparent *presentation of the reality*, not representation. We only appeal to "guise" in specific cases of misalignment, such as with co-referring proper names like 'Superman' and 'Clark Kent.'
[^7]: This is probably not the orthodoxical formulation. The orthodoxical formulation is, we distinguish absolute fundamentality and relative fundamentality. For absolutely fundamentality, we usually appeal to concepts like completeness, dependency, indispensability or naturalness. But when asked which set of things has this feature, we appeal to concepts, like grounding. And the standard story is to treat these relevant concepts as primitive. But then we need an epistemology for this primitive grounding. Ultimately the conclusion of "what grounds what" appeals to virtues like explanatory power or definitional simplicity. Therefore, ultimately we decide what's absolutely fundamental based on explanation or definition. I have not seen how could this be done otherwise. This also applies to relative fundamentality.
[^8]: In fact Sider tend to drop the term "naturalness" to favor the term "joint-carving" or "structural" for the extended meaning of "naturalness". But let me stick to "naturalness" here.
[^9]: In fact, there might be resistance. One can claim that, there can be reference magnetism when we want to use words in this way. However, according to this account, which referential candidate gets the magnetic power still depend on our disposition of use. Therefore, I don't see how there could be any heavyweight reference magnetism that can support Sider's view.
[^10]: One possible approach is to define the essential meaning of a term by its introduction and elimination rules (IE-rules), that is, by its functional role in language. This may be intuitive for logical constants. For a term to count as a quantifier, for example, it must obey the IE-rules for quantifiers. However, the full meaning of a quantifier also depends on its domain, and the IE-rules don't fix this. Therefore, the meaning of a quantifier can still vary as its domain varies. This approach faces several problems. First, this form of essentialism about meaning seems arbitrary. Why should IE-rules be the only thing that constitutes a term's essential meaning? For example, we might say in some sense 'quus' is a variation of 'plus', not a completely unrelated concept like 'Hello'. But they have different IE-rules. Second, for non-logical terms, the IE-rules themselves are usually formulated using terms that can vary. Third, and most importantly, even if we accept this IE-rules essentialism, it would not help fix "naturalness". It leads to a specific conclusion: a term's meaning is invariant if its meaning is exhausted by its IE-rules. For example, a logical conventionalist might say $\land$'s meaning entirely determined its IE-rules. Then according to IE-rules essentialism, $\land$ is invariant. However, Sider clearly intends "naturalness" to be more than just a conventional, syntactic symbol. Therefore, this strategy for securing semantic invariance is not a viable option for his project anyway.
[^11]: If this example is puzzling to you, you can try the following notation: she uses "Socrates" to refer to $\{Socrates\}$, and "-Socrates-" to refer to Socrates. Mathematically speaking, set formation has nothing to do with intrinsicality. Our intuition that *a member of a set* is somehow intrinsic and thus more fundamental than *the set itself* is just our *interpretation* of set formation, influenced by ordinary language words like "member" and how we use brackets to warp the members. But we could have used alternative notations, like using "M-relation" instead of membership, and use $\{\}-Socrates$ intead of $\{Socrates\}$.
[^12]: Bennett's formulation of the necessitating requirement is a bit more complicated than this. She also consider the circumstance of necessitation (==p.52-54==). But here I will just use the cleaner way to formulating it for the toy example.
[^13]: In other words, I am in fact suggesting two different principles. The stronger version is what I explicitly laid down and inclined to stand with. But even if one somehow resisted it, my conclusion can still be established by the weaker version, i.e. if we are capable of philosophical reflection, then we should not believe we are not rational agents.
[^14]: ==Hirsch forthcoming== discussed this corollary in chap. 3.
[^15]: The world is "inconsistent" is not a good formulation. In theory, consistency is a syntactic notion. It is a categorical mistake to apply it to the world. Maybe the better way to formulate the sentence is as follows: the world is such that, the "true" theory about the world will come out inconsistent. But for brevity, I will use the term "inconsistent world" to mean this.