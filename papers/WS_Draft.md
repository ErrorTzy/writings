---
documentclass: exam
title: Draft
author: Scott Tang
mainfont: FreeSerif
CJKmainfont: Noto Serif CJK SC
mathfont: XITS Math
fontsize: 12pt
---
# I

## Intro 

This paper is probably not a typical metaphysics paper. Although the main upshot is to establish fundamentality without magic, the kind of move I defend applies beyond metaphysics. 

In the background, this paper deals with a central tension. On one hand, it seems ultimately all possible philosophical conclusions must be derived from intuitive presuppositions. If so, then we cannot establish anything if we throw away all of our intuitions (or dispositions, sentiments, etc.). On the other hand, many of our intuitions are unreliable, and even contradict one another. The problem, then, is to figure out which part of our intuition cannot (or at least, should not) be thrown away and can safely rely on.

In the foreground, this paper focuses mainly on metaphysics, but the argument I made for metaphysics also extends to other areas, like ethics. To be specific, I first argue that the background tension is somehow ignored in discussions about fundamentality. As a result, the idea that some things are objectively fundamental, though well-motivated, seems to rely on magic or wishful thinking. 

Then I argue for a principle that basically says, we cannot throw away our rational agency. The rest of the paper explores the consequences of this principle. I mostly use it to establish a direct, non-magical account of fundamentality. But to show the principle's broader power, I also apply it to a problem in ethics: why we should sometimes trust a friend's word, even when it goes against the evidence.

## The Question of Reality

The starting point of this paper comes from the following question: *How much reality should we read off our linguistic features?*

This way of asking immediately requires clarification. First, it seems to suggest linguistic idealism. It doesn't. Reality certainly is not metaphysically constructed upon our language in any way.

Without language, we might still have ways to understand reality, perhaps through perception or mysterious experiences. But, this is not how metaphysics works. In metaphysics, we inevitably rely on sophisticated argument that relies on intuitive premises. But this has generated a long-standing worry in philosophy. Since medieval times, philosophers (e.g. nominalists) have been worrying that our intuition about the world comes from words instead of reality. Hence my question: when we try to talk about reality, which parts genuinely reflect how things are, and which parts are mere "shadows of language"?

This question appears central to almost all metaphysical problems. For example, 

- Almost all natural languages have a subject-predicate structure. Should we then say, the world itself is constituted of objects and properties? 
- Grammatically we have predicates like 'good' or 'bad'. Then should we read them off our language and say there are such properties in the world? 
- The proposition $P$ may seem syntactically more primitive than $\neg \neg P$. Should we read this structure off our language and say the *fact* that $P$ is different, and in some sense more fundamental than the *fact* that $\neg \neg P$? 
- Quantum physics describes the world in terms of wave functions. Should we then interpret that reality as just a bunch of waves (or maybe, a huge, holistic wave)?

It is obvious that not every linguistic feature reveals reality. Take the proper names "Superman" and "Clark Kent." We don't think there are really two distinct entities out there connected by a relation named "identity". This is a non-trivial claim. Formally, one could construct a toy model $M = \langle D, I \rangle$ with $D = \{s, c\}$, $I(\text{'Superman'}) = s$, $I(\text{'Clark Kent'}) = c$ and $I(\text{'is'})= \{ \langle s,c \rangle, \langle c,s \rangle \}$. Such model would satisfy 'Superman is Clark Kent'.[^1] But intuitively we do not take such a model to capture reality because we think 'Superman' and 'Clark Kent' should refer to the same entity in the domain. 

This question can be taken as a new way to formulate realism. The traditional approach formulates realism in terms of *truth*. For example, scientific realism is often described as the view that the picture science gives us of the world is a true one (==citation from BVF==). And anti-realists like van Fraassen argue that this picture isn't really true, but merely _empirically adequate_.

However, this way of framing the debate relies on a problematic conception of truth. The problem is that *truth is cheap*. Consider a game of chess: no one would think we are saying something false when we state, "There is a bishop on the board." Yet, the truth of that statement is partly established by the rules of chess, not by the world itself. This shows that truth can arise from linguistic rules, not just from reality.

The traditional way of framing realism in terms of truth creates a loophole for deflationists. It allows them to call themselves "lightweight realists" simply by agreeing that our metaphysical claims are true, while maintaining that this truth is superficial. But we do have a strong intuition to say, there is something deeper than the lightweight view. If we frame the question of realism in my fashion, however, then the deflationists must say more. For example, according to easy ontologists, adopting a language is easy, and thus ontology is easy. (==citation from Amie Thomasson==) But ontology can't be that easy, because it is not trivial to freely read reality off language. And the explicit distinction between language and reality is intended to capture the heavyweight idea of what is "really" out there. For example, Sider's project about Ontologese is best understood as, we should only read the (most fundamental) reality off the perfectly natural language, not unnatural languages that include kinds like "incars" and "outcars".

Another thing needs to be clarified. I use "language" in a broad fashion. First, it includes inferential rules and laws. In other words, "theory" and "language" are interchangeable in this paper. For example, Newtonian physics and classic logic are arguably part of our ordinary language. Second, language-like mental process also count as language. When I say someone is "using a language", I mean that person is also *thinking in this language*. For example, one is *not* using English if they are in fact thinking in German, then use Google Translate so that eventually the utterance comes out in English.

The way I've framed the question, i.e. how much reality we should read off our linguistic features, is different from the standard story told in metametaphysics. I think the standard narrative, which I'll lay out in a moment, tends to miss something important. I take the familiar story told by ==Schaffer (2009 on what grounds what)==, which is the most-cited paper in this field, as the orthodox version (though I'll be telling a bit differently).

### The standard story

It goes something like this: contemporary metaphysics began with the Quine-Carnap debate. After Quine arguably won the debate, his approach to metaphysics prevailed. For Quine, the main task of metaphysics is to figure out what indispensably exists. Whether numbers or electrons exist depends on whether we must quantify over them in our best theories. This methodology was extended to other topics. Lewis, in some sense, can be seen as following the Quinean methodology: Since our best account of modality requires quantifying over possible worlds, possible worlds are real. From there, other concepts can be analyzed. Causation is explained by counterfactuals, which are analyzed in terms of real possible worlds. Modality de re can be analyzed by real possible worlds, plus some pragmatic, context-sensitive similarities. The takeaway is that most problems in metaphysics can be solved by first figuring out what exists and then maybe add a dash of pragmatics.

But then, the neo-Carnapians entered the scene. I believe the most important the idea from the neo-Carnapians is that, the meaning of "exists" can vary.[^2] According to this view, when different people debate about what exists, they are speaking different languages. If their languages are truth-functionally equivalent, then this is a mere verbal dispute.

Imagine one side speaks A-language and the other speaks B-language. For any sentence uttered by A-speakers, B-speakers will have their respective interpretation the sentence, vice versa. If the interpretation from each side always agrees on the truth value, then there's no disagreement other than disagreement on language: the other side is just expressing the fact differently. If that's the case, then many debates about what exists are merely verbal.

The implication here is serious. If metaphysics is founded on questions of existence, but quantifier variance shows that there are many equally good meanings of "existence" and the debates are merely verbal, then metaphysics risks becoming a largely trivial field of study.

Meanwhile the Neo-Aristotelians also come into play.[^3] They also attack on the neo-Quineans. They argued that even if we had a complete list of what exists, we'd still be missing something important: what is most fundamental, and what grounds what. Schaffer, for instance, argues that questions about existence are trivial. He says that of fictional characters and abstract objects trivially exist. The real, interesting question is about what grounds their existence. This shift in focus from existence to fundamentality has been the dominant trend in metaphysics for the past twenty years or so.

### The problem with the standard story

Here's the problem. From the neo-Aristotelian perspective, this makes them look like the next logical step. In this view, the neo-Carnapians performed a useful, but purely *negative*, task by showing the flaws in the neo-Quinean focus on existence. With that groundwork laid, the neo-Aristotelians can then step in to offer a *positive* new project, turning the page on the neo-Quine-Carnap debate and building something new around concepts like grounding and fundamentality. (==citation==)

As a result of this framing, the standard story tends to sideline the neo-Carnapians. This isn't to say they're completely ignored, of course; people like Sider, Schaffer, and Bennett certainly take them seriously. Rather, the problem is that the standard story implies the neo-Carnapian critique is *only* about existence. And if that's the case, then the neo-Aristotelians, with their new focus, can safely disregard it. 

But this is surely *not* the case. The neo-Carnapian argument, on a high level, is about semantic variance. Therefore, it applies to any theory as long as the theory allows semantic variance. 

In order to see this more clearly, let's return to the question I framed at the start: *How much reality should we read off our linguistic features?*

**For the neo-Quineans**, the answer is *sometimes-ism about existence*. We can only sometimes read entities off our ordinary language. We need to paraphrase our ordinary language and scientific theories. While the exact rules for this paraphrasing are messy, but the basic idea is to create a language that maximizes pragmatic virtues like simplicity, elegance, and explanatory power. It is from this idealized, paraphrased language that we are meant to read real entities off the values being quantified over.

**For the neo-Carnapians**, the most plausible answer is *a general form of always-ism*: we can always freely read reality off any languages we think in, as long as they are truth-functionally equivalent.[^4] When another party, say John, uses a different yet equivalent language, there's nothing wrong with John, nor is there anything wrong with us. First, we can interpret both sides as speaking the truth, so there's no disagreement about facts. Second, from our point of view, there's nothing wrong with "John's way of thinking" because his resulting beliefs are also true. Here's why: if a sentence $S$ is true in John's language​, then it is true for John to believe that $S$ *under the guise of* John's language.[^5] By the same token, from John's point of view, there's nothing wrong with us either, whether in terms of truth or in terms of beliefs. I leave some complications of this account in a footnote.[^6]

**For the neo-Aristotelians**, it seems they are holding *sometimes-ism about explanation or definition*, and *always-ism about existence*.[^7] Everything we can talk about trivially exists, but it might be derivative or idea-dependent. And the way we trace this kind of dependency is through the structure of definition or explanation (==citation to Fine's essence and modality, guide to ground, and Schaffer's work==).

The essential feature of definitions (and explanations) is that they are inherently *asymmetric*: nothing can define itself, and if $A$ is defined by $B$, then $B$ cannot be defined by $A$. However, this only applies to "real definitions," not "nominal definitions." For example, we may nominally define "A is on the left side of B" by "B is on the right side of A," and define "B is on the right side of A" by "A is on the left side of B." In fact, this kind of circularity probably appears occasionally when terms are defined in the dictionary.

Given this distinction, we cannot say that any definition in our language establishes fundamentality. Only the "real" ones trace fundamentality. The same principle applies to explanation. Although grounding relations are explanatory, they are not explanation relations per se. Explanation is (in)famously context-sensitive (==citation to van Fraassen, Kitcher==). It depends on things such as what we're emphasizing, and what interests our audience. Grounding relations, by contrast, are intended to be worldly, "objectively objective", and context-independent. Since they are separate, only "real" explanation can trace fundamentality. This leads neo-Aristotelians to embrace *sometimes-ism* about explanation or definition.

Now we can clearly see the issue. Neo-Carnapians are arguing for a general form of *always-ism*, which would naturally includes the *always-ism about explanation or definition*. The pressure that the neo-Carnapian argument would put on the neo-Aristotelians' *sometimes-ism* is that, what makes some definition or explanation privileged so that they get to trace fundamentality, but the other doesn't? 

I will later defend sometimes-ism against always-ism. In fact, the *general form of always-ism* is already under pressure. In the previous paragraph, I constructed an exotic toy model that satisfies "Superman is Clark Kent". According to always-ism, there is no problem with this model. Something already seems wrong. ==I later address this problem. Will I?== But for now I will ignore these problems, put on the hat of neo-Carnapians and explain why the neo-Aristotelians did not give any satisfying answer. 

In the following three sections, I will leave aside the neo-Quinean-flavored view that, fundamentality is established by fallible epistemology determined by the best theory of fundamentality that has the most pragmatic virtues like simplicity, elegance and explanatory power. I argue against this view in the second part of this paper. In the next three sections, I attack the view that, we can objectively establish an absolute account of fundamentality that does not lead to semantic variance.

### Naturalness

Let me start with a way to establish a privileged definition using a familiar concept: *naturalness*. This notion was famously introduced by Lewis (==Citation a new theory of universals==), but his account was generally restricted to properties. It was Sider who attempted to use this concept to establish fundamentality (==Citation writing the book of the world==). Basically, Sider extends the notion of naturalness such that linguistic features beyond predicates (e.g., quantifiers, sentential connectives, sentential operators, predicate operators) can be evaluated by whether they carve the world at its joints or not.[^8] The linguistic features of the best language would then perfectly match the joints of reality. Given this extended understanding of naturalness, Sider contends that the most fundamental layer of the world is revealed by the structure of this best language. In this spirit, a "real explanation/definition" is the explanation/definition that only uses perfectly natural expressions.

But surely if the meaning of "naturalness" could vary between languages, then different languages could be considered "the best language" under different semantic interpretations of "naturalness." This is something Sider wants to avoid, because he argues that "naturalness" is supposed to be absolute or "objectively objective." The problem, then, is what makes the meaning of "naturalness" invariant across languages?

One way to defend this invariance is to appeal to reference magnetism: naturalness is itself natural, so the meaning of the term "naturalness" automatically gravitates to the most natural candidate meaning (==Sider p.23==). Sider stipulates that naturalness itself is perfectly natural (or, in his terminology, "structure itself is structural" ==p. 137==). Therefore, there is a unique answer to what the "reference magnet" is.

But this doesn't solve the problem. There is no contradiction in having two different languages, both of which agree on the sentence "naturalness itself is perfectly natural" while disagreeing on the extension of "naturalness." For example, in a mereological essentialist's language (ME-language), the ME-quantifier only ranges over simples, while in an ordinary language (OD-language), the OD-quantifier also ranges over composite objects. Assuming they both agree that naturalness itself is perfectly natural, there seems to be no contradiction in each side claiming its own quantifier to be perfectly natural, as long as they interpret "perfectly natural" differently. 

But the greater issue for this solution is that, it is more plausible to take there to be no reference magnets. As Hirsch (==citations==) and Warren (==2023==) pointed out, in the case where language use can determine the reference, magnetism cannot override it. Assume there are Martians seems as intelligent as we are. But they use '+' in a way that we need to interpret it as quus to make their utterance come out true. In this case, we should say the Martians are using '+' to mean quus. And in the cases of vagueness, like bald, where use cannot determine reference, we think it is just indeterminate. In other words, when language use determines the reference, magnetism does not exist. When the use does not determine the reference, magnetism does not exist. Therefore, magnetism does not exist. 

The initial motivation to posit reference magnetism is to explain why our '+' means plus, not quus. It seems our *actual use* does not determine plus, while there is a strong impulse to say the meaning of '+' is plus, not quus. But "actual use" might be a too narrow view about language use. Language use should include our disposition of use, and the disposition is enough to say '+' means plus. More will be said later, but for now I believe this should suffice to dismiss reference magnetism.

Let's return to the original problem: if reference magnetism fails, then what makes the meaning of "naturalness" invariant across languages? More generally, when is any term's meaning fixed in this way?

In a trivial sense, of course, any symbol can vary in meaning; "naturalness" could be used to mean "hello," for instance. But this isn't the kind of meaningful variation we're concerned with here. There might be several ways to explain "invariance".[^9] But the relevant idea might help establish the invariance of "naturalness" here is that, the world can somehow teach us whether we are getting the right concept. If we are using a wrong concept, the world will correct us.

What do I mean by "the world can teach us the right concept"? Special relativity seems to be a nice example. We naturally think that changing inertial reference frames is just a sheer transformation, and the speed of light would depend on the observer's motion. But empirical facts tell us that, surprisingly, our intuition is wrong. There are some privileged transformations, like Lorentz transformations, that objectively does better job at describing the world. Therefore, the world teaches us that, our seemingly natural intuition is in fact incorrect.

The parallel example used to support an "objectively objective naturalness" is Goodman's riddle of induction. The basic idea is that, a concept like "grue" must be flawed because it leads to a wrong induction. Therefore the world teaches us that some concepts are objectively better than others. This is perhaps the strongest case for an objective standard of naturalness, but I believe it ultimately fails.

To recap the riddle: Since all emeralds observed before 2050 are green, these emeralds are also by definition "grue" (where grue means green and observed before 2050, or blue otherwise). If we use grue for induction, the grue-induction would tell us that the next emerald we see after 2050 will be grue. However, we intuitively know the next emerald will be green, meaning it will not be grue. Since the green-induction and the grue-induction are formally the same, the problem does not lie in the structure of induction. Then the concept "grue" itself must be the problem.

But I think the world cannot teach grue-speakers that grue-induction is problematic. And therefore, for grue-speakers, there is nothing wrong with grue. Let's consider two cases.

**Case 1**: In the first case, *we* assume that "being green" is an essential property of the kind "emerald." But why would *grue-speakers* accept this? By the same token, they might take "being grue" as the essence of an "emerald." If we showed them a green emerald in 2051, they would simply say, "*This isn't an emerald, because it's not grue.*" In that case, their induction is perfectly reliable: everything they call an "emerald" is, in fact, grue. Of course, this scenario is overly simple because it assumes the kind "emerald" is defined solely by its color. If so, the disagreement becomes a matter of a priori definition, not a failure of induction.

**Case 2**: So, let's consider a more realistic and relevant case. Assume both we and the grue-speakers agree that emeralds are defined by their chemical structure ZYX, not their color. Through induction, our science posit a law of nature that "*ZYX necessitates greenness.*" In grue-science, however, induction would posit a law that "*ZYX necessitates grue-ness.*" Assume that when 2050 arrives, the next emerald turns out green. Therefore, grue-law will be falsified. Then it follows that, the laws of nature is simpler when formulated in terms of green. Thus, the world teaches us that, green is objectively better than grue. 

But this argument doesn't seem right for two reasons. First, Inductive failure is normal. A predicate can be natural, but simply fail to apply to the induction. For example, we might inductively conclude that all swans are white. The discovery of a black swan doesn't make the property "white" is objectively worse than "white or black". It just means we hit on a false hypothesis.

Second, the whole argument relies on the presupposition that in grue-science the grue-speakers start with the hypothesis *ZYX necessitates grue-ness.* But given they take the property "green if observed before 2050, or blue otherwise" to be simpler than "green", why wouldn't they instead adopt the hypothesis "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*" as simpler than "*ZYX necessitates grue-ness*"?

To prevent further complications, assume that ZYX is invariant in both our law and the grue-law. Let us even grant that the grue-speakers initially start with the hypothesis *ZYX necessitates grue-ness*, and that this hypothesis is falsified by the first emerald after 2050. After further work, grue scientists discover that the corrected grue-law, like "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*", is the right one. Now, the previous argument runs, our law "*ZYX necessitates green*" is objectively simpler than this corrected grue-law. Therefore, green appears to exhibit objective similarity. This is because the properties in the laws of nature 'carve nature at its joints'.

But why would the grue-speakers accept this argument? My stipulation is that grue-speakers have different intuitions about simplicity and similarity from ours. They take properties, that are disjunctive in our terms, to be genuinely simple similar; likewise they treat disjunctive propositions as simpler. Given this stipulation, they would say their corrected law is indeed simpler than ours, and thus grue and bleen are more privileged than green.

In other words, if the meaning of "the best system" could vary, then the best system account of laws of nature cannot be objectively established. If natural properties are specified by the laws of nature, while the laws of nature can vary in different frameworks, then "naturalness" can vary in different frameworks. Therefore, even if the grue-speakers agree with us that, natural properties are metaphysically privileged, the world cannot force the grue-speakers having a different extension of "natural properties".

Surely for us, there is no problem, in accordance with Bayesian principles, to adopt a higher credence for green-laws based on prior distribution. This may be enough to say we are justified to believe green-law, and this is enough for our confirmation. But there is no theoretical reason to take greenness as the metaphysically privileged concept across different frameworks. Nothing can stop "naturalness" having semantic variance.

### Essence

Maybe "naturalness" is already too niche. Indeed, the most orthodoxical neo-Aristotelian way to establish fundamentality is through concepts like grounding or essence. For example, the most fundamental layer is ungrounded (i.e. independent), and everything is either the most fundamental, or is grounded by the most fundamentals (i.e. compleness). In my terminology, $P$ "really" explains $Q$ iff $P$ is grounded by $Q$. In other words, the *sometimes-ism* about explanation is explained by grounding 

The concept of grounding, however, notoriously lacks consensus. Before getting further into this, I wish to go into a slightly earlier concept, i.e. essence.

The approach to establishing fundamentality through essence is largely due to Kit Fine's series of works (=="Essence and Modality," "Senses of Essence," "Ontological Dependence," and "The Logic of Essence"==). Of course, Fine prefers to treat ontological dependence as primitive. But Fine acknowledges that this is merely a technical choice, and we can instead define ontological dependence in terms of essence: $x$ depends on $y$ when there is some property $\phi$ (that does not mention $y$) such that in virtue of $x$'s nature it's true that $y$ has $\phi$, but in virtue of $x$'s nature it's not true that everything has $\phi$. Formally, according to Fine we can choose to define $x \ge y =_{df} \Box_x(y=y)$ along with other axioms in order to capture the informal definition. 

Here, I do not want to focus on the details of Fine's axiomatic system. And I acknowledge that, essence is indeed an intuitive concept upon which *de re* modality should be built, not the other way around. Instead, my question is, why should we take essence to be a feature of reality, not a feature of language? To show this, we need to show that, if there is an alternative language that attribute essence differently, then that attribution is *objectively* wrong.

Imagine a set-theoretic creature, maybe a robot called Robo, who perceives every object as a singleton set. For example, Robo directly perceives a table as $\{table\}$ and Socrates as $\{Socrates\}$. Only by theoretical reconstruction can Robo realize that what she perceives is necessarily related to the individual Socrates. For Robo, then, the individual we call Socrates is a theoretical reconstruction from the entity she directly perceives. Therefore, from her perspective, Socrates depends on $\{Socrates\}$.[^10]

Is there anything objectively wrong with this creature Robo? Of course, there is a sense in which Robo is wrong because her attribution of essence violates our intuition. But this reason is *symmetric*: Robo can by the same token accuse us of being counterintuitive. Yet, if Robo's attribution of essence is *objectively* flawed, the reason cannot be symmetric. So, appealing to our own intuition does not count as a good reason to say Robo's attribution of essence is flawed.

I think the only hope to establish the invariance of essence attribution is to let the world teach us whether the concept we are using is wrong. For example, the wrong attribution of essence will make some induction fail, etc. But I can't see any way to argue for this invariance.

### Grounding(Building)

Now let's get into grounding. Here what I mean by "grounding" here is Wilson's talk of "small-'g' grounding relations" (==2014==), or what Bennett calls "building". In fact, let me switch to the terminology of "building" in the rest of the discussion, in order not to conflate with the specific grounding that is usually thought of as a relation between facts. I use building here because fundamentality may not just be established by the specific version of grounding.

Building relation is in fact a set of relations that are directed, necessitating and generative (==Bennett citation==). Some examples are composition, constitution, set-formation, (property) realization, grounding and causation. To say that $x$ builds $y$ is to say $x$ bears one of these building relations with $y$. According to Bennett, fundamentality can be elegantly established by building relationships. In a rough way to put it, the most fundamental layer is unbuilt, and if $A$ builds $B$, then $A$ is more fundamental than $B$.

Immediately, this account of fundamentality is already *relative*, because there are different building relations. For example, according Shaffer's priority monism, the entire cosmos $builds_{grounds}$ its parts, but its parts $build_{compose}$ the cosmos. According to this view, there is no "objectively objective" fact of something being absoutely fundamental; Rather, something can only be fundamental *relative to a building relation*.

Even if so, we may still say, *the set of relations* that count as building relation is absolute and objectively objective. If so, then fundamentality, though being relative to a specific building relation, is still invariant across different language. The problem, then again, is whether this can be established.

In the discussion of essence, I have talked about the directness given by set-formation is problematic: the members of a set forms a set does not follow that the members are more fundamental than the set. For Robo, this direction might be reversed: instead of set-formation being a building relation, they take set-membership as a building relation. 

Recall Bennett's account about building: a building relation has to be *directed*, *necessitating* and *generative*. The problem here is generative requirement. It is this requirement that Robo and us would have different set of building relations. According to Bennett, the generative requirement is that,

> (G) For all building relations B, and all x and y, x's B-ing y makes true certain explanatory and generative claims. For example, if a builds b, then b exists (obtains, is instantiated . . . ) because a does, b exists (obtains, is instantiated) in virtue of a, a makes b exist (obtain, be instantiated), and so forth.

Surely, Robo agree with us that $\text{Socrates} \in \{Socrates\}$. But what they disagree with us that set-formation is not generative. They think set-membership is generative.

The problem can be made more general. If $Rxy$ makes $\square (x \leftrightarrow y)$ true, then let's call the relation $R$ *mutually necessitating*.[^11] Given any directed and mutually necessitating relation, we can always find a reverse relation that is also directed and necessitating. And reversal is not the only formal maneuver we can do. Say a binary asymmetric relation $R\subseteq D\times D$ is mutually necessitating. Then we can define inverse $R^{-1}=\{(x,y)\in D\times D \mid R(y,x)\}$. Given that $R^{-1}(x,y) \leftrightarrow R(y,x)$, it is easy to see $R^{-1}$ is also asymmetric and necessitating. Moreover, let $x\equiv_{\Box}y$ abbreviate $\Box(x\leftrightarrow y)$. $R$ lies inside the union of $\equiv_{\Box}$-classes, i.e. $R\subseteq\bigcup_{C\in D/\!\equiv_{\Box}}(C\times C)$. Hence within each $\equiv_{\Box}$-class $C$ of size $n$ one may independently pick for every unordered pair $\{u,v\}\subset C$ either $u\to v$, $v\to u$, or neither, producing $3^{\binom{n}{2}}$ asymmetric, necessitating relations on $C$. Whenever some $\equiv_{\Box}$-class has $n\ge 2$ there are therefore combinatorially many relations $R^{*}$ that (i) predicate over the same domain, (ii) are necessitating and directed, yet (iii) are extensionally different from a given directed, necessitating $R$.

Therefore, if there is any mutually necessitating relation $R$ in the set of building relations, and we want to rule out $R^{*}$, then the problem is how. Bennett suggest the generative constraint, but this constraint cannot establish the invariance of the set of building relations, because for Robo, $\{Socrates\}$ exists explains Socrates exists. But we cannot say this is not the "real" explanation because Socrates is more fundamental than $\{Socrates\}$, for this just begs the question. And I don't see other reason to say there is any problem with Robo's understanding of fundamentality.

Indeed, I believe this is the most problematic part of Bennett's account of building relation. Even Bennett concede that what counts generative may be arbitrary, conventional, framework-dependent and "nothing further to be said" (==p. 59==) If this is the case, then how can we save the idea that the set of building relations is invariant and "objectively objective"? I can only see the following two ways out.

The first way is to simply not rule out the alternative relations $R^{*}$. This suggestion, however, makes the generative constraint redundant. It would mean that for any group of mutually necessitating entities, we could order them however we want and claim that our arbitrary ordering reveals a sense of fundamentality. But if a collection of things can be ordered in any arbitrary way, it seems the best way to describe the situation is to say that the things themselves have no intrinsic order. They just merely necessitate each other. Therefore, even if this solution formally works, this move will depart from the intended meaning of fundamentality.

A second way out is to deny that there can be mutually necessitating building relations altogether. If so, then relations like set-formation, the successor relation, and logical connectives would be removed from the list of building relations. This would essentially mean that applying "fundamentality" to abstract objects is a category mistake. However, some other relations, like composition, would be preserved. For example, if we do not adopt mereological essentialism, then material composition is not mutually necessitating, because the parts necessitate the whole, but the whole does not necessitate those specific parts.

But this move still does not make the set of building relations invariant. For example, the meaning of "composition" can vary. Mereological nihilists, organicists, common sense metaphysicians, and four-dimensionalists all have different interpretations of "composition". If we allow any of these interpretations to count as a building relation, then what is considered fundamental becomes arbitrary. This, again, seems to depart from the intended meaning of fundamentality.

In addition, it is not only the generative constraint that may lead to semantic variance. The necessitating constraint may also lead to variance, given that there are different senses of necessitation. This has been discussed by Sidelle (==The Grounding Mystique==): Given that modality is at least partly conventional, the deflationary challenge for metaphysical necessity extends equally to grounding.
## Upshot

The gist of the tedious discussion above is simple. Imagine an alien whose language contains "fundamentality" that plays the same role in their metaphysics as our "fundamentality" does in our metaphysics. However, their "fundamentality" has a different extension. If we does not rely on the Quinean epistemology, and claim that fundamentality is a *worldly, objective feature*, then we must be able to show why their concept is *objectively* wrong. Crucially, the reason we provide must not be symmetric; it can't be a reason the aliens could also use against us (e.g., "it violates *our* intuitions"). As the previous sections have shown, it seems we cannot find such a non-question-begging reason in the current way of establishing fundamentality.

It seems the only hope for finding this kind of non-symmetric reason would be the following: the world itself could teach us which "fundamentality" is the correct use. But challenges from strange creatures essentially press the following question: if we were to throw away some of our intuitions, would the world still be able to teach us anything? The upshot of the discussion is that it would not. In a slogan: ***If we are willing to throw away our intuitions, the world cannot teach us anything.***

I believe this slogan is true. However, in the next part of this paper, I show that some of our intuitions cannot be thrown away, and it would lead to substantial consequences.

# II

The arguments discussed in the previous section share a common pattern: we imagine a strange creature who thinks and speaks a radically different language. The challenge is that we seem unable to prove their language is metaphysically worse than ours.

A natural, initial response is to dismiss these creatures as simply _insane_. This is, in fact, the kind of response I want to defend. My strategy, however, isn't to argue that there is something wrong with these alien languages _per se_. Instead, I want to focus on what's wrong with the relationship between _us_ and these languages.

To put it simply, *certain constraints prevents us from coherently adopting and committing to just any theory*. If these constraints have substantial metaphysical consequences, then any theory we can coherently adopt must be consistent with them. Moreover, these metaphysical consequences will be invariant across all theories available to us.

Before getting into my account, one immediate problem arises. The criticism is that what I am saying is mere pragmatism, and that deep metaphysical truth cannot be revealed by pragmatism.

There is some force to this criticism, and this force is related to footnote 6. But here consider one uncontroversial constraint: inconsistency. Why do we think an inconsistent theory is wrong? Indeed, we might even worry that the world itself is inconsistent. Yet we dismiss inconsistent theories. The reason is simple: we are incapable of following an inconsistent theory.[^12]

In other words, if the aim of metaphysics is to understand reality, we cannot coherently understand inconsistent theories. If the world were truly inconsistent, that kind of metaphysical knowledge, in Socratic terms, would belong not to human beings but to God. For human beings, then, we should pursue human knowledge and understand reality in a human way, not in God's way. Therefore, if metaphysics is to be a subject of human inquiry at all, it must accept our human limitations, and thus accept these constraints on theory choice.

## The principle of Sanity

The constraint I suggest is the following:

- **The principle of sanity**: For someone who is capable of philosophical reflection, it is impossible to genuinely believe oneself to not be a rational agent.

**Clarification for this principle**: This principle also applies to acceptance (i.e. It is impossible for someone to accept oneself to not be a rational agent), but I will focus on belief in the rest of the paper unless specified otherwise. Anyway, the upshot is, it is impossible for us to really adopt a theory that eliminates rational agency.

Here, "philosophical reflection" roughly refers to activities such as examining and revising one's beliefs, analyzing arguments, and choosing between competing theories. The condition "for a person capable of philosophical reflection" is to rule out certain exotic counterexamples. For instance, someone who has been so thoroughly brainwashed that their only possible belief is "I am not a rational agent" would not qualify as a counterexample. I assume all readers who have already made it here are automatically capable of philsosophical reflection. Similarly, saying "genuinely believe" is to rule out counterexamples from self-deception.

What I mean by "rational" here is a minimal sense of means-end coherence. For example, someone who wills their hand to rise but finds their leg going up instead would lack this coherence. Thus, even if this could somehow be a case of agency, it wouldn't be rational agency in the relevant sense.

The precise meaning of "rational agency", of course, is highly controversial and open to multiple acceptable interpretations. Consequently, this principle will have different variants depending on how one chooses to define the term. However, I contend that the *principle of sanity* is plausible under most, if not all, commonsensical interpretations.

For current purpose, it will suffice by adopting a minimal account of rational agency as *epistemic agency*. But I will add more to "rational agency" later in this paper. What I mean by "epistemic agency" here is not the kind of agency resisted by doxastic involuntarists. According to doxastic involuntarists, *direct* voluntary control over belief is not possible for us. But here, I use "epistemic agency" to refer to any sort of control over our beliefs, no matter how indirect. For example, we have the ability to choose to re-examine our beliefs and investigate new evidence. Doxastic involuntarists, like ==Hieronymi 2008==, also acknowledge that we possess this kind of indirect control. Therefore, it is uncontroversial that we have epistemic agency in the intended sense.

**Reasons for this principle**: This is more or less an axiom in this paper that I take to be self-evident. But I will still try to give some defense. 

First, implicitly, this is already a widely accepted principle. Aristotle appears to adopt this principle in Chapter 9 of _De Interpretatione_. There, he rejects the principle of bivalence due to our deliberation and action. He argues that, deliberation and action requires that, propositions about the future must be open (19a10). Then they cannot have a truth value now (19a26). Therefore the principle of bivalence is false. However, Aristotle's argument seems to epistemic instead of metaphysical. When he writes that "what will be has an origin both in deliberation and in action" (19a6-9), the plausible interpretation is that our *beliefs* about the future originate from these rational activities, not the future state of affairs itself. If so, his argument is best understood as follows: it is impossible for us to genuinely *believe* that the future is predetermined while simultaneously viewing ourselves as rational agents who deliberate. Therefore, by *the principle of sanity*, we cannot adopt the principle of bivalence.

There are also more contemporary cases.People seems to reject epiphenomenalism largely due to the reason that, if there's no mental causation, then it seems we wouldn't have any agency, which is insane. And I will argue later that, the motivation of a widely held metasemantics principle, *the Principle of Charity*, actually comes from of a stronger version of *the Principle of Sanity*. Therefore, in fact people have been implicitly using this principle.

Nonetheless, I will also try to sketch a direct defense. If the principle were false, then it would be possible for a creature to engage in philosophical reflection without believing itself to have rational agency. But philosophical reflection inherently requires making inferences and utilizing abstract concepts. Let's assume, as is widely held, that such complex thought requires language (or at least some language-like mentalese). Then, I argue in the following paragraph that using language for this purpose necessitates rational agency. Therefore this creature is impossible.

To use language requires intentionality. For example, a dinosaur that accidentally scratches the shape of the symbol "water" into the ground is not using language, because it does not use the symbol intentionally to represent something. Therefore, intentionality is necessary for language use. But to use representation intentionally for philosophical reflection just is to exercise the very sort of indirect, agential control over one's beliefs by potentially revising what one holds to be true. And we must be able, at least to some extent (including subconsciously), to be aware of our own intentions. If we are aware of our intentions to some extent, we believe that we have such intentions. Thus, if we use language to engage in philosophical reflection, we must believe in epistemic agency.

Maybe this is not an invincible argument. Perhaps with some heavy argument, one may claim we can build a "philosophy machine" that can reflect on arguments and produce a philosophy paper while being completely non-agential. Even if so, I believe it is quite obvious that rejecting the principle of sanity, plainly, is to be insane. Even if such a "philosophy machine" is possible, I still think no human being should reject this principle, simply because we should not be insane.[^13]

There are obvious consequence for adopting this principle. Together with the following innocent principle that:

- **Ought Implies Can**: If it is impossible for us to believe that P, then it is not the case that we ought to believe that P.

It follows that *the Principle of Sanity* is incompatible with the traditional account of rationality: A belief is rational iff the reason to believe is truth-tracking. This is because the principle of sanity commits us to a belief we must hold (that we are rational agents), regardless of the evidence. Assume our epistemic norm is to hold only true beliefs, and scientists tell us that rational agency is just an illusion, then we would be obligated to believe that we are not rational agents. But according to the principle that *Ought Implies Can*, there is no such obligation.

Then immediately, one might object that if epistemic norms must be truth-tracking, then we should rationally reject *the Principle of Sanity* itself. However, what does it mean to "rationally reject" a principle? If doing so is an act of philosophical reflection, then, as argued earlier, the very act requires us to presuppose our own epistemic agency. Therefore, any attempt to "rationally reject" *the Principle of Sanity* is self-defeating, as the act of rejection presupposes the very rational agency the principle commands us to accept.

There is also an auxiliary argument that help me establish the metaphysical argument that I want to establish. In the first part of this paper, I put aside the neo-Quinean epistemology for establishing fundamentality. According to this view, the best theory of fundamentality that has the most pragmatic virtues like simplicity, elegance and explanatory power. But what how could these pragmatic virtues more valuable than our rational agency?

## Metaphysical Consequences

Given this Principle of Sanity, there are some 
Direct consequence of the Sanity Principle: Cannot eliminate first person and causation

Indirect consequence of the Sanity Principle: establishing fundamentality without magic

## Beyond metaphysics: trusting friends against evidence

Conclusion



[^1]: Needless to say, in a complete model for English language, many other vocabularies need different interpretations.
[^2]: I'm focusing on quantifier variance because I believe it's the most plausible and basic form of the neo-Carnapian position. ==Thomasson (2015 p. 70)== herself thinks her easy ontology doesn't depend on quantifier variance, but this is surely mistaken. Her project relies on the triviality of changing the extension of the predicate "exists". But this surely changes the meaning of the quantifier.
[^3]: Surely this paper is not intended to become the zoology of metametaphysics, so these labels may not be precise. What I mean by neo-Aristotelians is basically the fans of fundamentality. For example, Sider counts as a neo-Aristotelian in my sense. Though Sider is usually considered as a neo-Lewisian, which is closer to neo-Quinean. But the label is not important anyway.
[^4]: A less plausible answer is that they cannot make sense of my question. My reply is, I cannot make sense of this answer.
[^5]: This "under the guise of" is motivated by the distinction between propositions and facts. (What I mean by 'proposition' here is the objects of our beliefs, and what I mean by 'fact' is the thing in the world that makes a sentence true.) For example, "Superman can fly" and "Clark Kent can fly" are the same fact, but they are different propositions: one might believe the former but not the latter. One way to explain this is to say that someone believes of $x$ *under the guise* of 'Superman', that $x$ can fly, while not believing $x$ under the guise of 'Clark Kent' that $x$ can fly. <br> By the same token, neo-Carnapians suggest we can see different linguistic frameworks as different "guises" for understanding the world. If there are equivalent languages can serve as equally good guises, there are many different ways to structure our knowledge of reality. Each of these guises constitutes genuine, true knowledge. This move allows neo-Carnapians to embrace a form of lightweight realism, distancing themselves from an anti-realist never-ism. 
[^6]: However, this approach immediately faces a challenge. If I treat other people's languages as mere guises of reality, why shouldn't I treat my own language as a mere guise as well? Moreover, if my language is also a guise, then it seems we would collapse back into the anti-realist's never-ism, because it presupposes a reality that is unspeakable without guise.<br> In fact, the answer to the challenge is worth a paper-length discussion, so I cannot fully address it here in a footnote. To sketch the answer, consider the following case. First, let's establish that the set of all well-formed English sentences, taken as finite strings, is countable. We can have a bijective function $f$ that maps every unique English sentence to a unique natural number. From this, we can construct a One-Word-Sentence language (OWS-language for short). In this language, each natural number (and thus each corresponding English sentence) is represented by a single, unique symbol (e.g., '∇', '§', 'Д'). Crucially, these symbols carry no internal syntactic information that mirrors the structure of the original English sentences. Now, imagine a creature that genuinely thinks in this OWS-language, not one that is secretly translating from English. Suppose that when we apply the inverse function, $f^{−1}$, to decode their one-word utterances, we find they agree with us on the truth value of every proposition. They even affirm the OWS-symbol that decodes to "There are objects and properties." Here is the problem. This creature's way of thinking is inconceivable to us. Given we stipulated that they think in OWS, it seems they cannot grasp the concepts of objects and properties. Yet, their statements, once translated, shows they think there are objects and properties. It is in cases like this that we must appeal to the "guised reality" view. We are forced to say that they believe there are objects and properties under the guise of their OWS-language, precisely because there is no way to understand their take on reality. Or in other words, we are *unable* to read reality off this OWS language. In contrast, our own understanding is transparent. We do not take of our language as a "guise" because it is normally a transparent *presentation of the reality*, not representation. We only appeal to "guise" in specific cases of misalignment, such as with co-referring proper names like 'Superman' and 'Clark Kent.'
[^7]: This is probably not the orthodoxical formulation. The orthodoxical formulation is, we distinguish absolute fundamentality and relative fundamentality. For absolutely fundamentality, we usually appeal to concepts like completeness, dependency, indispensability or naturalness. But when asked which set of things has this feature, we appeal to concepts, like grounding. And the standard story is to treat these relevant concepts as primitive. But then we need an epistemology for this primitive grounding. Ultimately the conclusion of "what grounds what" appeals to virtues like power of explanation or definitional simplicity. Therefore, ultimately we decide what's absolutely fundamental based on explanation or definition. I have not seen how could this be done otherwise. This also applies to relative fundamentality.
[^8]: In fact Sider tend to drop the term "naturalness" to favor the term "joint-carving" or "structural" for the extended meaning of "naturalness". But let me stick to "naturalness" here.
[^9]: One possible approach is to define the essential meaning of a term by its introduction and elimination rules (IE-rules), that is, by its functional role in language. This may be intuitive for logical constants. For a term to count as a quantifier, for example, it must obey the IE-rules for quantifiers. However, the full meaning of a quantifier also depends on its domain, and the IE-rules don't fix this. Therefore, the meaning of a quantifier can still vary as its domain varies. This approach faces several problems. First, this form of essentialism about meaning seems arbitrary. Why should IE-rules be the only thing that constitutes a term's essential meaning? For example, we might say in some sense 'quus' is a variation of 'plus', not a completely unrelated concept like 'Hello'. But they have different IE-rules. Second, for non-logical terms, the IE-rules themselves are usually formulated using terms that can vary. Third, and most importantly, even if we accept this IE-rules essentialism, it would not help fix "naturalness". It leads to a specific conclusion: a term's meaning is invariant if its meaning is exhausted by its IE-rules. For example, a logical conventionalist might say $\land$'s meaning entirely determined its IE-rules. Then according to IE-rules essentialism, $\land$ is invariant. However, Sider clearly intends "naturalness" to be more than just a conventional, syntactic symbol. Therefore, this strategy for securing semantic invariance is not a viable option for his project anyway.
[^10]: If this example is puzzling to you, you can try the following notation: she uses "Socrates" to refer to $\{Socrates\}$, and "-Socrates-" to refer to Socrates. Mathematically speaking, set formation has nothing to do with intrinsicality. Our intuition that *a member of a set* is somehow intrinsic and thus more fundamental than *the set itself* is just our *interpretation* of set formation, influenced by ordinary language words like "member" and how we use brackets to warp the members. But we could have used alternative notations, like using "M-relation" instead of membership, and use $\{\}-Socrates$ intead of $\{Socrates\}$.
[^11]: Bennett's formulation of the necessitating requirement is a bit more complicated than this. She also consider the circumstance of necessitation (==p.52-54==). But here I will just use the cleaner way to formulating it for the toy example.
[^12]: This is an empirical claim about our limits. I don't deny the possibility that a few intellectually superior beings, perhaps through mutation, or perhaps like Hegel, could handle inconsistent theories.
[^13]: In other words, I am in fact suggesting two different principles. The stronger version is what I explicitly laid down and inclined to stand with. But even if one somehow resisted it, my conclusion can still be established by the weaker version, i.e. if we are capable of philosophical reflection, then we should not believe we are not rational agents.
