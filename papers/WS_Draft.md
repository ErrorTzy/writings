---
documentclass: exam
title: Draft
author: Scott Tang
mainfont: FreeSerif
CJKmainfont: Noto Serif CJK SC
mathfont: XITS Math
fontsize: 12pt
---
# I

## Intro 

This paper is probably not a typical metaphysics paper. Although the main upshot is on metaphysics, the kind of move I make applies beyond metaphysics. 

In the background, this paper deals with a central tension. On one hand, it seems that, ultimately all possible philosophical conclusions must be derived from intuitive presuppositions. If so, then we cannot establish anything if we throw away all of our intuitions (or dispositions, sentiments, etc.). On the other hand, many of our intuitions are unreliable, and even contradict one another. The problem, then, is to figure out which part of our intuition cannot (or at least, should not) be thrown away and can safely rely on.

In the foreground, this paper focuses mainly on metaphysics, but the argument I made for metaphysics also extends to other areas, like ethics. To be specific, I first argue that the background tension is somehow ignored in discussions about fundamentality. As a result, the idea that facts about fundamentality are "objectively objective", though well-motivated, seems to rely on magic or wishful thinking. 

Then I argue for a principle that basically says, we cannot throw away our rational agency. The rest of the paper explores the consequences of this principle. For example, it can be used to establish a direct, non-magical account of fundamentality. But given that the background tension is topic-neutral, this principle can achieve more. For instance, it also solves a problem in ethics: why we should sometimes trust a friend's word, even when it goes against the evidence.

## The Question of Reality

The starting point of this paper comes from the following question: *How much reality should we read off our linguistic features?*

### clarification

First, it seems to suggest linguistic idealism. It doesn't. Reality certainly is not metaphysically constructed upon our language in any way.

Without language, we might still have ways to understand reality, perhaps through perception or mysterious experiences. But, this is not how metaphysics works. In metaphysics, we inevitably rely on sophisticated argument that relies on intuitive premises. But this has generated a long-standing worry in philosophy. Since medieval times, philosophers (e.g. nominalists) have been worrying that our intuition about the world comes from *words* instead of reality. Hence my question: when we try to talk about reality, which parts genuinely reflect how things are, and which parts are mere "shadows of language"?

This question appears central to almost all metaphysical problems. For example, 

- Almost all natural languages have a subject-predicate structure. Should we then say, the world itself is constituted of objects and properties? 
- Grammatically we have predicates like 'good' or 'bad'. Then should we read them off our language and say there are such properties in the world? 
- The proposition $P$ may seem syntactically more primitive than $\neg \neg P$. Should we read this structure off our language and say the *fact* that $P$ is different, and in some sense more fundamental than the *fact* that $\neg \neg P$? 
- Quantum physics describes the world in terms of wave functions. Should we then interpret that reality as just a bunch of waves (or maybe, a huge, holistic wave)?

It is obvious that not every linguistic feature reveals reality. Take the proper names "Superman" and "Clark Kent." We don't think there are really two distinct entities out there connected by a relation named "identity". This is a non-trivial claim. Formally, one could construct a toy model $M = \langle D, I \rangle$ with $D = \{s, c\}$, $I(\text{'Superman'}) = s$, $I(\text{'Clark Kent'}) = c$ and $I(\text{'is'})= \{ \langle s,c \rangle, \langle c,s \rangle \}$. Such model would satisfy 'Superman is Clark Kent'.[^1] But intuitively we do not take such a model to *capture reality* because we think 'Superman' and 'Clark Kent' should refer to the same entity in the domain. 

This question can be taken as a better way to formulate realism. The traditional approach formulates realism in terms of *truth*. For example, scientific realism is often described as the view that, scientiﬁc theories are true(==citation from BVF==). And anti-realists like van Fraassen argue that they aren't really true, but merely _empirically adequate_.

However, this way of framing realism relies on a problematic conception of truth. The problem is that *truth is cheap*. Consider a game of chess: no one would think we are saying something false when we state, "There is a bishop on the board." Yet, the truth of that statement is partly established by the rules of chess, not by the world itself. This shows that truth can arise from linguistic rules, not just from reality. 

This way of framing realism creates a loophole for deflationists. It allows them to call themselves "lightweight realists" simply by agreeing that our metaphysical claims are true, while maintaining that this truth is superficial. But we do have a strong intuition to say, there is something deeper than that. For example, the toy model $M$ about Superman and Clark Kent constructed earlier would make our utterance about their identity come out *true*. But we do feel there is something wrong about it.

If we frame the question of realism in my fashion, however, then the deflationists must say more. For example, according to easy ontologists, adopting a language is easy, and thus ontology is easy. (==citation from Amie Thomasson==) But ontology can't be that easy. Even if "there is $x$" can be made trivially true, it is not trivial to freely read $x$ off the sentence and take $x$ to be really *out there*. And Sider's project about Ontologese is best understood as, we should only read the (most fundamental) reality off the perfectly natural language, not unnatural languages that include kinds like "incars" and "outcars".

Another important distinction needs to be clarified. There is a difference between merely *utilizing* a language and *adopting* a language. To start with, what I mean by language is broad, and includes inferential rules and laws. In other words, that "theory" and "language" are interchangeable in this paper. 

In fact, a full account of this distinction has to be discussed at length elsewhere. But the basic idea is simple. To *utilize* a language is to merely follow syntactic rules and form well-formed sentences. One does not need to take a stance on how to interpret the language. For example, mathematical formalists, like Hilbert, are merely utilizing mathematical languages without reading anything off it. To *adopt* a language, however, one has to also *think* in its semantics and accept the picture of reality it implies. A calculator surely doesn't *adopt* arithmetic. In addition, we do not always adopt *literal* English because we don't really think "bank" is a unified category.

The way I've framed the question, i.e. how much reality we should read off our linguistic features, is different from the standard story told in metametaphysics. I think the standard narrative, which I'll lay out in a moment, tends to miss something important. I take the familiar story told by ==Schaffer (2009 on what grounds what)==, which is the most-cited paper in this field, as the orthodox version (though I'll be telling a bit differently).

## The standard story

It goes something like this: contemporary metaphysics began with the Quine-Carnap debate. After Quine arguably won the debate, his approach to metaphysics prevailed. For Quine, the main task of metaphysics is to figure out what indispensably *exists*. Whether numbers or electrons exist depends on whether we must quantify over them in our best theories. This methodology was extended to other topics. Lewis, in some sense, can be seen as following the Quinean methodology: Since our best account of *modality* requires quantifying over possible worlds, possible worlds are real. From there, other concepts can be analyzed. *Causation* is explained by counterfactuals, which are analyzed in terms of real possible worlds. Modality *de re* can be analyzed by real possible worlds, plus some pragmatic, context-sensitive similarities. The takeaway is that most problems in metaphysics can be solved by first figuring out what exists and then maybe add a dash of pragmatics.

But then, the neo-Carnapians entered the scene. I believe the most important the idea from the neo-Carnapians is that, the meaning of "exists" can vary.[^2] According to this view, when different people debate about what exists, they are speaking different languages. If their languages are truth-functionally equivalent, then this is a mere verbal dispute.

Imagine one side speaks A-language and the other speaks B-language. For any sentence uttered by A-speakers, B-speakers will have their respective interpretation the sentence, vice versa. If the interpretation from each side always agrees on the truth value, then there's no disagreement other than disagreement on language: the other side is just expressing the fact differently. If that's the case, then many debates about what exists are merely verbal.

The implication here is serious. If metaphysics is founded on questions of existence, but quantifier variance shows that there are many equally good meanings of "existence" and the debates are merely verbal, then metaphysics risks becoming a largely trivial field of study.

Meanwhile the Neo-Aristotelians also come into play.[^3] They also attack on the neo-Quineans. They argued that even if we had a complete list of what exists, we'd still be missing something important: what is most fundamental, and what grounds what. Schaffer, for instance, argues that questions about existence are trivial. He says that of fictional characters and abstract objects trivially exist. The real, interesting question is about what grounds their existence. This shift in focus from existence to fundamentality has been the dominant trend in metaphysics for the past twenty years or so.

### The problem with the standard story

Here's the problem. From the neo-Aristotelian perspective, this makes them look like the next logical step. In this view, the neo-Carnapians performed a useful, but purely *negative*, task by showing the flaws in the neo-Quinean focus on existence: the meaning of "exist" could vary. With that groundwork laid, the neo-Aristotelians can then step in to offer a *positive* new project, and say the various meaning of "exist" are "exist-more-fundamentally" and "exist-less-fundamentally". Thus, they turning the page on the neo-Quine-Carnap debate and building something new around fundamentality. (==citation==)

As a result of this framing, the standard story tends to sideline the neo-Carnapians. This isn't to say they're completely ignored, of course; people like Sider, Schaffer, and Bennett certainly take them seriously. Rather, the problem is that the standard story implies the neo-Carnapian critique is *only* about existence. And if that's the case, then the neo-Aristotelians, with their new focus on fundamentality, can safely disregard it. 

But this is surely *not* the case. The neo-Carnapian argument, on a high level, is about *semantic variance*. Therefore, it applies to any theory as long as the theory allows semantic variance. 

In order to see this more clearly, let's return to the question I framed at the start: *How much reality should we read off our linguistic features?*

**For the neo-Quineans**, the answer is *sometimes-ism about existence*. We can only sometimes read entities off our ordinary language. We need to paraphrase our ordinary language and scientific theories. While the exact rules for this paraphrasing are messy, but the basic idea is to create a language that maximizes pragmatic virtues like simplicity, elegance, and explanatory power. It is from this idealized, paraphrased language that we are meant to read real entities off the values being quantified over.

**For the neo-Carnapians**, the most plausible answer is *a general form of always-ism*: we can always freely read reality off any languages we think in, as long as they are truth-functionally equivalent.[^4] When another party, say John, uses a different yet equivalent language, there's nothing wrong with John, nor is there anything wrong with us. First, we can interpret both sides as speaking the truth, so there's no disagreement about facts. Second, from our point of view, there's nothing wrong with "John's way of thinking" because his resulting beliefs are also true. Here's why: if a sentence $S$ is true in John's language​, then it is true for John to believe that $S$ *under the guise of* John's language.[^5] By the same token, from John's point of view, there's nothing wrong with us either, whether in terms of truth or in terms of beliefs. I leave some complications of this account in a footnote.[^6]

**For the neo-Aristotelians**, it seems they are holding *sometimes-ism about explanation or definition*, and *always-ism about existence*.[^7] Everything we can talk about trivially exists, but it might be derivative or idea-dependent. And the way we trace this kind of dependency is through the structure of definition or explanation (==citation to Fine's essence and modality, guide to ground, and Schaffer's work==).

The essential feature of definitions (and explanations) is that they are inherently *asymmetric*: nothing can define itself, and if $A$ is defined by $B$, then $B$ cannot be defined by $A$. However, this only applies to "real definitions," not "nominal definitions." For example, we may nominally define "A is on the left side of B" by "B is on the right side of A," and define "B is on the right side of A" by "A is on the left side of B." In fact, this kind of circularity probably appears occasionally when terms are defined in the dictionary.

Given this distinction, we cannot say that any definition in our language trace fundamentality. Only the "real" ones trace fundamentality. The same principle applies to explanation. Although grounding relations are explanatory, they are not explanation relations per se. Explanation is (in)famously context-sensitive (==citation to van Fraassen, Kitcher==). It depends on things such as what we're emphasizing, and what interests our audience. Grounding relations, by contrast, are intended to be worldly, "objectively objective", and context-independent. Since they are separate, only "real" explanation can trace fundamentality. This leads neo-Aristotelians to embrace *sometimes-ism* about explanation or definition.

## Objective Metaphysical Privilege

Now we can clearly see the issue. Neo-Carnapians are arguing for a general form of *always-ism*, which would naturally includes the *always-ism about explanation or definition*. The pressure that the neo-Carnapian argument would put on the neo-Aristotelians' *sometimes-ism* is that, what makes some definition or explanation privileged so that they get to trace fundamentality, but the other doesn't? 

I will later defend sometimes-ism against always-ism. In fact, the *general form of always-ism* is already under pressure. In the previous paragraph, I constructed an exotic toy model that satisfies "Superman is Clark Kent". According to always-ism, there is no problem with this model. Something already seems wrong. ==I later address this problem. Will I?== But for now I will ignore these problems, put on the hat of neo-Carnapians and explain why the neo-Aristotelians did not give any satisfying answer. 

In the following three sections, I will leave aside the Quinean-flavored view. This view establish fundamentality by pragmatic epistemology, i.e. the fact is determined by the best system that has the most pragmatic virtues like simplicity, elegance and explanatory power. I argue against this view in the second part of this paper. But standard neo-Aristotelians vehemently reject that metaphysics is just Quinean pragmatics. Therefore, in the next three sections, I attack the non-pragmatic neo-Aristotlian view that, we can objectively establish an absolute account of fundamentality that does not lead to semantic variance.

### Naturalness

Let me start with a way to establish a privileged definition using a familiar concept: *naturalness*. This notion was famously introduced by Lewis (==Citation a new theory of universals==), but his account was generally restricted to properties. It was Sider who attempted to use this concept to establish fundamentality (==Citation writing the book of the world==). Basically, Sider extends the notion of naturalness such that linguistic features beyond predicates (e.g., quantifiers, sentential connectives, sentential operators, predicate operators) can be evaluated by whether they carve the world at its joints or not.[^8] The linguistic features of the best language would then perfectly match the joints of reality. Given this extended understanding of naturalness, Sider contends that the most fundamental layer of the world is revealed by the structure of this best language. In this spirit, a "real" explanation/definition is the explanation/definition that only uses perfectly natural expressions.

But surely if the meaning of "naturalness" could vary between languages, then different languages could be considered "the best language" under different semantic interpretations of "naturalness." This is something Sider wants to avoid, because he argues that "naturalness" is supposed to be absolute and "objectively objective." The problem, then, is what makes the meaning of "naturalness" invariant across languages?

One way to defend this invariance is to appeal to reference magnetism. Sider stipulates that naturalness itself is perfectly natural (or, in his terminology, "structure itself is structural" ==p. 137==). And the meaning of the term "naturalness" is automatically attracted to the most natural candidate meaning (==Sider p.23==). Therefore, there is a unique answer to what the "reference magnet" is.

But this doesn't solve the problem. There is no contradiction in having two different languages, both of which agree on the sentence "naturalness itself is perfectly natural" while disagreeing on the extension of "naturalness." For example, in a mereological essentialist's language (ME-language), the ME-quantifier only ranges over simples, while in an ordinary language (OD-language), the OD-quantifier also ranges over composite objects. Assuming they both agree that naturalness itself is perfectly natural, there seems to be no contradiction in each side claiming its own quantifier to be perfectly natural, as long as they interpret "perfectly natural" differently. 

But the greater issue for this solution is that, it is more plausible to take there to be no reference magnets. As Hirsch (==citations==) and Warren (==2023==) pointed out, in the case where language use can determine the reference, magnetism cannot override it. Assume there are Martians seems as intelligent as we are, but they use '+' in a way that we need to interpret it as quus to make their utterance come out true. In this case, we should say the Martians are using '+' to mean quus. And in the cases of vagueness, like bald, where use cannot determine reference, we think it is just indeterminate. In other words, when language use determines the reference, magnetism does not exist. When the use does not determine the reference, magnetism does not exist. Therefore, magnetism does not exist.[^15]

The initial motivation to posit reference magnetism is to explain why our '+' means plus, not quus. It seems our *actual use* does not determine plus, while there is a strong impulse to say the meaning of '+' is plus, not quus. But "actual use" might be a too narrow view about language use. Language use should include our disposition of use, and the disposition is enough to say '+' means plus. More will be said later, but for now I believe this should suffice to dismiss reference magnetism.

Let's return to the original problem: if reference magnetism fails, then what makes the meaning of "naturalness" invariant across languages? More generally, when is any term's meaning fixed in this way?

In a trivial sense, of course, any symbol can vary in meaning; "naturalness" could be used to mean "hello," for instance. But this isn't the kind of meaningful variation we're concerned with here. There might be several ways to explain "invariance".[^9] But the relevant idea might help establish the invariance of "naturalness" here is that, the world can somehow teach us whether we are getting the right concept. If we are using a wrong concept, the world will correct us.

What do I mean by "the world can teach us the right concept"? Special relativity seems to be a nice example. We naturally think that changing inertial reference frames is just a sheer transformation, and the speed of light would depend on the observer's motion. But empirical facts tell us that, surprisingly, our intuition is wrong. No matter how we change our reference frames, the speed of light remains invariant. Therefore there are some privileged transformations, like Lorentz transformations, that objectively does better job at describing the world. Therefore, the world teaches us that, our seemingly natural intuition is in fact incorrect.

The parallel argument used to support an "objectively objective naturalness" is Goodman's riddle of induction. The basic idea is that, a concept like "grue" must be flawed because it leads to a wrong induction. Therefore the world teaches us that some concepts are objectively better than others. 

This is perhaps the strongest case for the objectivity of naturalness, but I believe it ultimately fails. To recap the riddle: Since all emeralds observed before 2050 are green, these emeralds are also by definition "grue" (where grue means green and observed before 2050, or blue otherwise). If we use grue for induction, the grue-induction would tell us that the next emerald we see after 2050 will be grue. However, we intuitively know the next emerald will be green, meaning it will not be grue. Since the green-induction and the grue-induction are formally the same, the problem does not lie in the structure of induction. Then the concept "grue" itself must be the problem.

But I think the world cannot teach grue-speakers that grue-induction is problematic. And therefore, for grue-speakers, there is nothing wrong with grue. Let's consider two cases.

**Case 1**: In the first case, *we* assume that "being green" is an essential property of the kind "emerald." But why would *grue-speakers* accept this? By the same token, they might take "being grue" as the essence of an "emerald." If we showed them a green emerald in 2051, they would simply say, "*This isn't an emerald, because it's not grue.*" In that case, their induction is perfectly reliable: everything they call an "emerald" is, in fact, grue. Of course, this scenario is overly simple because it assumes the kind "emerald" is defined solely by its color. If so, the disagreement becomes a matter of a priori definition, not a failure of induction.

**Case 2**: So, let's consider a more realistic and relevant case. Assume both we and the grue-speakers agree that emeralds are defined by their chemical structure ZYX, not their color. Through induction, our science posit a law of nature that "*ZYX necessitates greenness.*" In grue-science, however, induction would posit a law that "*ZYX necessitates grue-ness.*" Assume that when 2050 arrives, the next emerald turns out green. Therefore, grue-law will be falsified. Then it follows that, the laws of nature is simpler when formulated in terms of green. Thus, the world teaches us that, green is objectively better than grue. 

But this argument doesn't seem right for two reasons. First, Inductive failure is normal. A predicate can be natural, but simply fail to apply to the induction. For example, we might inductively conclude that all swans are white. The discovery of a black swan doesn't make the property "white" is objectively worse than "white or black". It just means we hit on a false hypothesis.

Second, the whole argument relies on the presupposition that, in grue-science, the grue-speakers start with the hypothesis *ZYX necessitates grue-ness.* But given they take the property "green if observed before 2050, or blue otherwise" to be simpler than "green", why wouldn't they instead adopt the hypothesis "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*" as simpler than "*ZYX necessitates grue-ness*"?

To prevent further complications, assume that ZYX is invariant in both our law and the grue-law. Let us even grant that the grue-speakers initially start with the hypothesis *ZYX necessitates grue-ness*, and that this hypothesis is falsified by the first emerald after 2050. After further work, grue scientists discover that the corrected grue-law, like "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*", is the right one.

Now, the previous argument runs, our law "*ZYX necessitates green*" is objectively simpler than this corrected grue-law. Therefore, green exhibits objective similarity, not grue. Why? Because the properties in the simplest laws of nature 'carve nature at its joints'.

But why would the grue-speakers accept this argument? My stipulation is that grue-speakers have different intuitions about simplicity and similarity from ours. They take properties, that are disjunctive in our terms, to be genuinely simple similar; likewise they treat disjunctive propositions as simpler. Given this stipulation, they would say their corrected law is actually simpler than ours. Therefore, grue and bleen are more privileged than green.

In other words, if the meaning of "the best system" could vary, then the best system account of laws of nature cannot be objectively established. If natural properties are specified by the laws of nature, while the laws of nature can vary in different frameworks, then "naturalness" can vary in different frameworks. Therefore, even if the grue-speakers agree with us that, natural properties are metaphysically privileged, the world cannot force the grue-speakers to give up a different extension of "natural properties".

Surely for us, there is no problem, in accordance with Bayesian principles, to adopt a higher credence for green-laws based on prior distribution. This may be enough to say we are justified to believe green-law, and this is enough for our confirmation. But there is no theoretical reason to take greenness as the metaphysically privileged concept across different frameworks. Nothing can stop "naturalness" having semantic variance.

### Essence

Maybe "naturalness" is already too niche. Indeed, the most orthodoxical neo-Aristotelian way to establish fundamentality is through concepts like grounding or essence. For example, the most fundamental layer is ungrounded (i.e. independent), and everything is either the most fundamental, or is grounded by the most fundamentals (i.e. compleness). In other words, the *sometimes-ism* about definition/explanation is explained by grounding or essence.

The concept of grounding, however, notoriously lacks consensus. Before getting further into this, I wish to go into a slightly earlier concept, i.e. essence.

The approach to establishing fundamentality through essence is largely due to Kit Fine's series of works (=="Essence and Modality," "Senses of Essence," "Ontological Dependence," and "The Logic of Essence"==). Of course, Fine prefers to treat ontological dependence as primitive. But Fine acknowledges that this is merely a technical choice, and we can instead define ontological dependence in terms of essence: $x$ depends on $y$ when there is some property $\phi$ (that does not mention $y$) such that in virtue of $x$'s nature it's true that $y$ has $\phi$, but in virtue of $x$'s nature it's not true that everything has $\phi$. Formally, according to Fine we can choose to define $x \ge y =_{df} \Box_x(y=y)$ along with other axioms in order to capture the informal definition. 

Here, I do not want to focus on the details of Fine's axiomatic system. And I acknowledge that, essence is indeed an intuitive concept upon which *de re* modality should be built, not the other way around. Instead, my question is, why should we take essence to be a feature of reality, not a feature of language? To show this, we need to show that, if there is an alternative language that attribute essence differently, then that attribution is *objectively* wrong.

Imagine a set-theoretic creature, maybe a robot called Robo, who perceives every object as a singleton set. For example, Robo directly perceives a table as $\{table\}$ and Socrates as $\{Socrates\}$. Only by theoretical reconstruction can Robo realize that what she perceives is necessarily related to the individual Socrates. For Robo, then, the individual we call Socrates is a theoretical construction from the entity she directly perceives. Therefore, from her perspective, Socrates depends on $\{Socrates\}$.[^10]

Is there anything objectively wrong with this creature Robo? Of course, there is a sense in which Robo is wrong because her attribution of essence violates our intuition. But this reason is *symmetric*: Robo can by the same token accuse us of being counterintuitive. Yet, if Robo's attribution of essence is *objectively* flawed, the reason cannot be symmetric. So, appealing to our own intuition does not count as a good reason to say Robo's attribution of essence is flawed.

I think the only hope to establish the invariance of essence attribution is to let the world teach us whether the concept we are using is wrong. For example, the wrong attribution of essence will make some induction fail, etc. But I can't see any way to do this.

### Grounding(Building)

Now let's get into grounding. Here what I mean by "grounding" here is Wilson's talk of "small-'g' grounding relations" (==Wilson 2014==), or what Bennett calls "building". In fact, let me switch to the terminology of "building" in the rest of the discussion, in order not to conflate with the specific grounding that is usually thought of as a relation between facts. I use building here because fundamentality may not just be established by the specific version of grounding.

Building relation is a set of relations that are directed, necessitating and generative (==Bennett citation==). Some examples are composition, constitution, set-formation, (property) realization, grounding and causation. To say that $x$ builds $y$ is to say $x$ bears one of these building relations with $y$. According to Bennett, fundamentality can be elegantly established by building relationships. In a rough way to put it, the most fundamental layer is unbuilt, and if $A$ builds $B$, then $A$ is more fundamental than $B$.

Immediately, this account of fundamentality is already *relative*, because there are different building relations. For example, according Shaffer's priority monism, the entire cosmos $builds_{grounds}$ its parts, but its parts $build_{compose}$ the entire cosmos. According to this view, there is no "objectively objective" fact of something being absoutely fundamental; Rather, something can only be fundamental *relative to a building relation*.

Even if so, we may still say, *the set of relations* that count as building relation is absolute and objectively objective. If so, then fundamentality, though being relative to a specific building relation, is still invariant across different language. The problem, then again, is whether this can be established.

In the discussion of essence, I have talked about the directness given by set-formation is problematic: the members of a set forms a set does not follow that the members are more fundamental than the set. For Robo, this direction might be reversed: instead of set-formation being a building relation, they take set-membership as a building relation. 

Recall Bennett's account about building: a building relation has to be *directed*, *necessitating* and *generative*. The problem here is the generative requirement. According to Bennett, the generative requirement is that,

> (G) For all building relations B, and all x and y, x's B-ing y makes true certain explanatory and generative claims. For example, if a builds b, then b exists (obtains, is instantiated . . . ) because a does, b exists (obtains, is instantiated) in virtue of a, a makes b exist (obtain, be instantiated), and so forth.

Surely, Robo agree with us that $\text{Socrates} \in \{Socrates\}$. But what they disagree with us that set-formation is not generative. They think set-membership is generative instead.

The problem can be made more general. If $Rxy$ makes $\square (x \leftrightarrow y)$ true, then let's call the relation $R$ *mutually necessitating*.[^11] Given any directed and mutually necessitating relation, we can always find a reverse relation that is also directed and necessitating. And reversal is not the only formal maneuver we can do. Say a binary asymmetric relation $R\subseteq D\times D$ is mutually necessitating. Then we can define inverse $R^{-1}=\{(x,y)\in D\times D \mid R(y,x)\}$. Given that $R^{-1}(x,y) \leftrightarrow R(y,x)$, it is easy to see $R^{-1}$ is also asymmetric and necessitating. Moreover, let $x\equiv_{\Box}y$ abbreviate $\Box(x\leftrightarrow y)$. $R$ lies inside the union of $\equiv_{\Box}$-classes, i.e. $R\subseteq\bigcup_{C\in D/\!\equiv_{\Box}}(C\times C)$. Hence within each $\equiv_{\Box}$-class $C$ of size $n$ one may independently pick for every unordered pair $\{u,v\}\subset C$ either $u\to v$, $v\to u$, or neither, producing $3^{\binom{n}{2}}$ asymmetric, necessitating relations on $C$. Whenever some $\equiv_{\Box}$-class has $n\ge 2$ there are therefore combinatorially many relations $R^{*}$ that (i) predicate over the same domain, (ii) are necessitating and directed, yet (iii) are extensionally different from a given directed, necessitating $R$.

Therefore, if there is any mutually necessitating relation $R$ in the set of building relations, and we want to rule out $R^{*}$, then the problem is how. Bennett suggest the generative constraint, but this constraint cannot establish the invariance of the set of building relations. For Robo, $\{Socrates\}$ exists explains Socrates exists. But we cannot say this is not the "real" explanation, because Socrates is more fundamental than $\{Socrates\}$. This just begs the question. And I don't see other reason to say there is any problem with Robo's understanding of fundamentality.

Indeed, I believe this is the most problematic part of Bennett's account of building relation. Surprisingly, even Bennett concede that what counts generative may be arbitrary, conventional, framework-dependent and "nothing further to be said" (==p. 59==) If this is the case, then how can we save the idea that the set of building relations is invariant and "objectively objective"? I can only see the following two ways out.

The first way is to simply not rule out the alternative relations $R^{*}$. This suggestion, however, makes the generative constraint redundant. It would mean that for any group of mutually necessitating entities, we could order them however we want and claim that our arbitrary ordering reveals a sense of fundamentality. But if a collection of things can be ordered in any arbitrary way, it seems the best way to describe the situation is to say that the things themselves have no intrinsic order. They just merely necessitate each other. Therefore, even if this solution formally works, this move will depart from the intended meaning of fundamentality.

A second way out is to deny that there can be mutually necessitating building relations altogether. If so, then relations like set-formation, the successor relation, and logical connectives would be removed from the list of building relations. This would essentially mean that applying "fundamentality" to abstract objects is a category mistake. However, some other relations, like composition, would be preserved. For example, if we do not adopt mereological essentialism, then material composition is not mutually necessitating, because the parts necessitate the whole, but the whole does not necessitate those specific parts.

But this move still does not make the set of building relations invariant. For example, the meaning of "composition" can vary. Mereological nihilists, organicists, common sense metaphysicians, and four-dimensionalists all have different interpretations of "composition". If we allow any of these interpretations to count as a building relation, then what is considered fundamental becomes arbitrary. This, again, seems to depart from the intended meaning of fundamentality.

In addition, it is not only the generative constraint that may lead to semantic variance. The necessitating constraint may also lead to variance, given that there are different senses of necessitation. This has been discussed by Sidelle (==The Grounding Mystique==): Given that modality is at least partly conventional, the deflationary challenge for metaphysical necessity extends equally to grounding.

## Upshot

The gist of the tedious discussion above is simple. Imagine an alien whose language contains "fundamentality" that plays the same role in their metaphysics as our "fundamentality" does in our metaphysics. However, their "fundamentality" has a different extension. If we does not rely on the Quinean epistemology, and claim that fundamentality is a *worldly, objective feature*, then we must be able to show why their concept is *objectively* wrong. Crucially, the reason we provide must not be symmetric; it can't be a reason the aliens could also use against us (e.g., "it violates *our* intuitions"). As the previous sections have shown, it seems we cannot find such a non-question-begging reason in the current way of establishing fundamentality.

It seems the only hope for finding this kind of non-symmetric reason would be the following: the world itself could teach us which "fundamentality" is the correct use. But the challenges from imagining strange aliens essentially press the following question: if we were to throw away some of our intuitions, would the world still be able to teach us anything? The upshot of the discussion is that *it would not*. In a slogan: ***If we are willing to throw away our intuitions, the world cannot teach us anything.***

I believe this slogan is true. However, in the next part of this paper, I show that some of our intuitions cannot be thrown away, and it would lead to substantial consequences.

# II

The arguments discussed in the previous section share a common pattern: we imagine a strange creature who thinks and speaks a radically different language. The challenge is that we seem unable to prove their language is metaphysically worse than ours.

A natural, initial response is to dismiss these creatures as simply _insane_. This is, in fact, the kind of response I want to defend. My strategy, however, isn't to argue that there is something wrong with these alien languages _per se_. Instead, I want to focus on what's wrong with the relationship between _us_ and these languages.

To put it simply, *certain constraints prevents us from coherently adopting and committing to just any theory*. If these constraints have substantial metaphysical consequences, then any theory we can coherently adopt must be consistent with them. Moreover, these metaphysical consequences will be *invariant* across all theories available to us.

Before getting into my account, one immediate problem arises. The criticism is that what I am saying is mere pragmatism, and that deep metaphysical truth cannot be revealed by pragmatism.

There is some force to this criticism, and require a much longer discussion elsewhere. But here consider one uncontroversial constraint: inconsistency. Why do we think an inconsistent theory is wrong? Indeed, we might even worry that the world itself is inconsistent. Yet we dismiss inconsistent theories. The reason is simple: we are incapable of following an inconsistent theory.[^12]

In other words, if the aim of metaphysics is to understand reality, we cannot coherently understand inconsistent theories. If the world were truly inconsistent, then that kind of metaphysical knowledge, in Socratic terms, would belong not to human beings but to God. For human beings, then, we should pursue human knowledge and understand reality in a human way, not in God's way. Therefore, if metaphysics is to be a subject of human inquiry at all, it must accept our human limitations, and thus accept these constraints on theory choice.

Let me clarify further. One might object that physics already presents a picture of reality that is far beyond our understanding. For example, it's very difficult to truly make sense of concepts like fields, curved spacetime, or probabilistic states. Yet, these are highly successful theories, and we don't reject them. In fact, we make use of them all the time.

To reply, I want to emphasize again the difference between *adopting* a theory and merely *utilizing* it. This distinction is clear in quantum mechanics. Physicists apply the Schrödinger equation to compute predictions and do experiments. Yet they may remain silent about how to interpret the equation. They just shut up and calculate. If so, then they are merely utilizing these equations without adopting them into their worldview. What I am arguing for is the constraint on adopting a theory. There is no problem with using a theory that violates these constraints to do experiments or make predictions. However, as long as we want to truly understand the reality a theory claims to reveal, these constraints apply.

## The principle of Sanity

The constraint I suggest is the following:

- **The principle of sanity**: For someone who is capable of philosophical reflection, it is impossible to genuinely believe oneself to not be a rational agent.

### Clarification

This principle also applies to acceptance (i.e. It is impossible for someone to accept oneself to not be a rational agent), but I will focus on belief in the rest of the paper unless specified otherwise. Anyway, the upshot is, it is impossible for us to really adopt a theory that eliminates rational agency.

Here, "philosophical reflection" roughly refers to activities such as examining and revising one's beliefs, analyzing arguments, and choosing between competing theories. The condition "for a person capable of philosophical reflection" is to rule out certain exotic counterexamples. For instance, someone who has been so thoroughly brainwashed that their only possible belief is "I am not a rational agent" would not qualify as a counterexample. I assume all readers who have already made it here are automatically capable of philosophical reflection.

What I mean by "rational" here is a minimal sense of means-end coherence. For example, someone who wills their hand to rise but randomly cause their leg going up would lack this coherence. Thus, even if this randomness could somehow count as agency, it wouldn't be rational agency in the relevant sense.

The precise meaning of "rational agency", of course, is highly controversial and open to multiple acceptable interpretations. Consequently, this principle will have different variants depending on how one chooses to define the term. However, I contend that the *principle of sanity* is plausible under most, if not all, commonsensical interpretations.

For current purpose, it will suffice by adopting a minimal account of agency as*epistemic rational agency*. But I will add more to "rational agency" later in this paper. ==to be edited== What I mean by "epistemic agency" here is not the kind of agency resisted by doxastic involuntarists. According to doxastic involuntarists, *direct* voluntary control over belief is not possible for us. But here, I use "epistemic rational agency" to refer to any sort of non-random control over our beliefs, no matter how indirect. For example, we have the ability to choose to re-examine our beliefs and investigate new evidence. Doxastic involuntarists, like ==Hieronymi 2008==, also acknowledge that we possess this kind of indirect control. Therefore, it is uncontroversial that we have epistemic rational agency in the intended sense.

In the following discussion I will just use "epistemic agency" as a shorthand for"epistemic rational agency", unless I want to stress the means-end coherence aspect of my intended sense of agency.

### Defense for this principle

This is more or less an axiom in this paper that I take to be self-evident. But I will still try to give some defense. 

First, implicitly, this is already a widely accepted principle. Aristotle appears to adopt this principle in Chapter 9 of _De Interpretatione_. There, he rejects the principle of bivalence due to our deliberation and action. He argues that, deliberation and action requires that, propositions about the future must be open (19a10). Then they cannot have a truth value now (19a26). Therefore the principle of bivalence is false. However, Aristotle's argument seems to epistemic instead of metaphysical. When he writes that "what will be has an origin both in deliberation and in action" (19a6-9), the plausible interpretation is that our *beliefs* about the future originate from these rational activities, not the future state of affairs itself. If so, his argument is best understood as follows: it is impossible for us to genuinely *believe* that the future is predetermined while simultaneously viewing ourselves as rational agents who deliberate. Therefore, by *the principle of sanity*, we cannot adopt the principle of bivalence.

There are also more contemporary cases. People seems to reject epiphenomenalism largely due to the reason that, if there's no mental causation, then it seems we wouldn't have any agency, which is insane. And I will argue later that, the motivation of a widely held metasemantics principle, *the Principle of Charity*, actually comes from of a stronger version of *the Principle of Sanity*. Therefore, in fact people have been implicitly using this principle.

Nonetheless, I will also try to sketch a direct defense. If the principle were false, then it would be possible for a creature to engage in philosophical reflection without genuinely believing itself to have epistemic agency. I claim that this is impossible.

Philosophical reflection requires understanding. Therefore, it is not enough to just mechanically following linguistic rules. One must *adopt* a language and form concepts out of it. Here, the relevant sense of forming concept must involve intentionality. Mere production of symbols without intentionality does not count as adopting a language. For instance, a dinosaur may always scratch the ground in a way look like the symbol "water". But does not count as an evidence that some dinosaurs speak English. But the intentional, goal-directed use of language for philosophical reflection is itself an exercise of epistemic agency. It puts the agent in a position to choose to examine, endorse, or revise particular beliefs. 

Moreover, when one intentionally uses language in this way, they must have a minimal awareness to the intentions. This minimal awareness of intention entails a minimal belief in one's epistemic agency. Therefore, a creature that engages in philosophical reflection cannot genuinely disbelieve epistemic agency.

Maybe this is not an invincible argument. Perhaps with some heavy argument, one may claim we can build a "philosophy machine" that can reflect on arguments and produce a philosophy paper while being completely non-agential. Even if so, I believe it is quite obvious that rejecting the principle of sanity, plainly, is to be insane. Even if such a "philosophy machine" is possible, I still think no human being should reject this principle, simply because we should not be insane.[^13]

There are obvious consequence for adopting this principle. Together with the following innocent principle that:

- **Ought Implies Can**: If it is impossible for us to believe that P, then it is not the case that we ought to believe that P.

It follows that *the Principle of Sanity* is incompatible with the traditional account of rationality: A belief is rational iff the reason to believe is truth-tracking. This is because the principle of sanity commits us to a belief we must hold (that we are rational agents), regardless of the evidence. Assume our epistemic norm is to hold only true beliefs, and scientists tell us that rational agency is just an illusion, then we would be obligated to believe that we are not rational agents. But according to the principle that *Ought Implies Can*, there is no such obligation.

Then immediately, one might object that if epistemic norms must be truth-tracking, then we should rationally reject *the Principle of Sanity* itself. However, what does it mean to "rationally reject" a principle? If doing so is an act of philosophical reflection, then, as argued earlier, the very act requires us to presuppose our own epistemic agency. Therefore, any attempt to "rationally reject" *the Principle of Sanity* is self-defeating, as the act of rejection presupposes the very rational agency the principle commands us to accept.

There is also an auxiliary argument. In the first part of this paper, I put aside the neo-Quinean epistemology for establishing fundamentality. According to this view, the best theory of fundamentality that has the most pragmatic virtues like simplicity, elegance and explanatory power. But what how could these pragmatic virtues more valuable than our own rational agency?

## Metaphysical Consequences

Given the Principle of Sanity, some direct corollaries follow. The basic idea is, given that we must think we are epistemic agents, we cannot coherently adopt a theory that says we aren't.

### Negative Corollaries on causation and personal identity

**One straightforward corollary is on causation.** To be an agent means to bring something about. 'Bringing about' is a causal notion. Therefore it is analytically true that, there is agency only if there is causation. It directly follows that, we cannot the following kind of theory: There is no such thing as causation. All there is is just a description of Humean regularity, possible worlds, etc.

To clarify, I am not against non-eliminative analysis of causation. It is meaningful to show how causation connects to other ideas. All I am saying is that, if we eliminate causation yet still believe we are epistemic agents, we would be inconsistent.

**Another corollary is on personal identity.** Let me state the conclusion first and then explain why. Given *the Principle of Sanity*, we cannot adopt the following kind of theory: "I" is an illusion and/or does not exist. All there is is just psychologically continuous or connected personal states.[^14]

This kind of impersonal view is advocated by Hume and Anscombe (==some citation here==). But a more important advocator of this view is ==Parfit 1984==. Surely, Parfit did not went that far to say that "I" does not exist. However, according to Parfit, there is nothing deep about personal identity. It would be better if we describe our lives in an impersonal way, without using "I" or other concepts that require person identity. He argues that doing so reveals a deep and liberating truth about our nature.

However, according to the Principle of Sanity, we cannot adopt such an impersonal language. This is because rational agency requires first-person pronoun. To prove this, I will go back to the more general *rational agency* instead of the specific *epistemic agency*. But nothing important hangs on this. The proof also applies to *epistemic agency* with some modification of the example.

The proof is as follows: Rational agency requires de se attitude. De se attitudes requires the *adoption* of first-person pronouns 'I' (or other syntactic equivalence). Therefore, rational agency requires us to read reality off the first-person pronoun and adopt the existence of "I". 

What is attitude de se? This terminology is introduced by Lewis (==attitudes de dicto and de se==), but the motivation comes from Perry (==the essential indexical==). In a nutshell, there are a type of propositional attitude that are essentially different from other kinds of attitudes: believing that I..., intending that I..., regretting that I.... This uniqueness of this attitude is illustrated in the Perry's famous example:

> I once followed a trail of sugar on a supermarket floor, pushing my cart down the aisle on one side of a tall counter and back the aisle on the other, seeking the shopper with the torn sack to tell him he was making a mess. With each trip around the counter, the trail became thicker. But I seemed unable to catch up. Finally it dawned on me. ***I was the shopper I was trying to catch***.  
> I believed at the outset that the shopper with a torn sack was making a mess, and I was right. But I did not believe that I was making a mess—at least not initially. That belief came later. And when it did, I stopped circling the counter and rearranged the torn sack in my cart. (Perry 1979)

Only after Perry acquires the *de se* belief and thereby changes his behaviour, can his action be described as rational. A merely *de re* belief does not suffice. Suppose Perry sees a person, and forms the de re belief "this (person) is making a mess". Unknown to Perry, he is actually looking in a mirror, and "this" refers to Perry himself. Now Perry believes "*this is making a mess*" without believing "*I am making a mess*". Therefore, these two beliefs are distinct. 

But notice that, the sentences "*this is making a mess*" and "*I am making a mess*" have the same propositional content (in Kaplan's terminology). If the propositional content are the same but the two beliefs are distinct, then they must be believed in different ways:  "this is making a mess" is a belief *de re*, while "I am making a mess" is a belief *de se*. 

More importantly, rational action can only be produced by *de se* beliefs. One cannot rationally act on *de re* belief. It would be purely random if Perry stop chasing after the following: He look in the mirror, believing that *this person* is making a mess without believing that *he himself* is making a mess.

Some, myself included, have been tempted to resist this *de se* essentialism for action by arguing that one can act rationally just on *de re* beliefs. I think that resistance can't work.

For instance, @cappelenInessentialIndexicalPhilosophical2015 argues that there is nothing special about *de se* attitudes. For example, "Superman can fly" and "Clark Kent can fly" are both *de re* beliefs, and one can believe the former without believing the latter. Hence, we do not need to posit a *de se* attitude to explain the difference between the belief that "*This person is making a mess*" and the belief that "*I am making a mess*": 'I' works like a proper name and just yields *de re* attitudes.

The challenge for this account is to explain: why it appears that, only I-beliefs and I-intentions can produce rational action. For example, I can intend Perry to stop, but this intention cannot therefore directly make Perry to stop. But if I intend myself to stop, and this intention would directly make myself to stop.

If one insists that I-beliefs are nothing more than ordinary *de re* beliefs, one cannot also claim the following: I-beliefs are a special subtype of *de re* beliefs that uniquely produces action. This move merely renames *de se* attitudes as "*special de re* attitudes". It makes no substantive difference. To deny that the *de se* attitude is essential for action, one must maintain that all *de re* attitudes can in principle produce action.

This is indeed the direction Cappelen and Dever take. To make sense of this counterintuitive claim, they propose the so-called Action Inventory Model. According to this model, one keeps an inventory of actions they can perform. When Perry sees another person (say Kripke) making a mess and wants that person to stop, he forms an intention that *Kripke should stop*. But Kripke's stopping is not a possible action in Perry's inventory of action. So nothing happens. By contrast, when Perry forms the intention that *I should stop*, this intention picks out an action that is in Perry's inventory of action, i.e. Perry's stopping. Perry therefore stops.

But this account obviously doesn't work. In order to get rid of the complications that are brought into by the mirror, let's imagine the following scenario. 

Perry woke up in an empty room with a television. He suffers amnesia and forgets his name. Then he watches what he thinks is a live television broadcast. On the screen a person tagged "Perry" is being chased by a lion. Feeling sympathy, he forms the intention, "*Perry should run away now*."

Unknown to him, the footage is old and the person on screen is actually himself. This hidden fact does not alter his belief state: he still thinks the broadcast is live, and he still thinks "Perry" names some stranger. But his intention is indeed "Perry should run away now." According to the Action Inventory Model account, this is a *de re* intention, and Perry would run. But obviously it is not a rational action for someone to flee simply because they think a stranger on television should flee.

But let's even grant it that, in this case, Perry produces a rational action. Here, a bigger problem is the following. We can construct a similar scenario in which the person on the screen is Perry's identical twin brother, Berry (assuming there is such a person). In that scenario, Perry's qualitative experiences are identical to the original case; therefore the intentions he forms are the same. So Perry should produce the same action in both cases. But according to the Action Inventory Model, Berry's running is not available to Perry. Hence, if Berry is the person on television, Perry would not run. 

In other words, the problem with any denial of *de se* essentialism is this: It not only yields strange results, but it leads to different results in qualitatively identical scenarios. In any situation where Perry produces a rational action, we can always construct a case where everything is qualitatively identical for Perry. But in this case, the content of the *de re* belief is, accidentally, about someone else. If action depends on *de re* content, then the same intention would accidentally produce different unexpected results. But I have clarified earlier that this is not rational agency because it lacks minimal means-end coherence.

### Positive Corollaries on causation and fundamentality

So far, I've illustrated two direct corollaries of the Principle of Sanity on causation and personal identity. These are *negative* conclusions, telling us which theories we _cannot_ adopt. However, the principle can also yield *positive* results.

#### From the meta-level to the object-level

Immediately a question arises here. *The Principle of Sanity* is a meta-theoretical constraint. Why should an object-level theory has to adopt the content of the meta-level theory?

Here's the basic idea: If accepting *the Principle of Sanity* entails the acceptance of the metaphysical consequence that $P$, then $P$ becomes an invariable part for any metaphysical framework we can coherently adopt. Any attempt to vary $P$ semantically would violate *the Principle of Sanity* and thus be impossible for us to adopt. Hence, we get the "objectively objective" metaphysics that we want.

Some heavyweight metaphysicians will probably vehemently resist this and argue the following: This is a meta-metaphysical claim: $P$ is invariable in any metaphysical theory we can coherently adopt. But it does not entail the metaphysical claim: $P$ is metaphysically true. The former is a claim about theories, the latter is about the world. We cannot know anything about the world just by think about theories. Therefore, it is impossible for us to make any move from the meta-level to the object-level. 

There are two ways to answer this. The first way is to acknowledge this problem. For any object-level claim being discussed in the following sections, one should read it as a meta-level constraint. For example, I will argue for an agent-based interpretation for intervention. One should translate this as the following: we cannot coherently adopt a theory that claims semantic invariance yet does not give an agent-based interpretation of intervention.

The second way is to maintain that the entailment holds, and the heavyweight metaphysicians have a wrong conception of metaphysics. If it is metaphysically possible that $\neg P$, while we cannot coherently adopt that $\neg P$, then it is possible that we cannot coherently understand the world. But in practice, we reject this possibility all the time. Again, when $A$ and $B$ are inconsistent, we never seriously consider the possibility that both $A$ and $B$ are true. What we do is to argue for one and negate the other. We establish metaphysical conclusions in this way all the time. To the extent that metaphysics is a subject of human inquiry, we should reject the possibility that we cannot coherently understand the world.


We have to notice an important fact that is always ignored: Surprisingly, we are part of the world. We are not mysterious entities roaming outside the world. If the Principle of Sanity entails a fact about *us*, and we are objectively a part of the world, then *the Principle of Sanity* entails a fact about the world. Given that the object-level metaphysics aims to provide an adequate thoery of the world, it must account for *us* as well. Therefore, a fact about us entailed by meta-level constraint must be also included into the object-level theory in order to be adequate.

Recall my defense for *the Principle of Sanity*. I argued that, all readers who have already made it here are automatically capable of philosophical reflection. And if we are capable of philosophical reflections, then we must have epistemic agency. Therefore, one metaphysical consequence entailed by the principle is that, we should read "agency" off our language and take it as real. 

#### Causation

I want to argue that *the Principle of Sanity* gives an agency-based interpretation to the interventionist approach to causation. 

Let me give a quick introduction to this interventionist approach. This approach uses Structural Equation Models (SEM) to analyze causation. Nowadays among statisticians and scientists, this is the probably most popular way to make causal inference. The most basic interventionist causal analysis is this: $A$ causes $B$ if intervention on $A$'s state will change $B$'s state. The latter part can be formalized and evaluated by causal models. Let's quickly go through the basic formalization to get the gist. A causal model is $M=\{U,V,F\}$ where $U$ is a set of exogenous variables, $V$ is a set of endogenous variables, and $F$ is a set of structural equations that determines the value of endogenous variables by exogenous variables. Given an assignment of exogenous variables $u$, we will have all the values in $M$ fixed. Assume $A=a$ and $B=b$ in the actual world. To analyze whether $A$ causes $B$, we surgically set the value of $A$ to $a' \neq a$  and therefore produce a new model $M_{A=a'}$. Given the same assignment $u$, if value of $B$ in $M_{A=a'}$ is not $b$, then we say $A$ causes $B$.

Surely, this basic counterfactual account does not work in the cases of preemption and overdetermination. ==Halpern & Pearl 2005== developed a more complicated account of actual cause that solves these problems. But here I don't want to go into these details. Metaphysicians are more or less skeptical about this account not because of its detailed formalization. The worry of this approach is that, the analysis irreducibly rely on a "surgical intervention" to set the value of a variable. But this very concept of "intervention" is a causal concept. Therefore, this account is using causal notion to analyze causal notion, which is circular. 

And this circularity leads to *a problem of interpretation*. All languages need interpretation. The formal language of SEM also needs an interpretation. But if the semantic interpretation in turn requires SEM, then it will lead to infinite regression. The standard reply is to admit that causation as primitive, and the notion of intervention depends on causation. But how come that we automatically have a primitive interpretation of intervention? I believe more can be said.

##### The Strong Principle of Sanity

Now, I want to derive a positive account of causation from *the Principle of Sanity*. But the current interpretation of "rational agency" is *epistemic agency*. To the extent that it necessitates causation, it only necessitates the causation between *mental states*. Therefore, if we want to establish causation among physical objects, we have to adopt a stronger reading of "rational agency" in the _Principle of Sanity_. 

Now, "rational agency" not only include *epistemic agency*, but also *the capacity to control our bodies*. Call the strengthened claim _the Strong Principle of Sanity_. Note that my earlier defense appealed to the analytic entailment of *epistemic agency* from the capacity for philosophical reflection; That defense does not extend to *the Strong Principle of Sanity*: a smart ghost has no control over any body, but it might be capable of philosophical reflection. However, there are ways to make *the Strong Principle of Sanity* a priori again. For example, one could formulate it as 

- For someone who is capable of moving one's eyes intentionally in order to read this paper, it is impossible to genuinely believe oneself to not be a rational agent.

I will not further discuss the details of this maneuver. The gist is to make the constraint entail rational agency. And given that we are, we cannot genuinely believe we aren't. It just seems insane for any human being to reject this.

Now given *the Strong Principle of Sanity*, we get the indispensability of the agency of bodily control. But this kind of agency just is mental-physical causation. I will use it to establish physical causation. But again, the worry is that, our intended account for physical causation is worldly and mind-independent. Can mental-physical causation be used for an account for physical causation?

What is the worry when we say something is not "worldly and objective", and therefore should be excluded from a metaphysical analysis? It relies on the fact that, what's *out there* does not depend on how we think about it. For example, aesthetic values are typically not something *out there*. It changes just because my change of mind. But surely, the mental-physical causation itself can't change just because I change my mind. Therefore, mental-physical causation is a good enough worldly candidate for analyzing physical causation.

##### Solving the Interpretation Problem

Previously I discussed how to interpret "surgical intervention." Why must the intervention be "surgical"? Because we must ensure that the intervention affects only the target variable. Other variables may change as a result of intervention. But their changes should only be determined by the structural equations. The standard worry is that empirical interventions are never truly surgical. Assume we want to intervene on the current in a coil. We cannot directly create current to flow in a coil; we have to create a moving magnetic field. This in turn requires moving a magnet, and so on. And presumably this would lead to tons of variable to change. Therefore, no worldly intervention has the purity the SEM formalism presupposes, and the $do$ operator lacks an interpretation.

Given the _Strong Principle of Sanity_, the answer is straightforward. To surgically intervene on a physical event is to posit direct mental causation of that event. By mentally causing the relevant physical variable to take a new value, we obtain a clean, surgical intervention. Since we must *adopt* rational agency, we thereby have an interpretation of surgical intervention on physical events. And because interventionist models can analyze physical causation, we can, via SEMs, develop a full account of causation.

In short, on my interpretation there is no problematic circularity in the interventionist account. It analyzes non-agential causation by appeal to a primitive notion of agential causation, and the *Strong Principle of Sanity* licenses that primitive.

In fact, agency-based interpretations were actually the predecessors of contemporary interventionism. ==von Wright (1971); Peter Menzies and Huw Price (1993)== developed agent-based accounts that aimed to reduce causation to agency. However, that kind of reductionism does not work, because it is just impossible to separate the notion of causation from agency. My interpretation differs. I do not try to reduce causation to non-causal notions; I am just analyzing non-agential causation in terms of agential causation.

Another problem is more salient: an agent-based analysis of causation seems anthropocentric and subjective. I have argued that the mental–physical causation required by the *Strong Principle of Sanity* is worldly and objective, so the charge of subjectivity fails. But the accusation of anthropocentrism remains. A standard objection runs as follows: in the early universe there were arguably no agents. If physical causation is analyzed by agency, then there was no causation in the universe until agents existed. That consequence is surely wrong. So, the objector concludes, physical causation cannot be analyzed by agency.

My reply is that actual causation is analyzed by *metaphysically possible* intervention. Such intervention depends not on actual agency but on *metaphysically possible* agency. And by the *Strong Principle of Sanity*, we know there is actual agency; what is actual is possible. Therefore, even when there are no actual agents in the universe, there is still causation, because agency is possible. Again, the notion of agency here is not anthropocentric: it specifies no particular human body parts. It must be so. We cannot know a priori which parts of the world comprise our bodies, and technology can extend what we control. Therefore, in principle, there is no restriction on what could fall under agential control.

Let me explain where my argument lead us. The direct positive conclusion from the **Strong Principle of Sanity** require us to take mental and mental-physical causation to be *really out there*. And through *utilizing* interventionist's formal language, we can talk about physical causation. But it is not required for us to *adopt* this language and say causation is really our there. Indeed, what I am claiming is compatible with a deflationary view about physical causation, but not mental-physical or mental causation.

#### Fundamentality

Finally, it's time to go back to where I started: fundamentality. The reason why I spent so much time on the interventionist account is that, it has a unique feature that I need to establish fundamentality: It ensures the asymmetry. 

In the most basic counterfactual analysis of causation, A actually causes B if (1) A and B both occur in the actual world; and (2) if A had not occurred, then B would not have occurred. On Lewis-style possible-world semantics, (2) is true if, in the closest worlds to the actual world where A does not occur, B also does not occur. However, this semantics does not rule out symmetry. If, in the closest worlds where B does not occur, A also does not occur, we would conclude that B actually causes A. Setting aside the possibility of cyclic causation, causation should be asymmetric. Yet there is no straightforward way to modify Lewis-style possible-world semantics to guarantee this asymmetry.

By contrast, in interventionist semantics the asymmetry is ensured. Let us use the toy model $M = \{\{A\}, \{B\}, \{B = A\}\}$ and the actual assignment $u(A) = 1$ to illustrate. The counterfactual "if A had not occurred, then B would not have occurred" is true because, given $u$, the value of $B$ in $M_{A=0}$ is 0. But the counterfactual "if $B$ had not occurred, then $A$ would not have occurred" is false because, given $u$, the value of $A$ in $M_{B=0}$ is 1. In other words, so long as the model is acyclic, the asymmetry is ensured by design.

Is this circular? It seems that the structural equation $B = A$ in $M$ already presupposes this asymmetry. If we treat $M$ as an appropriate causal model because we presuppose that $A$ causes $B$, then the reasoning is circular. However, structural equations can be discovered empirically via intervention tests. For example, to test whether $B = A$, we can intervene on $A$ and observe how the value of $B$ changes. Therefore, the world can teach us whether is an appropriate model, without circularity.

Now, recall my problem for Bennett's account of fundamentality: for any abstract objects 

## Beyond metaphysics: trusting friends against evidence



## Conclusion



[^1]: Needless to say, in a complete model for English language, many other vocabularies need different interpretations.
[^2]: I'm focusing on quantifier variance because I believe it's the most plausible and basic form of the neo-Carnapian position. ==Thomasson (2015 p. 70)== herself thinks her easy ontology doesn't depend on quantifier variance, but this is surely mistaken. Her project relies on the triviality of changing the extension of the predicate "exists". But this surely changes the meaning of the quantifier.
[^3]: Surely this paper is not intended to become the zoology of metametaphysics, so these labels may not be precise. What I mean by neo-Aristotelians is basically the fans of fundamentality. For example, Sider counts as a neo-Aristotelian in my sense. Though Sider is usually considered as a neo-Lewisian, which is closer to neo-Quinean. But the label is not important anyway.
[^4]: A less plausible answer is that they cannot make sense of my question. My reply is, I cannot make sense of this answer. As long as one can make sense of the distinction between merely utilizing a language and truly adopting and think in a language, my question should make sense. For example, even Hirsch accept this distinction in ==Dividing Reality, p.14==: "*one does not abandon a position merely by adopting a language*". There are some terminology issue here. What Hirsch means by "adopting a language" is what I mean by "utilizing a language", and what Hirsch means by "taking a position" is what I mean by "adopt a language".
[^5]: This "under the guise" talk may be more rigorously put in 2D-semantics, but gist is simple. It is motivated by the following kind of phenomenon. "Superman can fly" and "Clark Kent can fly" are the same fact, but they are different objects of doxastic attitudes: one might believe the former but not the latter. One way to explain this is to say that someone believes of $x$ *under the guise* of 'Superman', that $x$ can fly, while not believing $x$ under the guise of 'Clark Kent' that $x$ can fly. The basic idea of the "guise" account is that, even if an expression directly refers without relating to a description, the belief content may still involve something linguistic beyond the referent. Kripke's puzzle about beliefs shows that, this also extends to sentences (==citation to Kripke==). A French sentence $S_{F}$ and an English sentence $S_{E}$ may co-refer to the same proposition $P$, yet one may believe that $S_{F}$ but disbelieve that $S_{E}$. The solution is to say one believe that $P$ *under the guise of* $S_{F}$, but disbelieve that $P$ *under the guise of* $S_{E}$. If there are equivalent languages can serve as equally good guises, there are many different ways to form beliefs about the reality. Each of these guises constitutes genuine, true knowledge. This move allows neo-Carnapians to embrace a form of lightweight realism, distancing themselves from an anti-realist never-ism. 
[^6]: However, this approach immediately faces a challenge. If I treat other people's languages as mere guises of reality, why shouldn't I treat my own language as a mere guise as well? Moreover, if my language is also a guise, then it seems we would collapse back into the anti-realist's never-ism, because it presupposes a reality that is unspeakable without guise.<br>In fact, the answer to the challenge is worth a paper-length discussion, so I cannot fully address it here in a footnote. To sketch the answer, consider the following case. First, let's establish that the set of all well-formed English sentences, taken as finite strings, is countable. We can have a bijective function $f$ that maps every unique English sentence to a unique natural number. From this, we can construct a One-Word-Sentence language (OWS-language for short). In this language, each natural number (and thus each corresponding English sentence) is represented by a single, unique symbol (e.g., '∇', '§', 'Д'). Crucially, these symbols carry no internal syntactic information that mirrors the structure of the original English sentences. Now, imagine a creature that genuinely thinks in this OWS-language, not one that is secretly translating from English. Suppose that when we apply the inverse function, $f^{−1}$, to decode their one-word utterances, we find they agree with us on the truth value of every proposition. They even affirm the OWS-symbol that decodes to "There are objects and properties." Here is the problem. This creature's way of thinking is inconceivable to us. Given we stipulated that they think in OWS, it seems they cannot grasp the concepts of objects and properties. Yet, their statements, once translated, shows they think there are objects and properties. It is in cases like this that we must appeal to the "guised reality" view. We are forced to say that they believe there are objects and properties under the guise of their OWS-language, because there is no other way to understand their take on reality. Or in other words, we are *unable* to read reality off this OWS language. In contrast, our own understanding is transparent. We do not take of our language as a "guise" because it is normally a transparent *presentation of the reality*, not representation. We only appeal to "guise" in specific cases of misalignment, such as with co-referring proper names like 'Superman' and 'Clark Kent.'
[^7]: This is probably not the orthodoxical formulation. The orthodoxical formulation is, we distinguish absolute fundamentality and relative fundamentality. For absolutely fundamentality, we usually appeal to concepts like completeness, dependency, indispensability or naturalness. But when asked which set of things has this feature, we appeal to concepts, like grounding. And the standard story is to treat these relevant concepts as primitive. But then we need an epistemology for this primitive grounding. Ultimately the conclusion of "what grounds what" appeals to virtues like explanatory power or definitional simplicity. Therefore, ultimately we decide what's absolutely fundamental based on explanation or definition. I have not seen how could this be done otherwise. This also applies to relative fundamentality.
[^8]: In fact Sider tend to drop the term "naturalness" to favor the term "joint-carving" or "structural" for the extended meaning of "naturalness". But let me stick to "naturalness" here.
[^9]: One possible approach is to define the essential meaning of a term by its introduction and elimination rules (IE-rules), that is, by its functional role in language. This may be intuitive for logical constants. For a term to count as a quantifier, for example, it must obey the IE-rules for quantifiers. However, the full meaning of a quantifier also depends on its domain, and the IE-rules don't fix this. Therefore, the meaning of a quantifier can still vary as its domain varies. This approach faces several problems. First, this form of essentialism about meaning seems arbitrary. Why should IE-rules be the only thing that constitutes a term's essential meaning? For example, we might say in some sense 'quus' is a variation of 'plus', not a completely unrelated concept like 'Hello'. But they have different IE-rules. Second, for non-logical terms, the IE-rules themselves are usually formulated using terms that can vary. Third, and most importantly, even if we accept this IE-rules essentialism, it would not help fix "naturalness". It leads to a specific conclusion: a term's meaning is invariant if its meaning is exhausted by its IE-rules. For example, a logical conventionalist might say $\land$'s meaning entirely determined its IE-rules. Then according to IE-rules essentialism, $\land$ is invariant. However, Sider clearly intends "naturalness" to be more than just a conventional, syntactic symbol. Therefore, this strategy for securing semantic invariance is not a viable option for his project anyway.
[^10]: If this example is puzzling to you, you can try the following notation: she uses "Socrates" to refer to $\{Socrates\}$, and "-Socrates-" to refer to Socrates. Mathematically speaking, set formation has nothing to do with intrinsicality. Our intuition that *a member of a set* is somehow intrinsic and thus more fundamental than *the set itself* is just our *interpretation* of set formation, influenced by ordinary language words like "member" and how we use brackets to warp the members. But we could have used alternative notations, like using "M-relation" instead of membership, and use $\{\}-Socrates$ intead of $\{Socrates\}$.
[^11]: Bennett's formulation of the necessitating requirement is a bit more complicated than this. She also consider the circumstance of necessitation (==p.52-54==). But here I will just use the cleaner way to formulating it for the toy example.
[^12]: This is an empirical claim about our limits. I don't deny the possibility that a few intellectually superior beings, perhaps through mutation, or perhaps like Hegel, could handle inconsistent theories.
[^13]: In other words, I am in fact suggesting two different principles. The stronger version is what I explicitly laid down and inclined to stand with. But even if one somehow resisted it, my conclusion can still be established by the weaker version, i.e. if we are capable of philosophical reflection, then we should not believe we are not rational agents.
[^14]: ==Hirsch forthcoming== discussed this corollary in chap. 3.
[^15]: In fact, there might be resistance. One can claim that, there can be reference magnetism when we want to use words in this way. However, according to this account, which referential candidate gets the magnetic power still depend on our disposition of use. Therefore, I don't see how there could be any heavyweight reference magnetism that can support Sider's view.