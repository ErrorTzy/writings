# I

## Intro 

This paper is probably not a typical metaphysics paper. Although the main upshot is on metaphysics, the kind of move I make applies beyond metaphysics. 

In the background, this paper deals with a central tension. On one hand, it seems that, ultimately all possible philosophical conclusions must be derived from intuitive presuppositions. If so, then we cannot establish anything if we throw away all of our intuitions (or dispositions, sentiments, etc.). On the other hand, many of our intuitions are unreliable, and even contradict one another. The problem, then, is to figure out which part of our intuition cannot (or at least, should not) be thrown away and can safely rely on.

In the foreground, this paper focuses mainly on metaphysics, but the argument should also extend to other areas. To be specific, I first argue that the background tension is somehow ignored in discussions about fundamentality. As a result, the idea that facts about fundamentality are "objectively objective", though well-motivated, seems to rely on magic or wishful thinking. 

Then I argue for a principle that basically says, we cannot throw away our rational agency. Given this, it can be used to establish a direct, non-magical account of fundamentality.

## The Question of Reality

The starting point of this paper comes from the following question: *How much reality should we read off our linguistic features?*

This question appears central to almost all metaphysical problems. For example, 

- Almost all natural languages have a subject-predicate structure. Should we then say, the world itself is constituted of objects and properties? 
- Grammatically we have predicates like 'good' or 'bad'. Then should we read them off our language and say there are such properties in the world? 
- The proposition $P$ may seem syntactically more primitive than $\neg \neg P$. Should we read this structure off our language and say the *fact* that $P$ is different, and in some sense more fundamental than the *fact* that $\neg \neg P$? 
- Quantum physics describes the world in terms of wave functions. Should we then interpret that reality as just a bunch of waves (or maybe, a huge, holistic wave)?

It is obvious that not every linguistic feature reveals reality. Take the proper names "Superman" and "Clark Kent." We don't think there are really two distinct entities out there connected by a relation named "identity". This is a non-trivial claim. Formally, one could construct a toy model $M = \langle D, I \rangle$ with $D = \{s, c\}$, $I(\text{'Superman'}) = s$, $I(\text{'Clark Kent'}) = c$ and $I(\text{'is'})= \{ \langle s,c \rangle, \langle c,s \rangle \}$. Such model would satisfy 'Superman is Clark Kent'.[^1] But intuitively we do not take such a model to *capture reality* because we think 'Superman' and 'Clark Kent' should refer to the same entity in the domain. 

This question can be taken as a better way to formulate realism. The traditional approach formulates realism in terms of *truth*. For example, scientific realism is often described as the view that, scientiÔ¨Åc theories are true(==citation from BVF==). And anti-realists like van Fraassen argue that they aren't really true, but merely _empirically adequate_.

However, this way of framing realism relies on a problematic conception of truth. The problem is that *truth is cheap*. Truth can arise from linguistic rules, not just from reality. This way of framing realism creates a loophole for deflationists. It allows them to call themselves "lightweight realists" simply by agreeing that our metaphysical claims are true, while maintaining that this truth is superficial. But we do have a strong intuition to say, there is something deeper than that. For example, the toy model $M$ about Superman and Clark Kent constructed earlier would make our utterance about their identity come out *true*. But we do feel there is something wrong about it.

If we frame the question of realism in my fashion, however, then the deflationists must say more. For example, according to easy ontologists, adopting a language is easy, and thus ontology is easy. (==citation from Amie Thomasson==) But ontology can't be that easy. Even if "there is $x$" can be made trivially true, it is not trivial to freely read $x$ off the sentence and take $x$ to be really *out there*. And Sider's project about Ontologese is best understood as, we should only read the (most fundamental) reality off the perfectly natural language, not unnatural languages that include kinds like "incars" and "outcars".

Another important distinction needs to be clarified. There is a difference between merely *utilizing* a language and *adopting* a language. To start with, what I mean by language is broad, and includes inferential rules and laws. In other words, that "theory" and "language" are interchangeable in this paper. 

In fact, a full account of this distinction has to be discussed at length elsewhere. But the basic idea is simple. To *utilize* a language is to merely follow syntactic rules and form well-formed sentences. One does not need to take a stance on how to interpret the language. For example, mathematical formalists, like Hilbert, are merely utilizing mathematical languages without reading anything off it. To *adopt* a language, however, one has to also *think* in its semantics and *accept* the picture of reality it implies. A calculator surely doesn't *adopt* arithmetic. In addition, we do not always adopt *literal* English because we don't really think "bank" is a unified category.

### Possible Positions

There are in general three positions for answering this question. 

*Never-ism* is the kind of position sometimes labeled as Kantian Idealism. According to this view, we can only reasonably talk about our own cognitive structure. The "thing-in-itself" out there is unspeakable. Therefore, we never read reality off our linguistic features.

*Always-ism* is the kind of position that modern neo-Carnapians arguably adopt.[^4] We can always freely read reality off any languages we think in, as long as they are truth-functionally equivalent.

*Sometimes-ism* is what mainstream metaphysicians tend to adopt. For example, Armstrong believes only some predicates are real universals. In other words, some linguistic features are privileged, and we should only read the privileged linguistic features off the reality.

Although the debate between the *neo-Quineans* and the *neo-Carnapians* has been mainly focusing on existence, but really it should be seen as a debate between *sometimes-ism* and *always-ism*. 
## The Raise of Fundamentality

I take the familiar story told by ==Schaffer (2009 on what grounds what)==, which is the most-cited paper in this field, as the orthodox version (though I'll be telling a bit differently). 

It goes something like this: contemporary metaphysics began with the Quinean approach to metaphysics. For Quine, the main task of metaphysics is to figure out what indispensably *exists*. Whether numbers or electrons exist depends on whether we must quantify over them in our best theories.

But then, the neo-Carnapians entered the scene. I believe the most important the idea from the neo-Carnapians is that, the meaning of "exists" can vary.[^2] According to this view, when different people debate about what exists, they are speaking different languages. If their languages are truth-functionally equivalent, then this is a mere verbal dispute.

The implication here is serious. If metaphysics is founded on questions of existence, but quantifier variance shows that there are many equally good meanings of "existence" and the debates are merely verbal, then metaphysics risks becoming a largely trivial field of study.

Meanwhile the Neo-Aristotelians also come into play.[^3] They also attack on the neo-Quineans. They argued that even if we had a complete list of what exists, we'd still be missing something important: what is most fundamental, and what grounds what. Schaffer, for instance, argues that questions about existence are trivial. He says that of fictional characters and abstract objects trivially exist. The real, interesting question is about what grounds their existence. This shift in focus from existence to fundamentality has been the dominant trend in metaphysics for the past twenty years or so.

### The problem with the standard story

Here's the problem. From the neo-Aristotelian perspective, this makes them look like the next logical step. In this view, the neo-Carnapians performed a useful, but purely *negative*, task by showing the flaws in the neo-Quinean focus on existence: the meaning of "exist" could vary. With that groundwork laid, the neo-Aristotelians can then step in to offer a *positive* new project, and say the various meaning of "exist" are "exist-more-fundamentally" and "exist-less-fundamentally". Thus, they turning the page on the neo-Quine-Carnap debate and building something new around fundamentality. (==citation==)

But this is surely *not* the case. The neo-Carnapian argument is not just about existence. It is about *semantic variance*. Basically, the pressure coming from the neo-Carnapians is the following: For any objective privilege 

In order to see this more clearly, let's return to the question I framed at the start: *How much reality should we read off our linguistic features?*

**For the neo-Quineans**, the answer is *sometimes-ism about existence*. We can only sometimes read entities off our ordinary language.

**For the neo-Carnapians**, the most plausible answer is *a general form of always-ism*: 

**For the neo-Aristotelians**, it seems they are holding *sometimes-ism about explanation or definition*, and *always-ism about existence*.[^5] Everything we can talk about trivially exists, but it might be derivative or idea-dependent. And the way we trace this kind of dependency is through the structure of definition or explanation (==citation to Fine's essence and modality, guide to ground, and Schaffer's work==).
## Objective Metaphysical Privilege

Now we can clearly see the issue. Neo-Carnapians are arguing for a general form of *always-ism*, which would naturally includes the *always-ism about explanation or definition*. The pressure that the neo-Carnapian argument would put on the neo-Aristotelians' *sometimes-ism* is that, what makes some definition or explanation privileged so that they get to trace fundamentality, but the other doesn't? 

I will later defend sometimes-ism against always-ism. In fact, the *general form of always-ism* is already under pressure. In the previous paragraph, I constructed an exotic toy model that satisfies "Superman is Clark Kent". According to always-ism, there is no problem with this model. Something already seems wrong. But for now I will ignore these problems, put on the hat of neo-Carnapians and explain why the neo-Aristotelians did not give any satisfying answer. 

In the following three sections, I will leave aside the Quinean-flavored view. This view establish fundamentality by pragmatic epistemology, i.e. the fact is determined by the best system that has the most pragmatic virtues like simplicity, elegance and explanatory power. I argue against this view in the second part of this paper. But standard neo-Aristotelians vehemently reject that metaphysics is just Quinean pragmatics. Therefore, in the next three sections, I attack the non-pragmatic neo-Aristotlian view that, we can objectively establish an absolute account of fundamentality that does not lead to semantic variance.

### Naturalness

Let me start with a way to establish a privileged definition using a familiar concept: *naturalness*. This notion was famously introduced by Lewis (==Citation a new theory of universals==), but his account was generally restricted to properties. It was Sider who attempted to use this concept to establish fundamentality (==Citation writing the book of the world==). Basically, Sider extends the notion of naturalness such that linguistic features beyond predicates (e.g., quantifiers, sentential connectives, sentential operators, predicate operators) can be evaluated by whether they carve the world at its joints or not.[^6] The linguistic features of the best language would then perfectly match the joints of reality. Given this extended understanding of naturalness, Sider contends that the most fundamental layer of the world is revealed by the structure of this best language. In this spirit, a "real" explanation/definition is the explanation/definition that only uses perfectly natural expressions.

But surely if the meaning of "naturalness" could vary between languages, then different languages could be considered "the best language" under different semantic interpretations of "naturalness." This is something Sider wants to avoid, because he argues that "naturalness" is supposed to be absolute and "objectively objective." The problem, then, is what makes the meaning of "naturalness" invariant across languages?

One way to defend this invariance is to appeal to reference magnetism. Sider stipulates that naturalness itself is perfectly natural (or, in his terminology, "structure itself is structural" ==p. 137==). And the meaning of the term "naturalness" is automatically attracted to the most natural candidate meaning (==Sider p.23==). Therefore, there is a unique answer to what the "reference magnet" is.

But this doesn't solve the problem. There is no contradiction in having two different languages, both of which agree on the sentence "naturalness itself is perfectly natural" while disagreeing on the extension of "naturalness." For example, in a mereological essentialist's language (ME-language), the ME-quantifier only ranges over simples, while in an ordinary language (OD-language), the OD-quantifier also ranges over composite objects. Assuming they both agree that naturalness itself is perfectly natural, there seems to be no contradiction in each side claiming its own quantifier to be perfectly natural, as long as they interpret "perfectly natural" differently. 

But the greater issue for this solution is that, it is more plausible to take there to be no reference magnets. As Hirsch (==citations==) and Warren (==2023==) pointed out, in the case where language use can determine the reference, magnetism cannot override it. Assume there are Martians seems as intelligent as we are, but they use '+' in a way that we need to interpret it as quus to make their utterance come out true. In this case, we should say the Martians are using '+' to mean quus. And in the cases of vagueness, like bald, where use cannot determine reference, we think it is just indeterminate. In other words, when language use determines the reference, magnetism does not exist. When the use does not determine the reference, magnetism does not exist. Therefore, magnetism does not exist.[^7]

The initial motivation to posit reference magnetism is to explain why our '+' means plus, not quus. It seems our *actual use* does not determine plus, while there is a strong impulse to say the meaning of '+' is plus, not quus. But "actual use" might be a too narrow view about language use. Language use should include our disposition of use, and the disposition is enough to say '+' means plus. More will be said later, but for now I believe this should suffice to dismiss reference magnetism.

Let's return to the original problem: if reference magnetism fails, then what makes the meaning of "naturalness" invariant across languages? More generally, when is any term's meaning fixed in this way?

In a trivial sense, of course, any symbol can vary in meaning; "naturalness" could be used to mean "hello," for instance. But this isn't the kind of meaningful variation we're concerned with here. There might be several ways to explain "invariance".[^8] But the relevant idea might help establish the invariance of "naturalness" here is that, the world can somehow teach us whether we are getting the right concept. If we are using a wrong concept, the world will correct us.

What do I mean by "the world can teach us the right concept"? Special relativity seems to be a nice example. We naturally think that changing inertial reference frames is just a sheer transformation, and the speed of light would depend on the observer's motion. But empirical facts tell us that, surprisingly, our intuition is wrong. No matter how we change our reference frames, the speed of light remains invariant. Therefore there are some privileged transformations, like Lorentz transformations, that objectively does better job at describing the world. Therefore, the world teaches us that, our seemingly natural intuition is in fact incorrect.

The parallel argument used to support an "objectively objective naturalness" is Goodman's riddle of induction. The basic idea is that, a concept like "grue" must be flawed because it leads to a wrong induction. Therefore the world teaches us that some concepts are objectively better than others. 

This is perhaps the strongest case for the objectivity of naturalness, but I believe it ultimately fails. To recap the riddle: Since all emeralds observed before 2050 are green, these emeralds are also by definition "grue" (where grue means green and observed before 2050, or blue otherwise). If we use grue for induction, the grue-induction would tell us that the next emerald we see after 2050 will be grue. However, we intuitively know the next emerald will be green, meaning it will not be grue. Since the green-induction and the grue-induction are formally the same, the problem does not lie in the structure of induction. Then the concept "grue" itself must be the problem.

But I think the world cannot teach grue-speakers that grue-induction is problematic. And therefore, for grue-speakers, there is nothing wrong with grue. Let's consider two cases.

**Case 1**: In the first case, *we* assume that "being green" is an essential property of the kind "emerald." But why would *grue-speakers* accept this? By the same token, they might take "being grue" as the essence of an "emerald." If we showed them a green emerald in 2051, they would simply say, "*This isn't an emerald, because it's not grue.*" In that case, their induction is perfectly reliable: everything they call an "emerald" is, in fact, grue. Of course, this scenario is overly simple because it assumes the kind "emerald" is defined solely by its color. If so, the disagreement becomes a matter of a priori definition, not a failure of induction.

**Case 2**: So, let's consider a more realistic and relevant case. Assume both we and the grue-speakers agree that emeralds are defined by their chemical structure ZYX, not their color. Through induction, our science posit a law of nature that "*ZYX necessitates greenness.*" In grue-science, however, induction would posit a law that "*ZYX necessitates grue-ness.*" Assume that when 2050 arrives, the next emerald turns out green. Therefore, grue-law will be falsified. Then it follows that, the laws of nature is simpler when formulated in terms of green. Thus, the world teaches us that, green is objectively better than grue. 

But this argument doesn't seem right for two reasons. First, Inductive failure is normal. A predicate can be natural, but simply fail to apply to the induction. For example, we might inductively conclude that all swans are white. The discovery of a black swan doesn't make the property "white" is objectively worse than "white or black". It just means we hit on a false hypothesis.

Second, the whole argument relies on the presupposition that, in grue-science, the grue-speakers start with the hypothesis *ZYX necessitates grue-ness.* But given they take the property "green if observed before 2050, or blue otherwise" to be simpler than "green", why wouldn't they instead adopt the hypothesis "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*" as simpler than "*ZYX necessitates grue-ness*"?

To prevent further complications, assume that ZYX is invariant in both our law and the grue-law. Let us even grant that the grue-speakers initially start with the hypothesis *ZYX necessitates grue-ness*, and that this hypothesis is falsified by the first emerald after 2050. After further work, grue scientists discover that the corrected grue-law, like "*ZYX necessitates grue-ness if observed before 2050, or bleen-ness if unobserved before 2050*", is the right one.

Now, the previous argument runs, our law "*ZYX necessitates green*" is objectively simpler than this corrected grue-law. Therefore, green exhibits objective similarity, not grue. Why? Because the properties in the simplest laws of nature 'carve nature at its joints'.

But why would the grue-speakers accept this argument? My stipulation is that grue-speakers have different intuitions about simplicity and similarity from ours. They take properties, that are disjunctive in our terms, to be genuinely simple similar; likewise they treat disjunctive propositions as simpler. Given this stipulation, they would say their corrected law is actually simpler than ours. Therefore, grue and bleen are more privileged than green.

In other words, if the meaning of "the best system" could vary, then the best system account of laws of nature cannot be objectively established. If natural properties are specified by the laws of nature, while the laws of nature can vary in different frameworks, then "naturalness" can vary in different frameworks. Therefore, even if the grue-speakers agree with us that, natural properties are metaphysically privileged, the world cannot force the grue-speakers to give up a different extension of "natural properties".

Surely for us, there is no problem, in accordance with Bayesian principles, to adopt a higher credence for green-laws based on prior distribution. This may be enough to say we are justified to believe green-law, and this is enough for our confirmation. But there is no theoretical reason to take greenness as the metaphysically privileged concept across different frameworks. Nothing can stop "naturalness" having semantic variance.

### Essence

Maybe "naturalness" is already too niche. Indeed, the most orthodoxical neo-Aristotelian way to establish fundamentality is through concepts like grounding or essence. For example, the most fundamental layer is ungrounded (i.e. independent), and everything is either the most fundamental, or is grounded by the most fundamentals (i.e., completeness). In other words, the *sometimes-ism* about definition/explanation is explained by grounding or essence.

The concept of grounding, however, notoriously lacks consensus. Before getting further into this, I wish to go into a slightly earlier concept, i.e. essence.

The approach to establishing fundamentality through essence is largely due to Kit Fine's series of works (=="Essence and Modality," "Senses of Essence," "Ontological Dependence," and "The Logic of Essence"==). Of course, Fine prefers to treat ontological dependence as primitive. But Fine acknowledges that this is merely a technical choice, and we can instead define ontological dependence in terms of essence: $x$ depends on $y$ when there is some property $\phi$ (that does not mention $y$) such that in virtue of $x$'s nature it's true that $y$ has $\phi$, but in virtue of $x$'s nature it's not true that everything has $\phi$. Formally, according to Fine we can choose to define $x \ge y =_{df} \Box_x(y=y)$ along with other axioms in order to capture the informal definition. 

Here, I will not go into the details of Fine's axiomatic system. And I acknowledge that, essence is indeed a concept upon which *de re* modality should be built, not the other way around. Instead, my question is, why should we take essence to be a feature of reality, not a feature of language? To show this, we need to show that, if there is an alternative language that attribute essence differently, then that attribution is *objectively* wrong.

Imagine a set-theoretic creature, maybe a robot called Robo, who perceives every object as a singleton set. For example, Robo directly perceives a table as $\{table\}$ and Socrates as $\{Socrates\}$. Only by theoretical reconstruction can Robo realize that what she perceives is necessarily related to the individual Socrates. For Robo, then, the individual we call Socrates is a theoretical construction from the entity she directly perceives. Therefore, from her perspective, Socrates depends on $\{Socrates\}$.[^9]

Is there anything objectively wrong with this creature Robo? Of course, there is a sense in which Robo is wrong because her attribution of essence violates our intuition. But this reason is *symmetric*: Robo can by the same token accuse us of being counterintuitive. Yet, if Robo's attribution of essence is *objectively* flawed, the reason cannot be symmetric. So, appealing to our own intuition does not count as a good reason to say Robo's attribution of essence is flawed.

I think the only hope to establish the invariance of essence attribution is to let the world teach us whether the concept we are using is wrong. For example, the wrong attribution of essence will make some induction fail, etc. But I can't see any way to do this.

### Grounding(Building)

Now let's get into grounding. Here what I mean by "grounding" here is Wilson's talk of "small-'g' grounding relations" (==Wilson 2014==), or what Bennett calls "building". In fact, let me switch to the terminology of "building" in the rest of the discussion, in order not to conflate with the specific grounding that is usually thought of as a relation between facts. I use building here because fundamentality may not just be established by the specific version of grounding.

Building relation is a set of relations that are directed, necessitating and generative (==Bennett citation==). Some examples are composition, constitution, set-formation, (property) realization, grounding, and causation. To say that $x$ builds $y$ is to say $x$ bears one of these building relations with $y$. According to Bennett, fundamentality can be elegantly established by building relationships. Roughly, the most fundamental layer is unbuilt, and if $A$ builds $B$, then $A$ is more fundamental than $B$.

Immediately, this account of fundamentality is already *relative*, because there are different building relations. For example, according Shaffer's priority monism, the entire cosmos $builds_{grounds}$ its parts, but its parts $build_{compose}$ the entire cosmos. According to this view, there is no "objectively objective" fact of something being absoutely fundamental; Rather, something can only be fundamental *relative to a building relation*.

Even if so, we may still say, *the set of relations* that count as building relation is absolute and objectively objective. If so, then fundamentality, though being relative to a specific building relation, is still invariant across different language. The problem, then again, is whether this can be established.

In the discussion of essence, I have talked about the directness given by set-formation is problematic: the members of a set forms a set does not follow that the members are more fundamental than the set. For Robo, this direction might be reversed: instead of set-formation being a building relation, they take set-membership as a building relation. 

Recall Bennett's account about building: a building relation has to be *directed*, *necessitating* and *generative*. The problem here is the generative requirement. According to Bennett, the generative requirement is that,

> (G) For all building relations B, and all x and y, x's B-ing y makes true certain explanatory and generative claims. For example, if a builds b, then b exists (obtains, is instantiated . . . ) because a does, b exists (obtains, is instantiated) in virtue of a, a makes b exist (obtain, be instantiated), and so forth.

Surely, Robo agree with us that $\text{Socrates} \in \{Socrates\}$. But what they disagree with us that set-formation is not generative. They think set-membership is generative instead.

The problem can be made more general. If $Rxy$ makes $\square (x \leftrightarrow y)$ true, then let's call the relation $R$ *mutually necessitating*.[^10] Given any directed and mutually necessitating relation, we can always find a reverse relation that is also directed and necessitating. And reversal is not the only formal maneuver we can do. Say a binary asymmetric relation $R\subseteq D\times D$ is mutually necessitating. Then we can define inverse $R^{-1}=\{(x,y)\in D\times D \mid R(y,x)\}$. Given that $R^{-1}(x,y) \leftrightarrow R(y,x)$, it is easy to see $R^{-1}$ is also asymmetric and necessitating. Moreover, let $x\equiv_{\Box}y$ abbreviate $\Box(x\leftrightarrow y)$. $R$ lies inside the union of $\equiv_{\Box}$-classes, i.e. $R\subseteq\bigcup_{C\in D/\!\equiv_{\Box}}(C\times C)$. Hence within each $\equiv_{\Box}$-class $C$ of size $n$ one may independently pick for every unordered pair $\{u,v\}\subset C$ either $u\to v$, $v\to u$, or neither, producing $3^{\binom{n}{2}}$ asymmetric, necessitating relations on $C$. Whenever some $\equiv_{\Box}$-class has $n\ge 2$ there are therefore combinatorially many relations $R^{*}$ that (i) predicate over the same domain, (ii) are necessitating and directed, yet (iii) are extensionally different from a given directed, necessitating $R$.

Therefore, if there is any mutually necessitating relation $R$ in the set of building relations, and we want to rule out $R^{*}$, then the problem is how. Bennett suggest the generative constraint, but this constraint cannot establish the invariance of the set of building relations. For Robo, $\{Socrates\}$ exists explains Socrates exists. But we cannot say this is not the "real" explanation, because Socrates is more fundamental than $\{Socrates\}$. This just begs the question. And I don't see other reason to say there is any problem with Robo's understanding of fundamentality.

Indeed, I believe this is the most problematic part of Bennett's account of building relation. Surprisingly, even Bennett concede that what counts generative may be arbitrary, conventional, framework-dependent and "nothing further to be said" (==p. 59==) If this is the case, then how can we save the idea that the set of building relations is invariant and "objectively objective"? I can only see the following two ways out.

The first way is to simply not rule out the alternative relations $R^{*}$. This suggestion, however, makes the generative constraint redundant. It would mean that for any group of mutually necessitating entities, we could order them however we want and claim that our arbitrary ordering reveals a sense of fundamentality. But if a collection of things can be ordered in any arbitrary way, it seems the best way to describe the situation is to say that the things themselves have no intrinsic order. They just merely necessitate each other. Therefore, even if this solution formally works, this move will depart from the intended meaning of fundamentality.

A second way out is to deny that there can be mutually necessitating building relations altogether. If so, then relations like set-formation, successor operator, and logical connectives would be removed from the list of building relations. And it seems that, some other relations, like composition, would be preserved. For example, if we do not adopt mereological essentialism, then material composition is not mutually necessitating, because the parts necessitate the whole, but the whole does not necessitate those specific parts.

But this move undermines the fundamentality project. The fundamentality project is advertised as "post-modal metaphysics". This is because building relations are advertised to be hyperintensional, and "tighter than necessity". But if we rule out mutually necessitating relations, then the remaining relations can be simply characterized by modality.

In addition, this move still does not make the set of building relations invariant. For example, the meaning of "composition" can vary. Mereological nihilists, organicists, common sense metaphysicians, and four-dimensionalists all have different interpretations of "composition". If we allow any of these interpretations to count as a building relation, then what is considered fundamental becomes arbitrary. This, again, seems to depart from the intended meaning of fundamentality.

In addition, it is not only the generative constraint that may lead to semantic variance. The necessitating constraint may also lead to variance, given that there are different senses of necessitation. This has been discussed by Sidelle (==The Grounding Mystique==): Given that modality is at least partly conventional, the deflationary challenge for metaphysical necessity extends equally to grounding.

## Upshot

The gist of the tedious discussion above is simple. Imagine an alien whose language contains "fundamentality" that plays the same role in their metaphysics as our "fundamentality" does in our metaphysics. However, their "fundamentality" has a different extension. If we do not rely on the Quinean epistemology, and claim that fundamentality is a *worldly, objective feature*, then we must be able to show why the alien's concept is *objectively* wrong. Crucially, the reason we provide must not be symmetric; it can't be a reason the aliens could also use against us (e.g., "it violates *our* intuitions"). As the previous sections have shown, it seems we cannot find such a non-question-begging reason in the current way of establishing fundamentality.

It seems the only hope for finding this kind of non-symmetric reason would be the following: the world itself could somehow teach us which "fundamentality" is the correct use. But the challenges from imagining strange aliens essentially press the following question: if we were to throw away some of our intuitions, would the world still be able to teach us anything? The upshot of the discussion is that *it would not*. In a slogan: ***If we are willing to throw away our intuitions, the world cannot teach us anything.***

I believe this slogan is true. However, in the next part of this paper, I show that some of our intuitions cannot be thrown away, and it would lead to substantial consequences.

# II

The arguments discussed in the previous section share a common pattern: we imagine a strange creature who thinks and speaks a radically different language. The challenge is that we seem unable to prove their language is metaphysically worse than ours.

A natural, initial response is to dismiss these creatures as simply _insane_. This is, in fact, the kind of response I want to defend. My strategy, however, isn't to argue that there is something wrong with these alien languages _per se_. Instead, I want to focus on what's wrong with the relationship between _us_ and these languages.

To put it simply, *certain constraints prevents us from coherently adopting and committing to just any theory*. If these constraints have substantial metaphysical consequences, then any theory we can coherently adopt must be consistent with them. Moreover, these metaphysical consequences will be *invariant* across all theories available to us.

Let me clarify further. One might object that, physics already presents a picture of reality that is far beyond our understanding. For example, it's very difficult to truly make sense of concepts like fields, curved spacetime, or probabilistic states. Yet, these are highly successful theories, and we don't reject them. In fact, we make use of them all the time.

To reply, I want to emphasize again a distinction I made in the first part of my paper. There is a distinction between *adopting* a theory and merely *utilizing* it. This distinction is clear in quantum mechanics. Physicists apply the Schr√∂dinger equation to compute predictions and do experiments. Yet they may remain silent about how to interpret the equation. They just shut up and calculate. If so, then they are merely utilizing these equations without adopting them into their worldview. What I am arguing for is the constraint on adopting a theory. There is no problem with using a theory that violates these constraints to do experiments or make predictions. However, as long as we want to truly understand the reality a theory claims to reveal, these constraints apply.

## The principle of Sanity

The constraint I suggest is the following:

**The principle of sanity**
: For someone who is capable of philosophical reflection, it is impossible to genuinely believe oneself to not be a rational agent.

### Clarification

This principle also applies to acceptance (i.e. It is impossible for someone to accept oneself to not be a rational agent), but I will focus on belief in the rest of the paper unless specified otherwise. Anyway, the gist is, it is impossible for us to really adopt a theory that eliminates rational agency.

Here, "philosophical reflection" roughly refers to activities such as examining and revising one's beliefs, analyzing arguments, and choosing between competing theories. The condition "for a person capable of philosophical reflection" is to rule out certain exotic counterexamples. For instance, someone who has been so thoroughly brainwashed that their only possible belief is "I am not a rational agent" would not qualify as a counterexample. I assume all readers who have already made it here are automatically capable of philosophical reflection.

What I mean by "rational" here is a minimal sense of means-end coherence. For example, someone who wills their hand to rise but randomly cause their leg going up would lack this coherence. Thus, even if this randomness could somehow count as agency, it wouldn't be rational agency in the relevant sense.

The precise meaning of "rational agency", of course, is highly controversial and open to multiple acceptable interpretations. Consequently, this principle will have different variants depending on how one chooses to define the term. However, I contend that the *principle of sanity* is plausible under most, if not all, commonsensical interpretations.

Throughout this paper, I will discuss three different commonsensical interpretation of "rational agency". But for current purpose, it will suffice by adopting a minimal account of agency as *epistemic rational agency*. In the following discussion I will just use "epistemic agency" as a shorthand for"epistemic rational agency", unless I want to stress the means-end coherence aspect of my intended sense of agency.

What I mean by "epistemic agency" here is not the kind of agency resisted by doxastic involuntarists. According to doxastic involuntarists, *direct* voluntary control over belief is not possible for us. But here, I use "epistemic rational agency" to refer to any sort of non-random control over our beliefs, no matter how indirect. For example, we have the ability to choose to re-examine our beliefs and investigate new evidence. Doxastic involuntarists, like ==Hieronymi 2008==, also acknowledge that we possess this kind of indirect control. Therefore, it is uncontroversial that we have epistemic rational agency in the intended sense.

### Defense

*The principle of sanity* is more or less a stipulative axiom in this paper that I take to be self-evident. But I will still try to give some defense. 

First, implicitly, this is already a widely accepted principle. Aristotle appears to adopt this principle in Chapter 9 of _De Interpretatione_. There, he rejects the principle of bivalence due to our deliberation and action. He argues that, deliberation and action requires that, propositions about the future must be open (19a10). Then they cannot have a truth value now (19a26). Therefore the principle of bivalence is false. However, Aristotle's argument seems to epistemic instead of metaphysical. When he writes that "what will be has an origin both in deliberation and in action" (19a6-9), the plausible interpretation is that our *beliefs* about the future originate from these rational activities, not the future state of affairs itself. If so, his argument is best understood as follows: it is impossible for us to genuinely *believe* that the future is predetermined while simultaneously viewing ourselves as rational agents who deliberate. Therefore, by *the principle of sanity*, we cannot adopt the principle of bivalence.

There are also more contemporary cases. People seems to reject epiphenomenalism largely due to the reason that, if there's no mental causation, then it seems we wouldn't have any agency, which is insane. Therefore, in fact people have been implicitly using this principle.

Second, I will try to sketch a direct defense. If the principle were false, then it would be possible for a creature to engage in philosophical reflection without genuinely believing itself to have epistemic agency. I claim that this is impossible. Philosophical reflection requires utilizing abstract concepts in order to make inference. But forming abstract concepts requires intentionality. And if someone can intentionality form concepts, one has the ability to bring about epistemic entities. And to bring about epistemic entities is to have epistemic agency. Moreover, one must be able to be aware of there intention to some extent, they must believe that they have such agency to some extent.

I don't expect this argument to be invincible, given the complex nature of agency and intentionality. But I hope it gives some further weight to this principle's plausibility. After all, I believe it is just insane to reject this principle.[^11]

## Consequences


So far, I've illustrated two direct corollaries of the Principle of Sanity on causation and personal identity. These are negative meta-level conclusions, telling us which theories we *cannot* adopt. However, the principle can also yield positive, objective-level results.

#### Are Positive Corollaries Possible?

Immediately a question arises here. *The Principle of Sanity* is a meta-theoretical constraint. Why should an object-level theory has to adopt the content of the meta-level theory?

Here's the basic idea: If *the Principle of Sanity* entails that, it is impossible for us to adopt that $\neg P$, then $P$ has to be an invariable part for any complete metaphysical framework we can coherently adopt. Therefore, we can claim that $P$. Moreover, any attempt to vary $P$ semantically would violate *the Principle of Sanity* and thus be impossible for us to adopt. Hence, we can claim that get the "objectively objective" metaphysics that we want.

To clarify, as I said, *the Principle of Sanity* itself can vary depending on the interpretation of "rational agency." This, however, doesn't weaken my argument. Once an interpretation of "rational agency" is precisified, so too is the set of $P$s that the principle requires us to accept.

Hard-core metaphysicians will probably vehemently resist this and argue the this: The following is a meta-metaphysical claim: *$P$ is invariable in any metaphysical theory we can coherently adopt.* But it does not entail the metaphysical claim: *$P$ is metaphysically true*. The former is a claim about theories, the latter is about the world. We cannot know anything about the world just by think about theories. Therefore, it is impossible for us to make any move from the meta-level to the object-level. 

There are two ways to answer this. The first way is to acknowledge this objection. For any object-level claim being discussed in the subsequent sections, one should translate it into a meta-level constraint. For example, I will argue for an agency-based interpretation for intervention. One should translate this as the following: we cannot coherently adopt a theory that claims semantic invariance yet does not give an agency-based interpretation of intervention.

The second way is to maintain that the entailment holds, and the hard-core metaphysicians have a wrong conception of metaphysics. According to the hard-core metaphysicians, it is possible that $P$, meanwhile it is impossible for us to coherently adopt that $P$. In other words, it is possible that, we cannot coherently adopt the "true metaphysics". 

But in practice, we reject this possiblity all the time. Assume $A$ and $B$ are two *inconsistent* sentences about the world. Anyone would agree that we must deny at least one of them, because they are inconsistent. But this justification rely on the following presupposition: *If a theory is inconsistent, then it is not a metaphysically true theory.*  But what justifies this? The only reason that I can conveive of is that, *we cannot coherently adopt such an inconsistent theory*. But this justification is not available for the hard-core metaphysicians, because "inconsistent theories are not adoptable" doesn't entail that "the world is consistent".[^12] Therefore, according to the hard-core metaphysicians, we do not have to deny $A$ or $B$. Maybe the "true metaphysics" really is somehow inconsistent. But this sounds insane. The referent of "the world" in metaphysics never entertains the possiblity that, it might be inconsistent. Indeed, if we allow this possibility, then I don't know if we can say anything meaningful at all about the world.

I admit this is a non-trivial move. So let me defend this further. What is the problem for the hard-core conception of metaphysics? According to this conception, we cannot derive anything about the world from how we would *think* about the world. But winding back to the beginning of this paper: *If we throw away all of our intuitions, then we cannot establish anything*. And the intuitive belief here is: we are rational agents. If we are not allowed to derive anything about the world from our intuition, then this kind of metaphysics can only belong to God, not to human beings. For human beings, then, we should pursue human knowledge and understand reality in a human way, not in God's way. Therefore, if metaphysics is to be a subject of human inquiry at all, it must accept our human limitations.

I personally stand with the second reply and allow the move from the meta-level to the object-level. However, for people who are not persuaded by this, they can adopt the first reply and translate my subsequent claims into meta-level claims.

#### Horizontal Building: Causation

I want to argue that *the Principle of Sanity* gives an agency-based interpretation to the interventionist approach to causation. After my defense of the interventionist account, I will then extend it to the analysis of fundamentality.

Let me give a quick introduction to this interventionist approach. This approach uses Structural Equation Models (SEM) to analyze causation. Nowadays among statisticians and scientists, this is the probably most popular way to make causal inference. The most basic interventionist causal analysis is this: $A$ causes $B$ if intervention on $A$'s state will change $B$'s state. The latter part can be formalized and evaluated by causal models. Let's quickly go through the basic formalization to get the gist. A causal model is $M=\{U,V,F\}$ where $U$ is a set of exogenous variables, $V$ is a set of endogenous variables, and $F$ is a set of structural equations that determines the value of endogenous variables by exogenous variables. A structural equation takes the form of $X=f(Y)$, where "$=$" does not mean equality, but means value assignment (like in programming). $X=f(Y)$ means that the value of $X$ is set to $f(Y)$. Given an assignment of exogenous variables $u$, we will have all the values in $M$ fixed. Assume $A$ is $a$ and $B$ is $b$ in the actual world. To analyze whether $A$ causes $B$, we surgically set the value of $A$ to $a' \neq a$  and therefore produce a new counterfactual model $M_{A=a'}$. Given the same assignment $u$, if value of $B$ in $M_{A=a'}$ is not $b$, then we say $A$ causes $B$.

Surely, this basic counterfactual account does not work in the cases of preemption and overdetermination. ==Halpern & Pearl 2005== developed a more complicated account of actual cause that solves these problems. But here I will not go into these details. Metaphysicians are more or less skeptical about this account *not* because of its detailed formalization. The worry of this approach is that, the analysis irreducibly rely on a "surgical intervention" to set the value of a variable. But this very concept of "intervention" is a causal concept. Therefore, this account is using causal notion to analyze causal notion, which is circular. 

And this circularity leads to *a problem of interpretation*. All languages need interpretation. The formal language of SEM also needs an interpretation. But if the semantic interpretation of SEM in turn requires another SEM, then it will lead to infinite regression. The standard reply is to admit that causation as primitive, and the notion of intervention depends on causation. But how come that we automatically have a primitive interpretation of causation? I believe more can be said.

##### The Strong Principle of Sanity

Now, I want to derive a positive account of causation from *the Principle of Sanity*. But the current interpretation of "rational agency" is *epistemic agency*. To the extent that it necessitates causation, it only necessitates the causation between *mental states*. Therefore, if we want to establish causation among physical objects, we have to adopt a stronger reading of "rational agency" in the _Principle of Sanity_. 

Now, "rational agency" not only include *epistemic agency*, but also *the capacity to control our bodies*. Call the strengthened claim _the Strong Principle of Sanity_. Note that my earlier defense appealed to the analytic entailment of epistemic agency from the capacity for philosophical reflection; That defense does not extend to *the Strong Principle of Sanity*: a smart ghost has no control over any body, but it might be capable of philosophical reflection. However, there are ways to make *the Strong Principle of Sanity* a priori again. For example, one could formulate it as 

- For someone who is capable of moving one's eyes intentionally in order to read this paper, it is impossible to genuinely believe oneself to not be a rational agent.

I will not further discuss the details of this maneuver. The gist is to make the constraint entail rational agency, and it is uncontroversial that we satisfy thiBut I think even without this constraint, it just seems insane for any human being to reject this.

Now given *the Strong Principle of Sanity*, we get the indispensability of the agency of bodily control. But this kind of agency just is mental-physical causation. I will use it to establish physical causation.

##### Solving the Interpretation Problem

Previously I discussed how to interpret "surgical intervention." Why must the intervention be "surgical"? Because we must ensure that the intervention affects only the target variable. Other variables may change as a result of intervention. But their changes should only be determined by the structural equations. The standard worry is that empirical interventions are never truly surgical. Assume we want to intervene on the current in a coil. We cannot directly create current to flow in a coil; we have to create a moving magnetic field. This in turn requires moving a magnet, and so on. And presumably this would lead to tons of variable to change. Therefore, no worldly intervention has the purity the SEM formalism presupposes, and the operation lacks an interpretation.

Given the _Strong Principle of Sanity_, the answer is straightforward. To surgically intervene on a physical event is to posit direct mental causation of that event. By mentally causing the relevant physical variable to take a new value, we obtain a clean, surgical intervention. And because interventionist models can analyze physical causation, we can, via SEMs, develop a full account of causation.

In other words, on my interpretation there is no problematic circularity in the interventionist account. It analyzes non-agential causation by appeal to a primitive notion of agential causation, and the *Strong Principle of Sanity* licenses that primitive.

In fact, agency-based interpretations were actually the predecessors of contemporary interventionism. ==von Wright (1971); Peter Menzies and Huw Price (1993)== developed agent-based accounts that aimed to reduce causation to agency. However, that kind of reductionism does not work, because it is just impossible to separate the notion of causation from agency. My interpretation differs. I do not try to reduce causation to non-causal notions; I am just analyzing non-agential causation in terms of agential causation, while maintaining that the interpretation of agential causation is guaranteed by my principle.

Another problem is more salient: an agent-based analysis of causation seems anthropocentric. A standard objection runs as follows: in the early universe there were arguably no agents. If physical causation is analyzed by agency, then there was no causation in the universe until agents existed. That consequence is surely wrong. So, the objector concludes, physical causation cannot be analyzed by agency.

My reply is that actual causation is analyzed by *possible* intervention. Such intervention depends not on actual agency but on *possible* agency. Therefore, even when there are no actual agents in the universe, there is still causation, because agency is still *possible*. Again, the notion of agency here is not anthropocentric: it specifies no particular human body parts. It must be so. We cannot know a priori which parts of the world comprise our bodies.

Let me explain where my argument lead us. The direct positive conclusion from the *Strong Principle of Sanity* require us to take mental and mental-physical causation to be *really out there*. And through *utilizing* interventionist's formal language, we can talk about physical causation. But it is not required for us to *adopt* this language and say physical causation is really out there. Indeed, what I am claiming is compatible with a deflationary view about physical causation, but not mental-physical or mental causation.

#### Fundamentality

Finally, it's time to go back to where I started: fundamentality. The reason why I spent so much time on the interventionist account is that, it has a very important feature that I need to establish fundamentality: It ensures the *asymmetry*. 

In the most basic counterfactual analysis of causation, A actually causes B if (1) A and B both occur in the actual world; and (2) if A had not occurred, then B would not have occurred. On Lewis-style possible-world semantics, (2) is true if, in the closest worlds to the actual world where A does not occur, B also does not occur. However, this semantics does not rule out symmetry. If, in the closest worlds where B does not occur, A also does not occur, we would conclude that B actually causes A. Setting aside the possibility of cyclic causation, causation should be asymmetric. Yet there is no straightforward way to modify Lewis-style possible-world semantics to guarantee this asymmetry.

By contrast, in interventionist semantics (setting aside cyclic models), the asymmetry is ensured. Let us illustrate with the toy model $M = \{\{A\}, \{B\}, \{B = A\}\}$ and the actual assignment $u(A) = 1$. The counterfactual "if A had not occurred, then B would not have occurred" is true because, given $u$, the value of $B$ in $M_{A=0}$ is 0. But the counterfactual "if $B$ had not occurred, then $A$ would not have occurred" is false because, given $u$, the value of $A$ in $M_{B=0}$ is 1. In other words, so long as the model is acyclic, the asymmetry is ensured by design.

Is this circular? It seems that the structural equation $B = A$ in $M$ already presupposes this asymmetry. If the reason we treat $M$ as an appropriate causal model is that, we presuppose that $A$ causes $B$, then it is circular. However, structural equations can be discovered empirically via intervention tests. For example, to test whether $B = A$, we can intervene on $A$ and observe how the value of $B$ changes. Therefore, the world can teach us whether is an appropriate model, without circularity. And given that *the Strong Principle of Sanity* ensures the semantic invariance of intervention, we do not need to worry about a strange creature who interpret it differently.

Now, recall my problem for Bennett's account of fundamentality: for any mutually necessitating relations, we were unable to privilege a direction for asymmetry. Bennett used the generative constraint to ensure the direction, but this constraint is framework dependent. Therefore, it does not ensure the invariance of the privilege.

However, given the invariance of SEM, the direction provided by SEM is privileged. But let me slow down: SEM is used for causation. How can it be used for fundamentality?

In fact, using SEM to analyze fundamentality is no new idea. ==Schaffer grounding in the image of causation== and ==Wilson metaphysical causation== have all written in detail on how to use structural equations to analyze fundamentality, and even see horizontal building as metaphysical causation. The hard question, however, is how to choose the *appropriate* model for analysis. In the case of causation, we can do intervention tests to empirically discover the appropriate equation. But it's not clear how to do intervention tests on singleton Socrates.

Let me use a toy example to illustrate this problem. Assume we choose a model where the structural equation is $\{Socreates\}=Socrates$. When we intervene on $Socrates$, the value of $\{Socreates\}$ would change. When we intervene on $\{Socreates\}$, the value of $Socrates$ would not. Therefore, we can easily get the result that Socrates builds singleton Socrates.

But the problem is, why do we choose $\{Socreates\}=Socrates$ as the *appropriate* equation? Recall the previous challenge. Robo would choose $Socrates=\{Socreates\}$. Why is Robo's choice problematic? I have not seen a satisfying reply in the current literature. For example, Schaffer seems to adopt a neutral position about model choice, and thus did not provide reason against Robo's choice.

Recall Bennett's formulation: "building", "making" and "generative". All of these are agential concepts. I believe that, Bennett's intuition aligns with my analysis. There must be something related to agency that leads to the directness that we want. Informally speaking, we can in some sense build singleton Socrates out of Socrates, but we cannot build Socrates out of singleton Socrates.

Let me explicate this intuition into an argument. According to my previous discuss, intervention depends on agency. But agency depends on intention formation. If an intervention is inconceivable, then there is no way to form an intention. If there cannot be such an intention, then the agential intervention is impossible. 

Now I want to argue that, there is no way to conceive of a surgical intervention on singleton Socrates. In other words, we cannot conceive of a way to directly modify singleton Socrates. We can intervene on Socrates, or maybe on set theory. But these are all indirect intervention, and thus not surgical. Therefore, a surgical intervention on singleton Socrates is impossible.

If an intervention is impossible for a variable, then the interventionist counterfactual "If we intervene on singleton Socrates, then the value of Socrates would change" is ill-posed. Therefore, it is impossible to establish $Socreates=\{Socrates\}$, because there is no way to intervene on $\{Socreates\}$.

Is this a good reason to show that, Robo's metaphysics is flawed? It doesn't seem so. This reason is still symmetric. Robo could say, they cannot conceive of an intervention on Socrates. But they can conceive of an intervention on $\{Socrates\}$. Does this mean that my attempt failed? No. What Robo mean by "agency" would be fundamentally different from ours. In our term, Robo is not a rational agent. Maybe Robo is a rational schmagent. But given *the Strong Principle of Sanity*, we cannot give up that we are rational agents and claim we are rational schmagents. Therefore, the principle tells us that, Robo's view about "intervention" is not adoptable for us. Therefore, their view on fundamentality is also not adoptable for us. If you accept the previous move from meta-level to object-level, then we should say the following: It is objectively true that, the appropriate equation is $\{Socrates\}=Socrates$. Therefore, $Socrates$ is more fundamental than $\{Socrates\}$.

Let's now check again where my argument leads us to. First, What I argued so far easily extends to building relations other than set-formation, but to save space, I leave this to the readers. Second, my argument established an account of how to establish a privileged direction for building relations. Like the previous discussion of physical causation, this is compatible with a deflationary view about the building relations established by SEM. However, there is one thing that cannot be deflated: the mental-physical causation required by rational agency. In fact, maybe it's better not to call this causation, because I am inclined to say this is atemporal. Anyway, the fact that, our intention *builds* our action must not be deflated. In this sense, fundamentality cannot be not altogether deflated into interventionist formalism, because this mental-physical building relation is required by our sanity.

### Beyond metaphysics

The next thing I want to show is that, *the Principle of Sanity* should be taken seriously because it is not just a metaphysical principle. It is supposed to be a constraint on any philosophical investigation. In order to show this, I want to talk about a consequence it implies in ethics.

Consider the follow dilemma from ==Baker 1987 trust and rationality==: Imagine your closest friend is Smith. One day, the police show up at your door with compelling evidence against him: surveillance footage shows someone who looks just like Smith robbing a bank at gunpoint, and they've found his fingerprints at the scene. The police don't know where Smith is and ask you to contact them if you see him. A few days later, Smith appears at your door. He desperately claims he's been framed and had nothing to do with the robbery. The question is: should you trust his word?

==Hieronymi 2005 The reasons of trust== argues that in this case we cannot rationally trust our friend. She argues that once we think about this belief, we'll find it lacks proper foundation. This is because only beliefs formed through a truth-directed process can withstand rational reflection. If one is rational, then one will lose the belief formed by practical considerations alone.

Let me lay out my argument first and then defend them step by step. First, I'll argue that for human beings, being a moral agent is impossible without social interaction. Second, social interaction is impossible without assuming that others are being truthful. Given our friends are central to our social interaction, trusting friends is necessary for sustaining moral agency. If unfortunately, your evidence constantly conflicts with a friend's testimony, your sanity requires you to trust your friend against the evidence.


## Conclusion

Now let's go back to the question that I raised in the beginning of the paper: *How much reality should we read off our linguistic features?* My reply is, we must at least read the things that sustain our rational agency off our linguistic features. For example, the first-person pronoun, certain sort of causation and fundamentality. Further than that, I remain neutral and tend to adopt a general form of *always-ism*.

So far I have suggested three senses of rational agency: epistemic agency, bodily agency, and moral agency. They correspond to *the Principle of Sanity*, *the Strong Principle of Sanity* and *The Principle of Moral Agency*. Although one can adopt some and reject some, I believe it is more plausible to accept them all. I believe they all derive from the commonsensical notion of rational agency.

What would be the takeaway point of this whole paper, then? I think the point I want to make is the following. Throughout the history of philosophy, there has always been a strong impulse to secure an absolute truth that is objective and eternal. But this is impossible. Again, only deductive proofs can secure truth in this fashion. But any deductive proof relies on premises. Ultimately, some premises will have to depend on our intuitions. If so, then we should not abandon the kind of intuition that we cherish in our ordinary life: our own rational agency. To adopt a worldview that eliminates this that is, for us, simply insane.


[^1]: Needless to say, in a complete model for English language, many other vocabularies need different interpretations.
[^2]: I'm focusing on quantifier variance because I believe it's the most plausible and basic form of the neo-Carnapian position. ==Thomasson (2015 p. 70)== herself thinks her easy ontology doesn't depend on quantifier variance, but this is surely mistaken. Her project relies on the triviality of changing the extension of the predicate "exists". But this surely changes the meaning of the quantifier.
[^3]: Surely this paper is not intended to become the zoology of metametaphysics, so these labels may not be precise. What I mean by neo-Aristotelians is basically the fans of fundamentality. For example, Sider counts as a neo-Aristotelian in my sense. Though Sider is usually considered as a neo-Lewisian, which is closer to neo-Quinean. But the label is not important anyway.
[^4]: A less plausible position for neo-Carnapians is that they cannot make sense of my question, because it is "heavyweight". However, as long as one can make sense of the distinction between merely utilizing a language and truly adopting and think in a language, my question should make sense. For example, even Hirsch accept this distinction in ==Dividing Reality, p.14==: "*one does not abandon a position merely by adopting a language*". There are some terminology issue here. What Hirsch means by "adopting a language" is what I mean by "utilizing a language", and what Hirsch means by "taking a position" is what I mean by "adopt a language".
[^5]: This is probably not the orthodoxical formulation. The orthodoxical formulation is, we distinguish absolute fundamentality and relative fundamentality. For absolutely fundamentality, we usually appeal to concepts like completeness, dependency, indispensability or naturalness. But when asked which set of things has this feature, we appeal to concepts, like grounding. And the standard story is to treat these relevant concepts as primitive. But then we need an epistemology for this primitive grounding. Ultimately the conclusion of "what grounds what" appeals to virtues like explanatory power or definitional simplicity. Therefore, ultimately we decide what's absolutely fundamental based on explanation or definition. I have not seen how could this be done otherwise. This also applies to relative fundamentality.
[^6]: In fact Sider tend to drop the term "naturalness" to favor the term "joint-carving" or "structural" for the extended meaning of "naturalness". But let me stick to "naturalness" here.
[^7]: In fact, there might be resistance. One can claim that, there can be reference magnetism when we want to use words in this way. However, according to this account, which referential candidate gets the magnetic power still depend on our disposition of use. Therefore, I don't see how there could be any heavyweight reference magnetism that can support Sider's view.
[^8]: One possible approach is to define the essential meaning of a term by its introduction and elimination rules (IE-rules), that is, by its functional role in language. This may be intuitive for logical constants. For a term to count as a quantifier, for example, it must obey the IE-rules for quantifiers. However, the full meaning of a quantifier also depends on its domain, and the IE-rules don't fix this. Therefore, the meaning of a quantifier can still vary as its domain varies. This approach faces several problems. First, this form of essentialism about meaning seems arbitrary. Why should IE-rules be the only thing that constitutes a term's essential meaning? For example, we might say in some sense 'quus' is a variation of 'plus', not a completely unrelated concept like 'Hello'. But they have different IE-rules. Second, for non-logical terms, the IE-rules themselves are usually formulated using terms that can vary. Third, and most importantly, even if we accept this IE-rules essentialism, it would not help fix "naturalness". It leads to a specific conclusion: a term's meaning is invariant if its meaning is exhausted by its IE-rules. For example, a logical conventionalist might say $\land$'s meaning entirely determined its IE-rules. Then according to IE-rules essentialism, $\land$ is invariant. However, Sider clearly intends "naturalness" to be more than just a conventional, syntactic symbol. Therefore, this strategy for securing semantic invariance is not a viable option for his project anyway.
[^9]: If this example is puzzling to you, you can try the following notation: she uses "Socrates" to refer to $\{Socrates\}$, and "-Socrates-" to refer to Socrates. Mathematically speaking, set formation has nothing to do with intrinsicality. Our intuition that *a member of a set* is somehow intrinsic and thus more fundamental than *the set itself* is just our *interpretation* of set formation, influenced by ordinary language words like "member" and how we use brackets to warp the members. But we could have used alternative notations, like using "M-relation" instead of membership, and use $\{\}-Socrates$ intead of $\{Socrates\}$.
[^10]: Bennett's formulation of the necessitating requirement is a bit more complicated than this. She also consider the circumstance of necessitation (==p.52-54==). But here I will just use the cleaner way to formulating it for the toy example.
[^11]: In other words, I am in fact suggesting two different principles. The stronger version is what I explicitly laid down and inclined to stand with. But even if one somehow resisted it, my conclusion can still be established by the weaker version, i.e. if we are capable of philosophical reflection, then we should not believe we are not rational agents.
[^12]: The world is "inconsistent" is not a good formulation. In theory, consistency is a syntactic notion. It is a categorical mistake to apply it to the world. Maybe the better way to formulate the sentence is as follows: the world is such that, the "true" theory about the world will come out inconsistent. But for brevity, I will use the term "inconsistent world" to mean this.
